{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "df = pandas.read_excel(\"Input data.xlsx\")\n",
    "y = df['y_up'].to_numpy(dtype=float)\n",
    "df = df.drop(['y_up', 'y_down'], axis=1)\n",
    "df\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.95140445, -0.77119774, -1.40167411, ...,  2.32691655,\n",
       "         0.03518696, -1.72966581],\n",
       "       [-0.95663765, -0.81838364, -0.81621862, ..., -0.21056596,\n",
       "         0.03518696, -1.7291517 ],\n",
       "       [-0.95280851, -1.05904612,  0.50299573, ...,  2.24344673,\n",
       "         0.03518696, -1.72863759],\n",
       "       ...,\n",
       "       [ 2.27970293, -0.16519809, -0.4315819 , ..., -0.72807884,\n",
       "        -0.53540121,  1.73389031],\n",
       "       [ 2.25900479, -0.6769311 , -0.62772882, ...,  0.85784773,\n",
       "         2.02653969,  1.73594674],\n",
       "       [ 2.23866985, -0.66585145, -0.62063376, ..., -1.12873398,\n",
       "        -0.32998947,  1.73646085]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the dataset\n",
    "x = df.to_numpy(dtype=float)\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape:    (4113, 58)\n",
      "transformed shape: (4113, 36)\n",
      "explained variance ratio: [0.18347142 0.16548167 0.11176524 0.07071035 0.05073588 0.03977421\n",
      " 0.03691413 0.02889651 0.02771614 0.02407093 0.02138354 0.01840944\n",
      " 0.01770325 0.01710249 0.01579806 0.01427089 0.01317567 0.01260686\n",
      " 0.01156106 0.0106204  0.00991628 0.00920336 0.00857581 0.00827396\n",
      " 0.00798753 0.00738008 0.00701171 0.00646579 0.00505147 0.00490368\n",
      " 0.00471088 0.0039477  0.00389131 0.00322634 0.00269572 0.0025521 ]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=36)\n",
    "X_pca = pca.fit_transform(x)\n",
    "\n",
    "print(\"original shape:   \", x.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)\n",
    "print(\"explained variance ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch import from_numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Convert features and labels into numpy arrays and then into PyTorch tensors\n",
    "X_data = from_numpy(x.astype(np.float32))\n",
    "y_data = from_numpy(y.astype(np.float32))\n",
    "\n",
    "# Load your data into the custom dataset\n",
    "dataset = CustomDataset(X_data, y_data)\n",
    "\n",
    "# Determine the lengths of your train/test splits (here, 70%/30% split)\n",
    "total_len = len(dataset)\n",
    "train_len = int(0.85 * total_len)\n",
    "test_len = total_len - train_len\n",
    "\n",
    "# Split the dataset\n",
    "train_set, test_set = random_split(dataset, [train_len, test_len])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.float64' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Wyett\\Desktop\\v3\\Model1.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Wyett/Desktop/v3/Model1.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Wyett/Desktop/v3/Model1.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Wyett/Desktop/v3/Model1.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m Model1(\u001b[39mlen\u001b[39m(x[\u001b[39m0\u001b[39m]), \u001b[39mlen\u001b[39;49m(y[\u001b[39m0\u001b[39;49m]))\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'numpy.float64' has no len()"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Model1(nn.Module):\n",
    "  def __init__(self, input_len:int, output_len:int):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_len, input_len // 2)\n",
    "    self.fc2 = nn.Linear(input_len // 2,input_len // 4)\n",
    "    self.fc3 = nn.Linear(input_len // 4,output_len)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = x.view(x.shape[0], -1)\n",
    "    \n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.sigmoid(self.fc3(x))\n",
    "    \n",
    "    return x\n",
    "\n",
    "model = Model1(len(x[0]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1, Training loss: 0.6925270465287295, run on the CPU with time 0.12607470000511967\n",
      "run 2, Training loss: 0.6832746256481518, run on the CPU with time 0.06403849998605438\n",
      "run 3, Training loss: 0.672013087164272, run on the CPU with time 0.07991709999623708\n",
      "run 4, Training loss: 0.6572752741250125, run on the CPU with time 0.11211619997629896\n",
      "run 5, Training loss: 0.6378598532893441, run on the CPU with time 0.08199119998607785\n",
      "run 6, Training loss: 0.6148515977642753, run on the CPU with time 0.06364919999032281\n",
      "run 7, Training loss: 0.5905910860408436, run on the CPU with time 0.06326399999670684\n",
      "run 8, Training loss: 0.5664362254467877, run on the CPU with time 0.06290170000283979\n",
      "run 9, Training loss: 0.5424927158789201, run on the CPU with time 0.06420210000942461\n",
      "run 10, Training loss: 0.5189721554517746, run on the CPU with time 0.06911149999359623\n",
      "run 11, Training loss: 0.49993431215936485, run on the CPU with time 0.07134860000223853\n",
      "run 12, Training loss: 0.481047507036816, run on the CPU with time 0.06838330000755377\n",
      "run 13, Training loss: 0.4666608593680642, run on the CPU with time 0.06587090002722107\n",
      "run 14, Training loss: 0.45332794866778636, run on the CPU with time 0.06778449998819269\n",
      "run 15, Training loss: 0.4451785552230748, run on the CPU with time 0.06545209998148493\n",
      "run 16, Training loss: 0.4330287911675193, run on the CPU with time 0.06536469998536631\n",
      "run 17, Training loss: 0.4248810272325169, run on the CPU with time 0.06357939998270012\n",
      "run 18, Training loss: 0.41727167909795587, run on the CPU with time 0.062472799996612594\n",
      "run 19, Training loss: 0.40878699056126855, run on the CPU with time 0.06426199999987148\n",
      "run 20, Training loss: 0.4038307422941381, run on the CPU with time 0.06982090001110919\n",
      "run 21, Training loss: 0.3980373486876488, run on the CPU with time 0.06614049998461269\n",
      "run 22, Training loss: 0.3933366495099935, run on the CPU with time 0.06328790000407025\n",
      "run 23, Training loss: 0.38831184709613975, run on the CPU with time 0.0838668999786023\n",
      "run 24, Training loss: 0.3836541281505065, run on the CPU with time 0.06261200000881217\n",
      "run 25, Training loss: 0.38041526052084834, run on the CPU with time 0.06389130000025034\n",
      "run 26, Training loss: 0.37790378426963633, run on the CPU with time 0.06768829998327419\n",
      "run 27, Training loss: 0.37127967382019217, run on the CPU with time 0.06927700000233017\n",
      "run 28, Training loss: 0.36936302835291085, run on the CPU with time 0.06451799999922514\n",
      "run 29, Training loss: 0.36530839855020697, run on the CPU with time 0.06354500001179986\n",
      "run 30, Training loss: 0.3630182988264344, run on the CPU with time 0.06232570001157001\n",
      "run 31, Training loss: 0.35902038988741963, run on the CPU with time 0.06512109999312088\n",
      "run 32, Training loss: 0.35696208734403956, run on the CPU with time 0.06363430002238601\n",
      "run 33, Training loss: 0.35746173858642577, run on the CPU with time 0.07448779998230748\n",
      "run 34, Training loss: 0.3514052061872049, run on the CPU with time 0.0808651999977883\n",
      "run 35, Training loss: 0.35260430466045034, run on the CPU with time 0.06978150000213645\n",
      "run 36, Training loss: 0.3458964018659158, run on the CPU with time 0.06993170001078397\n",
      "run 37, Training loss: 0.34494108923456884, run on the CPU with time 0.06288599999970756\n",
      "run 38, Training loss: 0.3434765207496556, run on the CPU with time 0.06956470001023263\n",
      "run 39, Training loss: 0.34134705486622724, run on the CPU with time 0.0657296999997925\n",
      "run 40, Training loss: 0.3393295621330088, run on the CPU with time 0.06557409997913055\n",
      "run 41, Training loss: 0.3353094457225366, run on the CPU with time 0.06393350000143982\n",
      "run 42, Training loss: 0.3342084153132005, run on the CPU with time 0.08502759999828413\n",
      "run 43, Training loss: 0.3334650359370492, run on the CPU with time 0.07867009998881258\n",
      "run 44, Training loss: 0.33073498593135314, run on the CPU with time 0.06482610001694411\n",
      "run 45, Training loss: 0.32961669726805254, run on the CPU with time 0.06911569999647327\n",
      "run 46, Training loss: 0.3270222774960778, run on the CPU with time 0.0724474000162445\n",
      "run 47, Training loss: 0.32567502070556986, run on the CPU with time 0.06695430001127534\n",
      "run 48, Training loss: 0.32382268363779243, run on the CPU with time 0.06754899999941699\n",
      "run 49, Training loss: 0.3216178555380214, run on the CPU with time 0.06400389998452738\n",
      "run 50, Training loss: 0.32007953693920915, run on the CPU with time 0.06990949998726137\n",
      "run 51, Training loss: 0.3195244304158471, run on the CPU with time 0.07081959999050014\n",
      "run 52, Training loss: 0.31890096705068244, run on the CPU with time 0.06622099998639897\n",
      "run 53, Training loss: 0.31539598215710035, run on the CPU with time 0.06383979998645373\n",
      "run 54, Training loss: 0.31519371569156646, run on the CPU with time 0.06505960001959465\n",
      "run 55, Training loss: 0.31429242261431434, run on the CPU with time 0.062219200015533715\n",
      "run 56, Training loss: 0.31285602233626625, run on the CPU with time 0.06261649998486973\n",
      "run 57, Training loss: 0.31134681898084554, run on the CPU with time 0.07152050000149757\n",
      "run 58, Training loss: 0.31055407226085663, run on the CPU with time 0.07657499998458661\n",
      "run 59, Training loss: 0.3097300544381142, run on the CPU with time 0.06966989999637008\n",
      "run 60, Training loss: 0.30760284770618784, run on the CPU with time 0.06278419998125173\n",
      "run 61, Training loss: 0.30648529258641327, run on the CPU with time 0.06498230001307093\n",
      "run 62, Training loss: 0.30691010599786583, run on the CPU with time 0.06480660001398064\n",
      "run 63, Training loss: 0.306147201359272, run on the CPU with time 0.06287729999166913\n",
      "run 64, Training loss: 0.3021501359275796, run on the CPU with time 0.06267539999680594\n",
      "run 65, Training loss: 0.30237547010183335, run on the CPU with time 0.06272480002371594\n",
      "run 66, Training loss: 0.3026105505498973, run on the CPU with time 0.06640480001806282\n",
      "run 67, Training loss: 0.2993377258154479, run on the CPU with time 0.0803087999811396\n",
      "run 68, Training loss: 0.298242419754917, run on the CPU with time 0.07631840000976808\n",
      "run 69, Training loss: 0.29998762607574464, run on the CPU with time 0.07145019999006763\n",
      "run 70, Training loss: 0.30049100992354477, run on the CPU with time 0.06212220000452362\n",
      "run 71, Training loss: 0.2974162317135117, run on the CPU with time 0.06339819999993779\n",
      "run 72, Training loss: 0.29814490120519294, run on the CPU with time 0.06377219999558292\n",
      "run 73, Training loss: 0.29335156340490687, run on the CPU with time 0.06312800000887364\n",
      "run 74, Training loss: 0.2933915838599205, run on the CPU with time 0.06167880000430159\n",
      "run 75, Training loss: 0.29152629165486854, run on the CPU with time 0.06325720000313595\n",
      "run 76, Training loss: 0.2915167120369998, run on the CPU with time 0.07572139997500926\n",
      "run 77, Training loss: 0.2888478130102158, run on the CPU with time 0.0827970000100322\n",
      "run 78, Training loss: 0.2879313827915625, run on the CPU with time 0.06776360000367276\n",
      "run 79, Training loss: 0.28838233717463235, run on the CPU with time 0.06395169999450445\n",
      "run 80, Training loss: 0.2857862044464458, run on the CPU with time 0.07216119999065995\n",
      "run 81, Training loss: 0.28597942170771684, run on the CPU with time 0.06347670001559891\n",
      "run 82, Training loss: 0.28653673949566755, run on the CPU with time 0.06209809999563731\n",
      "run 83, Training loss: 0.2823126260529865, run on the CPU with time 0.06298240000614896\n",
      "run 84, Training loss: 0.2819701391187581, run on the CPU with time 0.06328539998503402\n",
      "run 85, Training loss: 0.28144785788926213, run on the CPU with time 0.06179149998934008\n",
      "run 86, Training loss: 0.2790500290014527, run on the CPU with time 0.06280789998709224\n",
      "run 87, Training loss: 0.27721447118303993, run on the CPU with time 0.06443349999608472\n",
      "run 88, Training loss: 0.2759735141965476, run on the CPU with time 0.0967872999899555\n",
      "run 89, Training loss: 0.27577131187373943, run on the CPU with time 0.06938119998085313\n",
      "run 90, Training loss: 0.2762796412814747, run on the CPU with time 0.06548250000923872\n",
      "run 91, Training loss: 0.27270923487164755, run on the CPU with time 0.06690809997962788\n",
      "run 92, Training loss: 0.2743078863756223, run on the CPU with time 0.0630984999879729\n",
      "run 93, Training loss: 0.2726675342429768, run on the CPU with time 0.06284329999471083\n",
      "run 94, Training loss: 0.2705041748556224, run on the CPU with time 0.06362550001358613\n",
      "run 95, Training loss: 0.27021407132798975, run on the CPU with time 0.0640795000072103\n",
      "run 96, Training loss: 0.2687308176674626, run on the CPU with time 0.06326960001024418\n",
      "run 97, Training loss: 0.26676837321032176, run on the CPU with time 0.08258909999858588\n",
      "run 98, Training loss: 0.265024173496799, run on the CPU with time 0.07959149999078363\n",
      "run 99, Training loss: 0.2654730655252934, run on the CPU with time 0.06641279999166727\n",
      "run 100, Training loss: 0.26332415579394863, run on the CPU with time 0.0659876000136137\n",
      "run 101, Training loss: 0.26243369741873307, run on the CPU with time 0.06520379998255521\n",
      "run 102, Training loss: 0.26112021492286164, run on the CPU with time 0.06492759997490793\n",
      "run 103, Training loss: 0.2603145163844932, run on the CPU with time 0.06322090001776814\n",
      "run 104, Training loss: 0.2590466825630177, run on the CPU with time 0.0612805999990087\n",
      "run 105, Training loss: 0.2585932892154564, run on the CPU with time 0.06312780000735074\n",
      "run 106, Training loss: 0.25802395127036354, run on the CPU with time 0.0651992000057362\n",
      "run 107, Training loss: 0.259215358780189, run on the CPU with time 0.07678830000804737\n",
      "run 108, Training loss: 0.25750734359025956, run on the CPU with time 0.07364940000115894\n",
      "run 109, Training loss: 0.25503768771886826, run on the CPU with time 0.07179289998020977\n",
      "run 110, Training loss: 0.25333129235289314, run on the CPU with time 0.06705929999588989\n",
      "run 111, Training loss: 0.252831031517549, run on the CPU with time 0.0633213999972213\n",
      "run 112, Training loss: 0.25269389660520986, run on the CPU with time 0.06482939998386428\n",
      "run 113, Training loss: 0.2521295448595827, run on the CPU with time 0.06564169999910519\n",
      "run 114, Training loss: 0.25203821435570717, run on the CPU with time 0.06626210000831634\n",
      "run 115, Training loss: 0.2504931332035498, run on the CPU with time 0.07508099998813123\n",
      "run 116, Training loss: 0.2505255191840909, run on the CPU with time 0.0798998000100255\n",
      "run 117, Training loss: 0.2468623392961242, run on the CPU with time 0.07251839997479692\n",
      "run 118, Training loss: 0.2490739795294675, run on the CPU with time 0.08256879998953082\n",
      "run 119, Training loss: 0.2469621857458895, run on the CPU with time 0.07874749999609776\n",
      "run 120, Training loss: 0.24556931175968863, run on the CPU with time 0.0668596999894362\n",
      "run 121, Training loss: 0.2467777818441391, run on the CPU with time 0.08047330001136288\n",
      "run 122, Training loss: 0.2435187004506588, run on the CPU with time 0.07932160000200383\n",
      "run 123, Training loss: 0.2436137040230361, run on the CPU with time 0.07882930000778288\n",
      "run 124, Training loss: 0.24489808096127078, run on the CPU with time 0.0748609000002034\n",
      "run 125, Training loss: 0.24387688359076326, run on the CPU with time 0.06776669999817386\n",
      "run 126, Training loss: 0.2403751124712554, run on the CPU with time 0.06464230001438409\n",
      "run 127, Training loss: 0.23909164104949343, run on the CPU with time 0.0648667000059504\n",
      "run 128, Training loss: 0.2401792668483474, run on the CPU with time 0.062109699996653944\n",
      "run 129, Training loss: 0.23929270776835354, run on the CPU with time 0.06341380000230856\n",
      "run 130, Training loss: 0.23764554932713508, run on the CPU with time 0.06486969999969006\n",
      "run 131, Training loss: 0.23817395737225358, run on the CPU with time 0.07389820000389591\n",
      "run 132, Training loss: 0.23497239662842317, run on the CPU with time 0.06958020001184195\n",
      "run 133, Training loss: 0.23712719367309051, run on the CPU with time 0.06803120000404306\n",
      "run 134, Training loss: 0.23471922461282124, run on the CPU with time 0.06280660000629723\n",
      "run 135, Training loss: 0.23387687714262442, run on the CPU with time 0.0629870000120718\n",
      "run 136, Training loss: 0.23232058357786048, run on the CPU with time 0.0631299999949988\n",
      "run 137, Training loss: 0.2316325336017392, run on the CPU with time 0.06638840000960045\n",
      "run 138, Training loss: 0.23311030796983026, run on the CPU with time 0.06894969998393208\n",
      "run 139, Training loss: 0.23293267197229645, run on the CPU with time 0.08486460000858642\n",
      "run 140, Training loss: 0.23039208819920368, run on the CPU with time 0.07042309999815188\n",
      "run 141, Training loss: 0.22906013977798548, run on the CPU with time 0.07211330000427552\n",
      "run 142, Training loss: 0.22812236229127103, run on the CPU with time 0.0641165999986697\n",
      "run 143, Training loss: 0.22710270901972598, run on the CPU with time 0.06474279999383725\n",
      "run 144, Training loss: 0.23001220287247137, run on the CPU with time 0.06211170001188293\n",
      "run 145, Training loss: 0.22654859406704253, run on the CPU with time 0.061589599994476885\n",
      "run 146, Training loss: 0.2254542469301007, run on the CPU with time 0.06226879998575896\n",
      "run 147, Training loss: 0.2257138098844073, run on the CPU with time 0.06281040000612848\n",
      "run 148, Training loss: 0.22328655035658315, run on the CPU with time 0.07710309998947196\n",
      "run 149, Training loss: 0.22414031597700987, run on the CPU with time 0.07375369998044334\n",
      "run 150, Training loss: 0.22465222240848975, run on the CPU with time 0.07044089998817071\n",
      "run 151, Training loss: 0.22129074619574982, run on the CPU with time 0.06518460001097992\n",
      "run 152, Training loss: 0.22115853005512195, run on the CPU with time 0.06367150001460686\n",
      "run 153, Training loss: 0.22001556581394238, run on the CPU with time 0.06551700001000427\n",
      "run 154, Training loss: 0.21950675249099733, run on the CPU with time 0.06475180000416003\n",
      "run 155, Training loss: 0.21877048564228144, run on the CPU with time 0.06782590001239441\n",
      "run 156, Training loss: 0.22149791121482848, run on the CPU with time 0.06298009998863563\n",
      "run 157, Training loss: 0.2174570480530912, run on the CPU with time 0.06563299999106675\n",
      "run 158, Training loss: 0.21903745206919584, run on the CPU with time 0.0656402999884449\n",
      "run 159, Training loss: 0.21595817811109802, run on the CPU with time 0.06282140000257641\n",
      "run 160, Training loss: 0.21451288956132802, run on the CPU with time 0.06897580000804737\n",
      "run 161, Training loss: 0.21559289449995214, run on the CPU with time 0.08110059998580255\n",
      "run 162, Training loss: 0.21271729171276094, run on the CPU with time 0.06561259998125024\n",
      "run 163, Training loss: 0.21400660127401352, run on the CPU with time 0.06510060001164675\n",
      "run 164, Training loss: 0.21181941276246852, run on the CPU with time 0.06462289998307824\n",
      "run 165, Training loss: 0.21073072722012345, run on the CPU with time 0.06692260000272654\n",
      "run 166, Training loss: 0.21047392413020133, run on the CPU with time 0.06232409999938682\n",
      "run 167, Training loss: 0.20923837240446697, run on the CPU with time 0.0656643999973312\n",
      "run 168, Training loss: 0.20854459205134349, run on the CPU with time 0.064280399994459\n",
      "run 169, Training loss: 0.20777918080037291, run on the CPU with time 0.06359780000639148\n",
      "run 170, Training loss: 0.2071565991775556, run on the CPU with time 0.0663252999947872\n",
      "run 171, Training loss: 0.20743911591443148, run on the CPU with time 0.07522339999559335\n",
      "run 172, Training loss: 0.2071619103578004, run on the CPU with time 0.06504359998507425\n",
      "run 173, Training loss: 0.20656982206485489, run on the CPU with time 0.06715980000444688\n",
      "run 174, Training loss: 0.2077113741839474, run on the CPU with time 0.06736369998543523\n",
      "run 175, Training loss: 0.2044723615050316, run on the CPU with time 0.0651038000069093\n",
      "run 176, Training loss: 0.2046631747348742, run on the CPU with time 0.06393669999670237\n",
      "run 177, Training loss: 0.2035799338058992, run on the CPU with time 0.06216099997982383\n",
      "run 178, Training loss: 0.20082130587913774, run on the CPU with time 0.06245279998984188\n",
      "run 179, Training loss: 0.19976795326749033, run on the CPU with time 0.06240400002570823\n",
      "run 180, Training loss: 0.20100773627107793, run on the CPU with time 0.06684569999924861\n",
      "run 181, Training loss: 0.19854844994843007, run on the CPU with time 0.06428489999962039\n",
      "run 182, Training loss: 0.1994903771037405, run on the CPU with time 0.07774320000316948\n",
      "run 183, Training loss: 0.19778126383369618, run on the CPU with time 0.06917570001678541\n",
      "run 184, Training loss: 0.1968985217538747, run on the CPU with time 0.06426959999953397\n",
      "run 185, Training loss: 0.19780008217150516, run on the CPU with time 0.06634150000172667\n",
      "run 186, Training loss: 0.19685683717781846, run on the CPU with time 0.06500669999513775\n",
      "run 187, Training loss: 0.19376349327239123, run on the CPU with time 0.06463529999018647\n",
      "run 188, Training loss: 0.1932985902509906, run on the CPU with time 0.06266100000357255\n",
      "run 189, Training loss: 0.19655533907088366, run on the CPU with time 0.06192920001922175\n",
      "run 190, Training loss: 0.19478391266681933, run on the CPU with time 0.06206269998801872\n",
      "run 191, Training loss: 0.1935707026584582, run on the CPU with time 0.0919339999963995\n",
      "run 192, Training loss: 0.19188150245357644, run on the CPU with time 0.0672809999960009\n",
      "run 193, Training loss: 0.19141548411412673, run on the CPU with time 0.06339769999613054\n",
      "run 194, Training loss: 0.18959279771555554, run on the CPU with time 0.06353919999673963\n",
      "run 195, Training loss: 0.18959993110461668, run on the CPU with time 0.07068110001273453\n",
      "run 196, Training loss: 0.18727031085978854, run on the CPU with time 0.061851800011936575\n",
      "run 197, Training loss: 0.18941918340596287, run on the CPU with time 0.06826030000229366\n",
      "run 198, Training loss: 0.1877271920442581, run on the CPU with time 0.0642525999865029\n",
      "run 199, Training loss: 0.18623277467082847, run on the CPU with time 0.0636823000095319\n",
      "run 200, Training loss: 0.18590532073920424, run on the CPU with time 0.0784194000007119\n",
      "run 201, Training loss: 0.18373480540784923, run on the CPU with time 0.06535339998663403\n",
      "run 202, Training loss: 0.1828635343096473, run on the CPU with time 0.06551059999037534\n",
      "run 203, Training loss: 0.18378660556944934, run on the CPU with time 0.06377049998263828\n",
      "run 204, Training loss: 0.18558371446349403, run on the CPU with time 0.061961500003235415\n",
      "run 205, Training loss: 0.18322763575071638, run on the CPU with time 0.06076669998583384\n",
      "run 206, Training loss: 0.1832749522545121, run on the CPU with time 0.061078200000338256\n",
      "run 207, Training loss: 0.17993129915134473, run on the CPU with time 0.06380639999406412\n",
      "run 208, Training loss: 0.1788905493915081, run on the CPU with time 0.0628281999961473\n",
      "run 209, Training loss: 0.17920324517921968, run on the CPU with time 0.0697263999900315\n",
      "run 210, Training loss: 0.17913944717835295, run on the CPU with time 0.07464939999044873\n",
      "run 211, Training loss: 0.17532951323823495, run on the CPU with time 0.06996110000181943\n",
      "run 212, Training loss: 0.17732787474312567, run on the CPU with time 0.06096460000844672\n",
      "run 213, Training loss: 0.17820294374092058, run on the CPU with time 0.06198599998606369\n",
      "run 214, Training loss: 0.18014790618961507, run on the CPU with time 0.061752400011755526\n",
      "run 215, Training loss: 0.17643945525315674, run on the CPU with time 0.06152769998880103\n",
      "run 216, Training loss: 0.17473509633405643, run on the CPU with time 0.061752600013278425\n",
      "run 217, Training loss: 0.17374054759063504, run on the CPU with time 0.07041129999561235\n",
      "run 218, Training loss: 0.17536720369349826, run on the CPU with time 0.08992689999286085\n",
      "run 219, Training loss: 0.17345778471366927, run on the CPU with time 0.06710909999674186\n",
      "run 220, Training loss: 0.17416304769841107, run on the CPU with time 0.06516490000649355\n",
      "run 221, Training loss: 0.1722198216075247, run on the CPU with time 0.06307279999600723\n",
      "run 222, Training loss: 0.17192239629274064, run on the CPU with time 0.06281709999893792\n",
      "run 223, Training loss: 0.16973905258558014, run on the CPU with time 0.06443709999439307\n",
      "run 224, Training loss: 0.17154251861978662, run on the CPU with time 0.06371250000665896\n",
      "run 225, Training loss: 0.16934469000182367, run on the CPU with time 0.07484110002405941\n",
      "run 226, Training loss: 0.16820794161070476, run on the CPU with time 0.07348580000689253\n",
      "run 227, Training loss: 0.1689309572293, run on the CPU with time 0.0662536000018008\n",
      "run 228, Training loss: 0.1652524546804753, run on the CPU with time 0.0660869000130333\n",
      "run 229, Training loss: 0.165862946246158, run on the CPU with time 0.06323739999788813\n",
      "run 230, Training loss: 0.16487156406722286, run on the CPU with time 0.06411259999731556\n",
      "run 231, Training loss: 0.16593937785788015, run on the CPU with time 0.06367790000513196\n",
      "run 232, Training loss: 0.16597292856736617, run on the CPU with time 0.07115770000382327\n",
      "run 233, Training loss: 0.1626255238936706, run on the CPU with time 0.08434040000429377\n",
      "run 234, Training loss: 0.16459985653107817, run on the CPU with time 0.06758329999865964\n",
      "run 235, Training loss: 0.16357062926346605, run on the CPU with time 0.07156079998821951\n",
      "run 236, Training loss: 0.16133968230675566, run on the CPU with time 0.06768020000890829\n",
      "run 237, Training loss: 0.15902427075938744, run on the CPU with time 0.06726839998736978\n",
      "run 238, Training loss: 0.16128135008568115, run on the CPU with time 0.06395730000804178\n",
      "run 239, Training loss: 0.1619001656770706, run on the CPU with time 0.06287600001087412\n",
      "run 240, Training loss: 0.1593581981618296, run on the CPU with time 0.0626963000104297\n",
      "run 241, Training loss: 0.1588710126213052, run on the CPU with time 0.06538789998739958\n",
      "run 242, Training loss: 0.1589664811268449, run on the CPU with time 0.07323360000737011\n",
      "run 243, Training loss: 0.15868251001970335, run on the CPU with time 0.08532850001938641\n",
      "run 244, Training loss: 0.16131637120111422, run on the CPU with time 0.07569620001595467\n",
      "run 245, Training loss: 0.15444424233653328, run on the CPU with time 0.06589150000945665\n",
      "run 246, Training loss: 0.15565602786161684, run on the CPU with time 0.06528430001344532\n",
      "run 247, Training loss: 0.15674530376087537, run on the CPU with time 0.06150270000216551\n",
      "run 248, Training loss: 0.15674612396820026, run on the CPU with time 0.06366800001705997\n",
      "run 249, Training loss: 0.1545285905626687, run on the CPU with time 0.062382400006754324\n",
      "run 250, Training loss: 0.15203500207174908, run on the CPU with time 0.062131800019415095\n",
      "run 251, Training loss: 0.15417257869108156, run on the CPU with time 0.06336530001135543\n",
      "run 252, Training loss: 0.1510681913657622, run on the CPU with time 0.08581750001758337\n",
      "run 253, Training loss: 0.15277524383907967, run on the CPU with time 0.07171649998053908\n",
      "run 254, Training loss: 0.15134450867772103, run on the CPU with time 0.06449069999507628\n",
      "run 255, Training loss: 0.15044158509170466, run on the CPU with time 0.0640817999956198\n",
      "run 256, Training loss: 0.14893981516361238, run on the CPU with time 0.06565179998870008\n",
      "run 257, Training loss: 0.14962122132155029, run on the CPU with time 0.06363749998854473\n",
      "run 258, Training loss: 0.14841601767323234, run on the CPU with time 0.06279009999707341\n",
      "run 259, Training loss: 0.1462890038774772, run on the CPU with time 0.06206009999732487\n",
      "run 260, Training loss: 0.14615825333378532, run on the CPU with time 0.07174320000922307\n",
      "run 261, Training loss: 0.1463593782687729, run on the CPU with time 0.08962190002785064\n",
      "run 262, Training loss: 0.1456417176533829, run on the CPU with time 0.06934769998770207\n",
      "run 263, Training loss: 0.14677335481074724, run on the CPU with time 0.07031569999526255\n",
      "run 264, Training loss: 0.1431457663124258, run on the CPU with time 0.06832210000720806\n",
      "run 265, Training loss: 0.14400346292690797, run on the CPU with time 0.06330290000187233\n",
      "run 266, Training loss: 0.14385575245727192, run on the CPU with time 0.06457090002368204\n",
      "run 267, Training loss: 0.14357699123634535, run on the CPU with time 0.06301600000006147\n",
      "run 268, Training loss: 0.14208268844945865, run on the CPU with time 0.06370329999481328\n",
      "run 269, Training loss: 0.1435040416703983, run on the CPU with time 0.06562050001230091\n",
      "run 270, Training loss: 0.14417168169536373, run on the CPU with time 0.06621619997895323\n",
      "run 271, Training loss: 0.14095981261608276, run on the CPU with time 0.0862355999997817\n",
      "run 272, Training loss: 0.14266578072512692, run on the CPU with time 0.0671633999736514\n",
      "run 273, Training loss: 0.14098008864305236, run on the CPU with time 0.062483200017595664\n",
      "run 274, Training loss: 0.1385538266802376, run on the CPU with time 0.06656340000336058\n",
      "run 275, Training loss: 0.13958258845589377, run on the CPU with time 0.0655187000229489\n",
      "run 276, Training loss: 0.135923493179408, run on the CPU with time 0.0636664999765344\n",
      "run 277, Training loss: 0.1372747106308287, run on the CPU with time 0.06720749998930842\n",
      "run 278, Training loss: 0.13575058213689112, run on the CPU with time 0.06569909999961965\n",
      "run 279, Training loss: 0.13623252793807875, run on the CPU with time 0.06418329998268746\n",
      "run 280, Training loss: 0.1367497380314903, run on the CPU with time 0.06267280000611208\n",
      "run 281, Training loss: 0.13669227459891276, run on the CPU with time 0.06199580000247806\n",
      "run 282, Training loss: 0.1320435896346515, run on the CPU with time 0.06270740000763908\n",
      "run 283, Training loss: 0.13361980027773163, run on the CPU with time 0.06739600002765656\n",
      "run 284, Training loss: 0.13552504747428679, run on the CPU with time 0.08962529999553226\n",
      "run 285, Training loss: 0.1342580919238654, run on the CPU with time 0.06654720002552494\n",
      "run 286, Training loss: 0.13203479400412602, run on the CPU with time 0.0663199000118766\n",
      "run 287, Training loss: 0.13296845372427593, run on the CPU with time 0.07936299999710172\n",
      "run 288, Training loss: 0.1321681735529141, run on the CPU with time 0.0660498000215739\n",
      "run 289, Training loss: 0.12942956110293216, run on the CPU with time 0.06892960000550374\n",
      "run 290, Training loss: 0.13008855887773363, run on the CPU with time 0.06339049999951385\n",
      "run 291, Training loss: 0.12864991267296402, run on the CPU with time 0.06898969999747351\n",
      "run 292, Training loss: 0.13028362569483845, run on the CPU with time 0.07201189998886548\n",
      "run 293, Training loss: 0.12862069088626993, run on the CPU with time 0.09000550000928342\n",
      "run 294, Training loss: 0.13062833969227292, run on the CPU with time 0.0654536000220105\n",
      "run 295, Training loss: 0.12761631445451216, run on the CPU with time 0.07051370001863688\n",
      "run 296, Training loss: 0.12889060760763557, run on the CPU with time 0.07204460000502877\n",
      "run 297, Training loss: 0.12861390037631446, run on the CPU with time 0.07204429997364059\n",
      "run 298, Training loss: 0.1270026788454164, run on the CPU with time 0.06527859999914654\n",
      "run 299, Training loss: 0.1274466087533669, run on the CPU with time 0.07534739997936413\n",
      "run 300, Training loss: 0.12755625271661716, run on the CPU with time 0.09176700000534765\n",
      "run 301, Training loss: 0.12400182150304317, run on the CPU with time 0.07946159999119118\n",
      "run 302, Training loss: 0.12489308301698078, run on the CPU with time 0.089857100014342\n",
      "run 303, Training loss: 0.12307597517289899, run on the CPU with time 0.06635739997727796\n",
      "run 304, Training loss: 0.1244644372639331, run on the CPU with time 0.06615650001913309\n",
      "run 305, Training loss: 0.12227087793025103, run on the CPU with time 0.06944719998864457\n",
      "run 306, Training loss: 0.12129332565448502, run on the CPU with time 0.06360039999708533\n",
      "run 307, Training loss: 0.12298049825158987, run on the CPU with time 0.06322420001379214\n",
      "run 308, Training loss: 0.12134240499951622, run on the CPU with time 0.06341679999604821\n",
      "run 309, Training loss: 0.1219894623553211, run on the CPU with time 0.06463730000541545\n",
      "run 310, Training loss: 0.11997537288222124, run on the CPU with time 0.08422860002610832\n",
      "run 311, Training loss: 0.1191410094499588, run on the CPU with time 0.06884469999931753\n",
      "run 312, Training loss: 0.12157221973281016, run on the CPU with time 0.07329450000543147\n",
      "run 313, Training loss: 0.11764747429300439, run on the CPU with time 0.06797130001359619\n",
      "run 314, Training loss: 0.11785451981493018, run on the CPU with time 0.0684719999844674\n",
      "run 315, Training loss: 0.11674380834129723, run on the CPU with time 0.0687292999937199\n",
      "run 316, Training loss: 0.11957921998744661, run on the CPU with time 0.0648108999885153\n",
      "run 317, Training loss: 0.11574839314615185, run on the CPU with time 0.08093820000067353\n",
      "run 318, Training loss: 0.11581326243890958, run on the CPU with time 0.08114179997937754\n",
      "run 319, Training loss: 0.11337015346031297, run on the CPU with time 0.06805460000759922\n",
      "run 320, Training loss: 0.11819702178578485, run on the CPU with time 0.06531629999517463\n",
      "run 321, Training loss: 0.11437613491646269, run on the CPU with time 0.07023909999406897\n",
      "run 322, Training loss: 0.1142562316561287, run on the CPU with time 0.06577970000216737\n",
      "run 323, Training loss: 0.11395750939846039, run on the CPU with time 0.06296430001384579\n",
      "run 324, Training loss: 0.11407271786169572, run on the CPU with time 0.07049700000789016\n",
      "run 325, Training loss: 0.11106545349413699, run on the CPU with time 0.07139239998650737\n",
      "run 326, Training loss: 0.1129969930339774, run on the CPU with time 0.0637072000245098\n",
      "run 327, Training loss: 0.11237572887065736, run on the CPU with time 0.06790640001418069\n",
      "run 328, Training loss: 0.11173432379622351, run on the CPU with time 0.06414419997599907\n",
      "run 329, Training loss: 0.1121002972972664, run on the CPU with time 0.06376590000581928\n",
      "run 330, Training loss: 0.11035223404284228, run on the CPU with time 0.06855239998549223\n",
      "run 331, Training loss: 0.1087952758608894, run on the CPU with time 0.08110889999079518\n",
      "run 332, Training loss: 0.11040183723142201, run on the CPU with time 0.06528320000506938\n",
      "run 333, Training loss: 0.11132301032881844, run on the CPU with time 0.0644142999954056\n",
      "run 334, Training loss: 0.10684976388107646, run on the CPU with time 0.06601169999339618\n",
      "run 335, Training loss: 0.10886430896141312, run on the CPU with time 0.06375620001927018\n",
      "run 336, Training loss: 0.1063650671731342, run on the CPU with time 0.06534169998485595\n",
      "run 337, Training loss: 0.10728041089393876, run on the CPU with time 0.06887689998256974\n",
      "run 338, Training loss: 0.10628638692538847, run on the CPU with time 0.06306750001385808\n",
      "run 339, Training loss: 0.10572851923379031, run on the CPU with time 0.07262069999706\n",
      "run 340, Training loss: 0.10483855010772293, run on the CPU with time 0.08081340001081116\n",
      "run 341, Training loss: 0.10291055456989191, run on the CPU with time 0.07494449999649078\n",
      "run 342, Training loss: 0.10560379946096377, run on the CPU with time 0.0708407000056468\n",
      "run 343, Training loss: 0.10891928031024607, run on the CPU with time 0.06957729998975992\n",
      "run 344, Training loss: 0.10574910481545058, run on the CPU with time 0.06728909999947064\n",
      "run 345, Training loss: 0.10474614909934726, run on the CPU with time 0.06874879999668337\n",
      "run 346, Training loss: 0.10306963802061297, run on the CPU with time 0.07139269998879172\n",
      "run 347, Training loss: 0.10044311012395403, run on the CPU with time 0.06448480000835843\n",
      "run 348, Training loss: 0.10168723140589216, run on the CPU with time 0.0655661000055261\n",
      "run 349, Training loss: 0.09937499268827113, run on the CPU with time 0.06393409997690469\n",
      "run 350, Training loss: 0.10280968303030187, run on the CPU with time 0.06553299998631701\n",
      "run 351, Training loss: 0.0989567517387596, run on the CPU with time 0.06605719999060966\n",
      "run 352, Training loss: 0.10127408235249194, run on the CPU with time 0.07674240000778809\n",
      "run 353, Training loss: 0.0996897078203884, run on the CPU with time 0.08469679998233914\n",
      "run 354, Training loss: 0.1007651016454805, run on the CPU with time 0.06523839998408221\n",
      "run 355, Training loss: 0.09777354033325206, run on the CPU with time 0.06785049999598414\n",
      "run 356, Training loss: 0.09565884573808447, run on the CPU with time 0.06660200000624172\n",
      "run 357, Training loss: 0.09746894787319682, run on the CPU with time 0.06201850000070408\n",
      "run 358, Training loss: 0.0990146289664236, run on the CPU with time 0.07095439999829978\n",
      "run 359, Training loss: 0.09988062593408606, run on the CPU with time 0.0724839000031352\n",
      "run 360, Training loss: 0.09704357698898423, run on the CPU with time 0.07877819999703206\n",
      "run 361, Training loss: 0.09539071956056763, run on the CPU with time 0.06699049999588169\n",
      "run 362, Training loss: 0.09661278413100675, run on the CPU with time 0.07057139999233186\n",
      "run 363, Training loss: 0.09594987327740952, run on the CPU with time 0.06476809998275712\n",
      "run 364, Training loss: 0.0975983843038028, run on the CPU with time 0.06501439999556169\n",
      "run 365, Training loss: 0.09552344786511226, run on the CPU with time 0.06590429999050684\n",
      "run 366, Training loss: 0.09412227024070241, run on the CPU with time 0.07995489999302663\n",
      "run 367, Training loss: 0.09414878137071024, run on the CPU with time 0.06570279999868944\n",
      "run 368, Training loss: 0.0919824068011208, run on the CPU with time 0.06897209997987375\n",
      "run 369, Training loss: 0.09383837026967244, run on the CPU with time 0.0691190000216011\n",
      "run 370, Training loss: 0.09573938319967552, run on the CPU with time 0.06829769999603741\n",
      "run 371, Training loss: 0.09373789356852119, run on the CPU with time 0.06682690000161529\n",
      "run 372, Training loss: 0.09013922670009461, run on the CPU with time 0.06808339999406599\n",
      "run 373, Training loss: 0.09050052897919986, run on the CPU with time 0.06515579999540932\n",
      "run 374, Training loss: 0.09005358363078399, run on the CPU with time 0.06755880001583137\n",
      "run 375, Training loss: 0.0892807905562222, run on the CPU with time 0.10233910000533797\n",
      "run 376, Training loss: 0.09051292017948899, run on the CPU with time 0.07803440000861883\n",
      "run 377, Training loss: 0.08877506449141286, run on the CPU with time 0.06679350000922568\n",
      "run 378, Training loss: 0.0901978877182542, run on the CPU with time 0.06510229999548756\n",
      "run 379, Training loss: 0.08811803898689421, run on the CPU with time 0.06635420001111925\n",
      "run 380, Training loss: 0.08731777017555115, run on the CPU with time 0.06337330001406372\n",
      "run 381, Training loss: 0.0859995282001116, run on the CPU with time 0.0636911999899894\n",
      "run 382, Training loss: 0.08647560731888833, run on the CPU with time 0.06267549999756739\n",
      "run 383, Training loss: 0.08732556254518303, run on the CPU with time 0.06319970000186004\n",
      "run 384, Training loss: 0.08635982750830325, run on the CPU with time 0.0861972999991849\n",
      "run 385, Training loss: 0.08483061808669431, run on the CPU with time 0.07279700000071898\n",
      "run 386, Training loss: 0.08517212820324031, run on the CPU with time 0.08242589997826144\n",
      "run 387, Training loss: 0.08491843615404585, run on the CPU with time 0.06453920001513325\n",
      "run 388, Training loss: 0.08691839427602562, run on the CPU with time 0.0708277000230737\n",
      "run 389, Training loss: 0.08490984289991585, run on the CPU with time 0.06615009999950416\n",
      "run 390, Training loss: 0.08422563735565002, run on the CPU with time 0.06603479999466799\n",
      "run 391, Training loss: 0.0865619657561183, run on the CPU with time 0.06594020000193268\n",
      "run 392, Training loss: 0.08378284781832587, run on the CPU with time 0.0654742999759037\n",
      "run 393, Training loss: 0.08367003974589435, run on the CPU with time 0.07074600001215003\n",
      "run 394, Training loss: 0.08162762436338446, run on the CPU with time 0.0955305999959819\n",
      "run 395, Training loss: 0.08347500547428023, run on the CPU with time 0.0683949000085704\n",
      "run 396, Training loss: 0.08182542869736525, run on the CPU with time 0.06339259998640046\n",
      "run 397, Training loss: 0.08030072491277349, run on the CPU with time 0.06446409999625757\n",
      "run 398, Training loss: 0.08083493828096174, run on the CPU with time 0.06845859999884851\n",
      "run 399, Training loss: 0.07924196919934316, run on the CPU with time 0.06764670001575723\n",
      "run 400, Training loss: 0.08056880471042611, run on the CPU with time 0.06843059998936951\n",
      "run 401, Training loss: 0.07913951806046746, run on the CPU with time 0.06472960000974126\n",
      "run 402, Training loss: 0.07897116003388709, run on the CPU with time 0.06744030001573265\n",
      "run 403, Training loss: 0.07852554832669821, run on the CPU with time 0.08629870001459494\n",
      "run 404, Training loss: 0.0797516304152933, run on the CPU with time 0.06661479998729192\n",
      "run 405, Training loss: 0.08216978505423123, run on the CPU with time 0.06384550000075251\n",
      "run 406, Training loss: 0.07937696281481874, run on the CPU with time 0.06325929999002255\n",
      "run 407, Training loss: 0.07515407694842327, run on the CPU with time 0.06878830000641756\n",
      "run 408, Training loss: 0.07888108356432481, run on the CPU with time 0.0655067999905441\n",
      "run 409, Training loss: 0.07807539414868436, run on the CPU with time 0.06627110001863912\n",
      "run 410, Training loss: 0.07931977160782977, run on the CPU with time 0.06820820001303218\n",
      "run 411, Training loss: 0.0737078943120485, run on the CPU with time 0.06885750000947155\n",
      "run 412, Training loss: 0.07437037533487786, run on the CPU with time 0.06566530000418425\n",
      "run 413, Training loss: 0.07628476574847644, run on the CPU with time 0.07353770002373494\n",
      "run 414, Training loss: 0.0717305229214782, run on the CPU with time 0.08015969998086803\n",
      "run 415, Training loss: 0.07396602619608694, run on the CPU with time 0.08418119998532347\n",
      "run 416, Training loss: 0.07196741483428261, run on the CPU with time 0.06893310000305064\n",
      "run 417, Training loss: 0.0730626678602262, run on the CPU with time 0.06926319998456165\n",
      "run 418, Training loss: 0.07424546500498598, run on the CPU with time 0.06717519997619092\n",
      "run 419, Training loss: 0.07543998096476902, run on the CPU with time 0.06484950002050027\n",
      "run 420, Training loss: 0.07238072076579556, run on the CPU with time 0.06337090002489276\n",
      "run 421, Training loss: 0.07190709281289442, run on the CPU with time 0.06361670000478625\n",
      "run 422, Training loss: 0.07340780747631057, run on the CPU with time 0.07446239999262616\n",
      "run 423, Training loss: 0.07263086072423242, run on the CPU with time 0.07303130000946112\n",
      "run 424, Training loss: 0.07183299123055556, run on the CPU with time 0.06621910000103526\n",
      "run 425, Training loss: 0.06976702343672514, run on the CPU with time 0.07073039998067543\n",
      "run 426, Training loss: 0.07122457475740124, run on the CPU with time 0.06943110001157038\n",
      "run 427, Training loss: 0.06533389296382666, run on the CPU with time 0.06539619999239221\n",
      "run 428, Training loss: 0.07063606518574736, run on the CPU with time 0.06572210000013001\n",
      "run 429, Training loss: 0.06986487286842682, run on the CPU with time 0.07655359999625944\n",
      "run 430, Training loss: 0.06715079775418747, run on the CPU with time 0.06922550001763739\n",
      "run 431, Training loss: 0.07020049060440876, run on the CPU with time 0.07924050002475269\n",
      "run 432, Training loss: 0.06910237600518898, run on the CPU with time 0.06842240001424216\n",
      "run 433, Training loss: 0.06659185250967063, run on the CPU with time 0.06514710001647472\n",
      "run 434, Training loss: 0.07148850490762429, run on the CPU with time 0.06944739999016747\n",
      "run 435, Training loss: 0.0713429295373234, run on the CPU with time 0.06435899998177774\n",
      "run 436, Training loss: 0.06743090102787722, run on the CPU with time 0.06462190000456758\n",
      "run 437, Training loss: 0.06627148935883517, run on the CPU with time 0.0675700000138022\n",
      "run 438, Training loss: 0.06489197911525314, run on the CPU with time 0.06449140000040643\n",
      "run 439, Training loss: 0.0688197948588905, run on the CPU with time 0.06357779999962077\n",
      "run 440, Training loss: 0.06389032301323658, run on the CPU with time 0.069977100007236\n",
      "run 441, Training loss: 0.06335945994254541, run on the CPU with time 0.08654369998839684\n",
      "run 442, Training loss: 0.06347826975821094, run on the CPU with time 0.08212690000073053\n",
      "run 443, Training loss: 0.0639506267807023, run on the CPU with time 0.07178580001345836\n",
      "run 444, Training loss: 0.06204372504726052, run on the CPU with time 0.06645519999437965\n",
      "run 445, Training loss: 0.0636993696405129, run on the CPU with time 0.0636141000140924\n",
      "run 446, Training loss: 0.06618518419987099, run on the CPU with time 0.0629905000096187\n",
      "run 447, Training loss: 0.06431051031263037, run on the CPU with time 0.06307599999126978\n",
      "run 448, Training loss: 0.06107953985814344, run on the CPU with time 0.06267250000382774\n",
      "run 449, Training loss: 0.06291750494909304, run on the CPU with time 0.09363849999499507\n",
      "run 450, Training loss: 0.0612554567696696, run on the CPU with time 0.06984949999605305\n",
      "run 451, Training loss: 0.05980336009948091, run on the CPU with time 0.06745830000727437\n",
      "run 452, Training loss: 0.06035481344866143, run on the CPU with time 0.06452380001428537\n",
      "run 453, Training loss: 0.05919854482635856, run on the CPU with time 0.07200569999986328\n",
      "run 454, Training loss: 0.06311157530342991, run on the CPU with time 0.06374159999541007\n",
      "run 455, Training loss: 0.05950749401341785, run on the CPU with time 0.06461470000795089\n",
      "run 456, Training loss: 0.059483582323247734, run on the CPU with time 0.07093460002215579\n",
      "run 457, Training loss: 0.0599795505235141, run on the CPU with time 0.06361340000876226\n",
      "run 458, Training loss: 0.05645820466716858, run on the CPU with time 0.08551739997346886\n",
      "run 459, Training loss: 0.05830132925713604, run on the CPU with time 0.0789877999923192\n",
      "run 460, Training loss: 0.059534235603430054, run on the CPU with time 0.06558460000087507\n",
      "run 461, Training loss: 0.0595957987814803, run on the CPU with time 0.0640459000132978\n",
      "run 462, Training loss: 0.05625592702153054, run on the CPU with time 0.071696900005918\n",
      "run 463, Training loss: 0.05618808123452419, run on the CPU with time 0.06414010000298731\n",
      "run 464, Training loss: 0.055480908831073476, run on the CPU with time 0.06343390000984073\n",
      "run 465, Training loss: 0.05841592882590538, run on the CPU with time 0.06275350000942126\n",
      "run 466, Training loss: 0.06195340638302944, run on the CPU with time 0.0618024000141304\n",
      "run 467, Training loss: 0.05516173323955048, run on the CPU with time 0.06327549999696203\n",
      "run 468, Training loss: 0.05399736908095127, run on the CPU with time 0.08585149998543784\n",
      "run 469, Training loss: 0.05437931181354956, run on the CPU with time 0.06825650000246242\n",
      "run 470, Training loss: 0.05201907113660127, run on the CPU with time 0.06545620001270436\n",
      "run 471, Training loss: 0.05156721748750318, run on the CPU with time 0.06392179999966174\n",
      "run 472, Training loss: 0.057578614684329794, run on the CPU with time 0.07350060000317171\n",
      "run 473, Training loss: 0.05429158669642427, run on the CPU with time 0.0655370999884326\n",
      "run 474, Training loss: 0.051583966790613806, run on the CPU with time 0.062185299990233034\n",
      "run 475, Training loss: 0.0526223032227294, run on the CPU with time 0.0714985000086017\n",
      "run 476, Training loss: 0.05327058865011416, run on the CPU with time 0.06910490000154823\n",
      "run 477, Training loss: 0.050289352974769747, run on the CPU with time 0.06443399999989197\n",
      "run 478, Training loss: 0.05212691642174667, run on the CPU with time 0.06723879999481142\n",
      "run 479, Training loss: 0.053887282350015914, run on the CPU with time 0.06535630000871606\n",
      "run 480, Training loss: 0.04965798306194219, run on the CPU with time 0.06672360000084154\n",
      "run 481, Training loss: 0.04758657472292808, run on the CPU with time 0.06403320000390522\n",
      "run 482, Training loss: 0.05081069626930085, run on the CPU with time 0.06290630000876263\n",
      "run 483, Training loss: 0.05117676867680116, run on the CPU with time 0.06727229998796247\n",
      "run 484, Training loss: 0.047005527314137326, run on the CPU with time 0.06807630002731457\n",
      "run 485, Training loss: 0.04924740212207491, run on the CPU with time 0.06906390000949614\n",
      "run 486, Training loss: 0.048983723822642454, run on the CPU with time 0.06897620001109317\n",
      "run 487, Training loss: 0.04895500643602149, run on the CPU with time 0.06288929999573156\n",
      "run 488, Training loss: 0.053277264086698944, run on the CPU with time 0.06412000002455898\n",
      "run 489, Training loss: 0.04940831204876304, run on the CPU with time 0.062149500008672476\n",
      "run 490, Training loss: 0.04619496670466933, run on the CPU with time 0.06239619999541901\n",
      "run 491, Training loss: 0.04570323944261128, run on the CPU with time 0.07229120002011769\n",
      "run 492, Training loss: 0.0457218430809338, run on the CPU with time 0.06506379999336787\n",
      "run 493, Training loss: 0.046615842839872294, run on the CPU with time 0.06388469997909851\n",
      "run 494, Training loss: 0.04567987360060215, run on the CPU with time 0.06226459998288192\n",
      "run 495, Training loss: 0.048233579539439894, run on the CPU with time 0.06249589999788441\n",
      "run 496, Training loss: 0.0464156783524562, run on the CPU with time 0.06313540000701323\n",
      "run 497, Training loss: 0.04417574871331453, run on the CPU with time 0.06152480002492666\n",
      "run 498, Training loss: 0.04767892844128338, run on the CPU with time 0.06135569998878054\n",
      "run 499, Training loss: 0.04689261685972187, run on the CPU with time 0.061853099992731586\n",
      "run 500, Training loss: 0.04384797496975146, run on the CPU with time 0.0633877000072971\n",
      "run 501, Training loss: 0.04595034566115249, run on the CPU with time 0.08586940000532195\n",
      "run 502, Training loss: 0.04341571280648085, run on the CPU with time 0.0644419000018388\n",
      "run 503, Training loss: 0.044022206907075916, run on the CPU with time 0.06437599999480881\n",
      "run 504, Training loss: 0.04431072240695357, run on the CPU with time 0.06473370001185685\n",
      "run 505, Training loss: 0.042715284194458615, run on the CPU with time 0.06230849999701604\n",
      "run 506, Training loss: 0.041569183153015644, run on the CPU with time 0.06201449999934994\n",
      "run 507, Training loss: 0.043016391721638764, run on the CPU with time 0.06351320000248961\n",
      "run 508, Training loss: 0.046300521954386074, run on the CPU with time 0.06139180000172928\n",
      "run 509, Training loss: 0.04212488363581625, run on the CPU with time 0.06135319999884814\n",
      "run 510, Training loss: 0.04296977848068557, run on the CPU with time 0.062157200009096414\n",
      "run 511, Training loss: 0.039398765030571006, run on the CPU with time 0.06496289998176508\n",
      "run 512, Training loss: 0.040561454129320655, run on the CPU with time 0.08847439999226481\n",
      "run 513, Training loss: 0.03965533088723367, run on the CPU with time 0.06888289999915287\n",
      "run 514, Training loss: 0.04027190313843841, run on the CPU with time 0.06397619997733273\n",
      "run 515, Training loss: 0.04086593292992224, run on the CPU with time 0.06449319998500869\n",
      "run 516, Training loss: 0.04093455935574391, run on the CPU with time 0.062139100016793236\n",
      "run 517, Training loss: 0.03900837159122933, run on the CPU with time 0.06321370002115145\n",
      "run 518, Training loss: 0.03849831310481849, run on the CPU with time 0.06189429998630658\n",
      "run 519, Training loss: 0.03849724554033442, run on the CPU with time 0.062008899985812604\n",
      "run 520, Training loss: 0.04003802259791304, run on the CPU with time 0.061492000008001924\n",
      "run 521, Training loss: 0.03953188694590195, run on the CPU with time 0.06251160000101663\n",
      "run 522, Training loss: 0.037269364432855084, run on the CPU with time 0.0722922999993898\n",
      "run 523, Training loss: 0.03891504959372634, run on the CPU with time 0.07417499998700805\n",
      "run 524, Training loss: 0.03832561477002772, run on the CPU with time 0.06315619999077171\n",
      "run 525, Training loss: 0.03740258675745942, run on the CPU with time 0.07250549999298528\n",
      "run 526, Training loss: 0.03917745328084989, run on the CPU with time 0.06350639997981489\n",
      "run 527, Training loss: 0.04146964300254529, run on the CPU with time 0.0622735999932047\n",
      "run 528, Training loss: 0.038261747711592096, run on the CPU with time 0.06233109999448061\n",
      "run 529, Training loss: 0.03712236828340048, run on the CPU with time 0.06176380001124926\n",
      "run 530, Training loss: 0.036587554423815824, run on the CPU with time 0.06171660000109114\n",
      "run 531, Training loss: 0.036784263992343434, run on the CPU with time 0.06161699999938719\n",
      "run 532, Training loss: 0.037803568216887386, run on the CPU with time 0.06280340001103468\n",
      "run 533, Training loss: 0.03445137517357415, run on the CPU with time 0.07538379999459721\n",
      "run 534, Training loss: 0.03551298556426032, run on the CPU with time 0.070379699987825\n",
      "run 535, Training loss: 0.03538801394910975, run on the CPU with time 0.06339620001381263\n",
      "run 536, Training loss: 0.03472676820291037, run on the CPU with time 0.06386749999364838\n",
      "run 537, Training loss: 0.03603259018537673, run on the CPU with time 0.062149599980330095\n",
      "run 538, Training loss: 0.03362937781705775, run on the CPU with time 0.06261930000619031\n",
      "run 539, Training loss: 0.03613160542127761, run on the CPU with time 0.061717100004898384\n",
      "run 540, Training loss: 0.03459446769927375, run on the CPU with time 0.06224830000428483\n",
      "run 541, Training loss: 0.034396851985630666, run on the CPU with time 0.06297040000208654\n",
      "run 542, Training loss: 0.03544757287864658, run on the CPU with time 0.07290179998381063\n",
      "run 543, Training loss: 0.03352423897859725, run on the CPU with time 0.06293149999692105\n",
      "run 544, Training loss: 0.03205012270588089, run on the CPU with time 0.06473510002251714\n",
      "run 545, Training loss: 0.032764102588407694, run on the CPU with time 0.0648345000226982\n",
      "run 546, Training loss: 0.03225672604537315, run on the CPU with time 0.06547770000179298\n",
      "run 547, Training loss: 0.03299706999466501, run on the CPU with time 0.0631610999989789\n",
      "run 548, Training loss: 0.032174692720979114, run on the CPU with time 0.06327729998156428\n",
      "run 549, Training loss: 0.032611534151841294, run on the CPU with time 0.06687670000246726\n",
      "run 550, Training loss: 0.031218831698325546, run on the CPU with time 0.06294480001088232\n",
      "run 551, Training loss: 0.030545352392999287, run on the CPU with time 0.06502120001823641\n",
      "run 552, Training loss: 0.029732146139510654, run on the CPU with time 0.06924159999471158\n",
      "run 553, Training loss: 0.03134259887420657, run on the CPU with time 0.06799650000175461\n",
      "run 554, Training loss: 0.038452498517422516, run on the CPU with time 0.06360940000740811\n",
      "run 555, Training loss: 0.03484751579182392, run on the CPU with time 0.06648110001697205\n",
      "run 556, Training loss: 0.03248897218992087, run on the CPU with time 0.06717580000986345\n",
      "run 557, Training loss: 0.03101013255601918, run on the CPU with time 0.06176939999568276\n",
      "run 558, Training loss: 0.03252959299028258, run on the CPU with time 0.06521000000066124\n",
      "run 559, Training loss: 0.03050801246003671, run on the CPU with time 0.062468699994497\n",
      "run 560, Training loss: 0.03096081699742089, run on the CPU with time 0.06348250000155531\n",
      "run 561, Training loss: 0.030110986717045308, run on the CPU with time 0.062316900002770126\n",
      "run 562, Training loss: 0.03060958666134287, run on the CPU with time 0.06357160001061857\n",
      "run 563, Training loss: 0.0292565595400943, run on the CPU with time 0.07157270002062432\n",
      "run 564, Training loss: 0.02859241617843509, run on the CPU with time 0.09436629997799173\n",
      "run 565, Training loss: 0.029199244088323955, run on the CPU with time 0.07092629998805933\n",
      "run 566, Training loss: 0.03064655663699589, run on the CPU with time 0.06273999999393709\n",
      "run 567, Training loss: 0.0282317132900723, run on the CPU with time 0.06327730001066811\n",
      "run 568, Training loss: 0.030722419782118364, run on the CPU with time 0.06278410000959411\n",
      "run 569, Training loss: 0.029779885279607368, run on the CPU with time 0.06225119999726303\n",
      "run 570, Training loss: 0.029049272257411343, run on the CPU with time 0.06302359999972396\n",
      "run 571, Training loss: 0.02901618576142937, run on the CPU with time 0.061881500005256385\n",
      "run 572, Training loss: 0.028411668983542106, run on the CPU with time 0.06351310000172816\n",
      "run 573, Training loss: 0.026643818166022272, run on the CPU with time 0.07086989999515936\n",
      "run 574, Training loss: 0.02736979819707234, run on the CPU with time 0.07835900000645779\n",
      "run 575, Training loss: 0.02841907058927146, run on the CPU with time 0.06985810000333004\n",
      "run 576, Training loss: 0.025751646974293347, run on the CPU with time 0.06902190000982955\n",
      "run 577, Training loss: 0.026751737178049306, run on the CPU with time 0.06328729999950156\n",
      "run 578, Training loss: 0.02869160847569054, run on the CPU with time 0.06249549999483861\n",
      "run 579, Training loss: 0.02769980978601697, run on the CPU with time 0.06338689997210167\n",
      "run 580, Training loss: 0.026728083858986132, run on the CPU with time 0.06263160001253709\n",
      "run 581, Training loss: 0.029021835736503736, run on the CPU with time 0.06182459997944534\n",
      "run 582, Training loss: 0.0264977919572795, run on the CPU with time 0.06090979999862611\n",
      "run 583, Training loss: 0.02807445300488987, run on the CPU with time 0.06303169997408986\n",
      "run 584, Training loss: 0.02580068041676317, run on the CPU with time 0.07697829999960959\n",
      "run 585, Training loss: 0.026281569445167074, run on the CPU with time 0.07788849997450598\n",
      "run 586, Training loss: 0.02524415926785547, run on the CPU with time 0.069263499986846\n",
      "run 587, Training loss: 0.027064100576734, run on the CPU with time 0.06732570001622662\n",
      "run 588, Training loss: 0.027652950593355027, run on the CPU with time 0.06321789999492466\n",
      "run 589, Training loss: 0.025502236432988535, run on the CPU with time 0.06274510000366718\n",
      "run 590, Training loss: 0.02363084322264926, run on the CPU with time 0.06307249999372289\n",
      "run 591, Training loss: 0.023792786615244536, run on the CPU with time 0.06396940001286566\n",
      "run 592, Training loss: 0.024292247638699006, run on the CPU with time 0.06163440001546405\n",
      "run 593, Training loss: 0.0242900772634047, run on the CPU with time 0.07398070002091117\n",
      "run 594, Training loss: 0.02419407902158458, run on the CPU with time 0.06554989999858662\n",
      "run 595, Training loss: 0.023378094911194323, run on the CPU with time 0.07115589999011718\n",
      "run 596, Training loss: 0.02389917482029308, run on the CPU with time 0.06513299999642186\n",
      "run 597, Training loss: 0.023968251785051756, run on the CPU with time 0.06502509998972528\n",
      "run 598, Training loss: 0.023693764095448634, run on the CPU with time 0.06608839999535121\n",
      "run 599, Training loss: 0.023977853848852895, run on the CPU with time 0.06756870000390336\n",
      "run 600, Training loss: 0.023047597338022155, run on the CPU with time 0.0644668000168167\n",
      "run 601, Training loss: 0.023591379586353222, run on the CPU with time 0.06218409998109564\n",
      "run 602, Training loss: 0.022856391965284606, run on the CPU with time 0.06670349999330938\n",
      "run 603, Training loss: 0.022420006234791467, run on the CPU with time 0.06874309998238459\n",
      "run 604, Training loss: 0.027838988196824423, run on the CPU with time 0.06604530001641251\n",
      "run 605, Training loss: 0.02654844629049132, run on the CPU with time 0.06791869999142364\n",
      "run 606, Training loss: 0.02228826477234675, run on the CPU with time 0.06252050001057796\n",
      "run 607, Training loss: 0.02205962466965006, run on the CPU with time 0.06355069999699481\n",
      "run 608, Training loss: 0.022757435293698853, run on the CPU with time 0.06321180000668392\n",
      "run 609, Training loss: 0.02422840672291138, run on the CPU with time 0.06641859997762367\n",
      "run 610, Training loss: 0.023390361935492942, run on the CPU with time 0.06601109998882748\n",
      "run 611, Training loss: 0.021642048834738407, run on the CPU with time 0.06819579997682013\n",
      "run 612, Training loss: 0.021471109342846004, run on the CPU with time 0.06370610001613386\n",
      "run 613, Training loss: 0.0242423787268556, run on the CPU with time 0.06221120001282543\n",
      "run 614, Training loss: 0.029638288709843023, run on the CPU with time 0.06343870001728646\n",
      "run 615, Training loss: 0.02168719355778938, run on the CPU with time 0.06713569999556057\n",
      "run 616, Training loss: 0.021077733328142625, run on the CPU with time 0.06539329999941401\n",
      "run 617, Training loss: 0.02065735245546834, run on the CPU with time 0.06539110001176596\n",
      "run 618, Training loss: 0.022288246179761533, run on the CPU with time 0.06490929998108186\n",
      "run 619, Training loss: 0.02451600729572502, run on the CPU with time 0.06326739999349229\n",
      "run 620, Training loss: 0.02172390223184431, run on the CPU with time 0.0635113000171259\n",
      "run 621, Training loss: 0.02070154899053953, run on the CPU with time 0.06266029999824241\n",
      "run 622, Training loss: 0.02039362817515873, run on the CPU with time 0.07790070000919513\n",
      "run 623, Training loss: 0.02005017715379257, run on the CPU with time 0.06407649998436682\n",
      "run 624, Training loss: 0.020197542722929607, run on the CPU with time 0.06412260001525283\n",
      "run 625, Training loss: 0.02009913215879351, run on the CPU with time 0.06680830000550486\n",
      "run 626, Training loss: 0.019728954130021684, run on the CPU with time 0.06279889997676946\n",
      "run 627, Training loss: 0.020599355137197895, run on the CPU with time 0.06768279999960214\n",
      "run 628, Training loss: 0.02732009859332307, run on the CPU with time 0.062051899993093684\n",
      "run 629, Training loss: 0.020831298087300224, run on the CPU with time 0.06162260001292452\n",
      "run 630, Training loss: 0.020775262473828413, run on the CPU with time 0.0641730000206735\n",
      "run 631, Training loss: 0.020863471487113698, run on the CPU with time 0.06355910000274889\n",
      "run 632, Training loss: 0.020169306063855235, run on the CPU with time 0.06607619998976588\n",
      "run 633, Training loss: 0.01935015378985554, run on the CPU with time 0.06202019998454489\n",
      "run 634, Training loss: 0.018873240853215314, run on the CPU with time 0.06787210001493804\n",
      "run 635, Training loss: 0.020004189751026306, run on the CPU with time 0.06270179999410175\n",
      "run 636, Training loss: 0.019422048815018075, run on the CPU with time 0.0686312000034377\n",
      "run 637, Training loss: 0.018691157272339543, run on the CPU with time 0.07282520001172088\n",
      "run 638, Training loss: 0.01771156070806848, run on the CPU with time 0.06432949998998083\n",
      "run 639, Training loss: 0.01999076551503756, run on the CPU with time 0.07087850000243634\n",
      "run 640, Training loss: 0.01903861723311076, run on the CPU with time 0.06439760001376271\n",
      "run 641, Training loss: 0.017804446079852907, run on the CPU with time 0.06275660000392236\n",
      "run 642, Training loss: 0.018973172686740078, run on the CPU with time 0.0632288999913726\n",
      "run 643, Training loss: 0.01870464732743461, run on the CPU with time 0.06306960000074469\n",
      "run 644, Training loss: 0.019312975369393824, run on the CPU with time 0.06195810000644997\n",
      "run 645, Training loss: 0.017946386451578953, run on the CPU with time 0.06234330000006594\n",
      "run 646, Training loss: 0.01869245387444442, run on the CPU with time 0.06169529998442158\n",
      "run 647, Training loss: 0.01833528755232692, run on the CPU with time 0.07138189999386668\n",
      "run 648, Training loss: 0.01828587218822742, run on the CPU with time 0.07443899998907\n",
      "run 649, Training loss: 0.01798360004997283, run on the CPU with time 0.06581830000504851\n",
      "run 650, Training loss: 0.017386606773784893, run on the CPU with time 0.06234899998526089\n",
      "run 651, Training loss: 0.017908782790453354, run on the CPU with time 0.06432830000994727\n",
      "run 652, Training loss: 0.017115521371703257, run on the CPU with time 0.06278360000578687\n",
      "run 653, Training loss: 0.01716717900893524, run on the CPU with time 0.06186620000516996\n",
      "run 654, Training loss: 0.0168186681186357, run on the CPU with time 0.061928899987833574\n",
      "run 655, Training loss: 0.01723179907897826, run on the CPU with time 0.07016329999896698\n",
      "run 656, Training loss: 0.01705900923433629, run on the CPU with time 0.06651390000479296\n",
      "run 657, Training loss: 0.0177451119821688, run on the CPU with time 0.0734128000040073\n",
      "run 658, Training loss: 0.016435558782805774, run on the CPU with time 0.06368720001773909\n",
      "run 659, Training loss: 0.016147269412282516, run on the CPU with time 0.06396580001455732\n",
      "run 660, Training loss: 0.016930878050201995, run on the CPU with time 0.06402010002057068\n",
      "run 661, Training loss: 0.017764696032232182, run on the CPU with time 0.06273499998496845\n",
      "run 662, Training loss: 0.01675028422085399, run on the CPU with time 0.06224480000673793\n",
      "run 663, Training loss: 0.016825807048007846, run on the CPU with time 0.06213230002322234\n",
      "run 664, Training loss: 0.016657180444930086, run on the CPU with time 0.06208469998091459\n",
      "run 665, Training loss: 0.016216374209827997, run on the CPU with time 0.06338050001068041\n",
      "run 666, Training loss: 0.017199364967398816, run on the CPU with time 0.06273240002337843\n",
      "run 667, Training loss: 0.018506937395696612, run on the CPU with time 0.069497099990258\n",
      "run 668, Training loss: 0.01718655029438775, run on the CPU with time 0.06675080000422895\n",
      "run 669, Training loss: 0.016109314139678397, run on the CPU with time 0.06456830000388436\n",
      "run 670, Training loss: 0.01621374721765857, run on the CPU with time 0.07328519999282435\n",
      "run 671, Training loss: 0.016290502038530327, run on the CPU with time 0.0726178000040818\n",
      "run 672, Training loss: 0.01551230991396799, run on the CPU with time 0.06208169998717494\n",
      "run 673, Training loss: 0.01571598893285475, run on the CPU with time 0.06205290000070818\n",
      "run 674, Training loss: 0.015108875603288073, run on the CPU with time 0.06412250001449138\n",
      "run 675, Training loss: 0.01573829032501883, run on the CPU with time 0.0626343000039924\n",
      "run 676, Training loss: 0.016480068445459687, run on the CPU with time 0.06357709999429062\n",
      "run 677, Training loss: 0.01599148818164725, run on the CPU with time 0.09058370001730509\n",
      "run 678, Training loss: 0.015863625805782662, run on the CPU with time 0.07019349999609403\n",
      "run 679, Training loss: 0.015600370846434751, run on the CPU with time 0.0667163000034634\n",
      "run 680, Training loss: 0.015983142820186914, run on the CPU with time 0.06765430001541972\n",
      "run 681, Training loss: 0.014514346357265657, run on the CPU with time 0.06179939999128692\n",
      "run 682, Training loss: 0.01522125645159659, run on the CPU with time 0.061565800016978756\n",
      "run 683, Training loss: 0.014770037340084937, run on the CPU with time 0.061595299979671836\n",
      "run 684, Training loss: 0.015826249671888285, run on the CPU with time 0.061647199996514246\n",
      "run 685, Training loss: 0.014647157916756855, run on the CPU with time 0.06570509998709895\n",
      "run 686, Training loss: 0.01575279233934866, run on the CPU with time 0.08351409999886528\n",
      "run 687, Training loss: 0.015146522664740173, run on the CPU with time 0.06891530001303181\n",
      "run 688, Training loss: 0.01452694439583204, run on the CPU with time 0.06203159998403862\n",
      "run 689, Training loss: 0.014668516102458604, run on the CPU with time 0.0637779000098817\n",
      "run 690, Training loss: 0.014442626725543629, run on the CPU with time 0.06356440001400188\n",
      "run 691, Training loss: 0.014394438336603343, run on the CPU with time 0.06191339998622425\n",
      "run 692, Training loss: 0.015244170003147288, run on the CPU with time 0.06278730000485666\n",
      "run 693, Training loss: 0.014445060147673675, run on the CPU with time 0.06971530002192594\n",
      "run 694, Training loss: 0.014055179317735813, run on the CPU with time 0.06407500000204891\n",
      "run 695, Training loss: 0.013785963004920632, run on the CPU with time 0.07160830000066198\n",
      "run 696, Training loss: 0.014484514198689298, run on the CPU with time 0.06624680000822991\n",
      "run 697, Training loss: 0.013986101843924684, run on the CPU with time 0.06201080000028014\n",
      "run 698, Training loss: 0.01651975831165063, run on the CPU with time 0.06419519998598844\n",
      "run 699, Training loss: 0.015397213767705992, run on the CPU with time 0.06445730000268668\n",
      "run 700, Training loss: 0.014545391106300733, run on the CPU with time 0.06217980000656098\n",
      "run 701, Training loss: 0.015394482070537792, run on the CPU with time 0.06818160001421347\n",
      "run 702, Training loss: 0.014052618728865955, run on the CPU with time 0.06387680000625551\n",
      "run 703, Training loss: 0.014427486129782417, run on the CPU with time 0.06318100000498816\n",
      "run 704, Training loss: 0.0140385972304185, run on the CPU with time 0.06130760000087321\n",
      "run 705, Training loss: 0.014094931158152494, run on the CPU with time 0.06291770000825636\n",
      "run 706, Training loss: 0.014300577685670843, run on the CPU with time 0.06390899998950772\n",
      "run 707, Training loss: 0.013994347386654804, run on the CPU with time 0.0704089000064414\n",
      "run 708, Training loss: 0.016164764294146813, run on the CPU with time 0.06733739998890087\n",
      "run 709, Training loss: 0.013468281702477146, run on the CPU with time 0.0629301999870222\n",
      "run 710, Training loss: 0.013348142262971537, run on the CPU with time 0.06267359998309985\n",
      "run 711, Training loss: 0.01328381977298043, run on the CPU with time 0.061538999987533316\n",
      "run 712, Training loss: 0.013167342080057345, run on the CPU with time 0.06301539999549277\n",
      "run 713, Training loss: 0.013597963522823359, run on the CPU with time 0.06140080001205206\n",
      "run 714, Training loss: 0.016100828513630074, run on the CPU with time 0.06498309999005869\n",
      "run 715, Training loss: 0.013930441229604185, run on the CPU with time 0.0683955000131391\n",
      "run 716, Training loss: 0.012757188516182148, run on the CPU with time 0.06795930000953376\n",
      "run 717, Training loss: 0.012594406444325365, run on the CPU with time 0.07298159998026676\n",
      "run 718, Training loss: 0.01319227270591496, run on the CPU with time 0.0747655000013765\n",
      "run 719, Training loss: 0.014478539510376073, run on the CPU with time 0.06311110002570786\n",
      "run 720, Training loss: 0.013016189596700397, run on the CPU with time 0.06172050000168383\n",
      "run 721, Training loss: 0.013234049712561749, run on the CPU with time 0.06552219999139197\n",
      "run 722, Training loss: 0.012487341592681002, run on the CPU with time 0.06372679999913089\n",
      "run 723, Training loss: 0.012693576512604275, run on the CPU with time 0.06203370000002906\n",
      "run 724, Training loss: 0.01293622221538416, run on the CPU with time 0.062031500012381\n",
      "run 725, Training loss: 0.013250297030688009, run on the CPU with time 0.0652393999916967\n",
      "run 726, Training loss: 0.012653615813575346, run on the CPU with time 0.06243600000743754\n",
      "run 727, Training loss: 0.012271599229213528, run on the CPU with time 0.06652459999895655\n",
      "run 728, Training loss: 0.01231543702412058, run on the CPU with time 0.0708734999934677\n",
      "run 729, Training loss: 0.012474708866317418, run on the CPU with time 0.0685972000064794\n",
      "run 730, Training loss: 0.01257566361772743, run on the CPU with time 0.06503979998524301\n",
      "run 731, Training loss: 0.012168808224272321, run on the CPU with time 0.06657159997848794\n",
      "run 732, Training loss: 0.012045921298505907, run on the CPU with time 0.062296399992192164\n",
      "run 733, Training loss: 0.014243361828795804, run on the CPU with time 0.06505340000148863\n",
      "run 734, Training loss: 0.012702621887332167, run on the CPU with time 0.06179149998934008\n",
      "run 735, Training loss: 0.011644656197379597, run on the CPU with time 0.06094379999558441\n",
      "run 736, Training loss: 0.011505778283092447, run on the CPU with time 0.061699199985014275\n",
      "run 737, Training loss: 0.011603598261717706, run on the CPU with time 0.06795769999735057\n",
      "run 738, Training loss: 0.011740676414708353, run on the CPU with time 0.06923449999885634\n",
      "run 739, Training loss: 0.018697920932688496, run on the CPU with time 0.06566269998438656\n",
      "run 740, Training loss: 0.015580196889244358, run on the CPU with time 0.06310220001614653\n",
      "run 741, Training loss: 0.015304275129032745, run on the CPU with time 0.06243470002664253\n",
      "run 742, Training loss: 0.012498404992881908, run on the CPU with time 0.0682669999951031\n",
      "run 743, Training loss: 0.011623359158296476, run on the CPU with time 0.0673767999978736\n",
      "run 744, Training loss: 0.011796978386965666, run on the CPU with time 0.06227059999946505\n",
      "run 745, Training loss: 0.013231271141822534, run on the CPU with time 0.06336540001211688\n",
      "run 746, Training loss: 0.011827542631759901, run on the CPU with time 0.06477510000695474\n",
      "run 747, Training loss: 0.011814949375746602, run on the CPU with time 0.06226029997924343\n",
      "run 748, Training loss: 0.011750302495519546, run on the CPU with time 0.06159070000285283\n",
      "run 749, Training loss: 0.012771877416840157, run on the CPU with time 0.06249889999162406\n",
      "run 750, Training loss: 0.011267836839595641, run on the CPU with time 0.06293899999582209\n",
      "run 751, Training loss: 0.01141168907051906, run on the CPU with time 0.09693239998887293\n",
      "run 752, Training loss: 0.011868952226094817, run on the CPU with time 0.07197339998674579\n",
      "run 753, Training loss: 0.012071300717070698, run on the CPU with time 0.06421969999792054\n",
      "run 754, Training loss: 0.011964746308512986, run on the CPU with time 0.06190239998977631\n",
      "run 755, Training loss: 0.011189212044370254, run on the CPU with time 0.062469600001350045\n",
      "run 756, Training loss: 0.011085321521386504, run on the CPU with time 0.06280460002017207\n",
      "run 757, Training loss: 0.010942236833613027, run on the CPU with time 0.062307999993208796\n",
      "run 758, Training loss: 0.011001493523574688, run on the CPU with time 0.06206239998573437\n",
      "run 759, Training loss: 0.010943000891728099, run on the CPU with time 0.0634309999877587\n",
      "run 760, Training loss: 0.011274301617363976, run on the CPU with time 0.06687189999502152\n",
      "run 761, Training loss: 0.01090267181227153, run on the CPU with time 0.07457190001150593\n",
      "run 762, Training loss: 0.010966347487093034, run on the CPU with time 0.08271240000613034\n",
      "run 763, Training loss: 0.011113852737683125, run on the CPU with time 0.06680460000643507\n",
      "run 764, Training loss: 0.010911394628188149, run on the CPU with time 0.06836540001677349\n",
      "run 765, Training loss: 0.01056734144010327, run on the CPU with time 0.06454009999288246\n",
      "run 766, Training loss: 0.011037418628323145, run on the CPU with time 0.06445659999735653\n",
      "run 767, Training loss: 0.010879419665698978, run on the CPU with time 0.06504660000791773\n",
      "run 768, Training loss: 0.01041787928816947, run on the CPU with time 0.06187520001549274\n",
      "run 769, Training loss: 0.01029643817640714, run on the CPU with time 0.06181259997538291\n",
      "run 770, Training loss: 0.01054761890925213, run on the CPU with time 0.06164200001512654\n",
      "run 771, Training loss: 0.010692664349070666, run on the CPU with time 0.06437489998643287\n",
      "run 772, Training loss: 0.01227483080001548, run on the CPU with time 0.07266279999748804\n",
      "run 773, Training loss: 0.011348021873378787, run on the CPU with time 0.0757762000139337\n",
      "run 774, Training loss: 0.010813465260434895, run on the CPU with time 0.06913499999791384\n",
      "run 775, Training loss: 0.010510622556003827, run on the CPU with time 0.06556670001009479\n",
      "run 776, Training loss: 0.010767238792190752, run on the CPU with time 0.06402130000060424\n",
      "run 777, Training loss: 0.01177960244981064, run on the CPU with time 0.06422939998446964\n",
      "run 778, Training loss: 0.010911070406225257, run on the CPU with time 0.06404420000035316\n",
      "run 779, Training loss: 0.012177082151703706, run on the CPU with time 0.06206620001466945\n",
      "run 780, Training loss: 0.07743845993113196, run on the CPU with time 0.06263280002167448\n",
      "run 781, Training loss: 0.05883880934879099, run on the CPU with time 0.06309239999973215\n",
      "run 782, Training loss: 0.02521679136605764, run on the CPU with time 0.06383210001513362\n",
      "run 783, Training loss: 0.015143146020868286, run on the CPU with time 0.06315309999627061\n",
      "run 784, Training loss: 0.014180717756839427, run on the CPU with time 0.08099219997529872\n",
      "run 785, Training loss: 0.011632532895204018, run on the CPU with time 0.0652991000097245\n",
      "run 786, Training loss: 0.01124221675828184, run on the CPU with time 0.07145039999159053\n",
      "run 787, Training loss: 0.01070584511881779, run on the CPU with time 0.06314589999965392\n",
      "run 788, Training loss: 0.010976012313569134, run on the CPU with time 0.06532350002089515\n",
      "run 789, Training loss: 0.010491237087196417, run on the CPU with time 0.06649999998626299\n",
      "run 790, Training loss: 0.011166090344671498, run on the CPU with time 0.06162259998382069\n",
      "run 791, Training loss: 0.010505373927298934, run on the CPU with time 0.06176039998535998\n",
      "run 792, Training loss: 0.01087424140211872, run on the CPU with time 0.06257780001033098\n",
      "run 793, Training loss: 0.010577232224485752, run on the CPU with time 0.06961690000025555\n",
      "run 794, Training loss: 0.009797446223356846, run on the CPU with time 0.0689070999796968\n",
      "run 795, Training loss: 0.009978441603016109, run on the CPU with time 0.06504290000884794\n",
      "run 796, Training loss: 0.01044019049202854, run on the CPU with time 0.06373490000260063\n",
      "run 797, Training loss: 0.009815170644200902, run on the CPU with time 0.06994230000418611\n",
      "run 798, Training loss: 0.009874511635544795, run on the CPU with time 0.06484310000087135\n",
      "run 799, Training loss: 0.01003914515551349, run on the CPU with time 0.06874239997705445\n",
      "run 800, Training loss: 0.011094805024648931, run on the CPU with time 0.06555120000848547\n",
      "run 801, Training loss: 0.012006654448553242, run on the CPU with time 0.06466220002039336\n",
      "run 802, Training loss: 0.009833707943008366, run on the CPU with time 0.0619107999955304\n",
      "run 803, Training loss: 0.010008806668073786, run on the CPU with time 0.06212319998303428\n",
      "run 804, Training loss: 0.010127309534635226, run on the CPU with time 0.061640100000659004\n",
      "run 805, Training loss: 0.009785692623435436, run on the CPU with time 0.06227490000310354\n",
      "run 806, Training loss: 0.009075190564511683, run on the CPU with time 0.06216559998574667\n",
      "run 807, Training loss: 0.010291520575992762, run on the CPU with time 0.06824920000508428\n",
      "run 808, Training loss: 0.010826187259094282, run on the CPU with time 0.08651219998137094\n",
      "run 809, Training loss: 0.009503046477171168, run on the CPU with time 0.06417669999063946\n",
      "run 810, Training loss: 0.009226116153877228, run on the CPU with time 0.06240660001640208\n",
      "run 811, Training loss: 0.010342726289209994, run on the CPU with time 0.0627946000022348\n",
      "run 812, Training loss: 0.009618333494290709, run on the CPU with time 0.06511080000200309\n",
      "run 813, Training loss: 0.010406736758622257, run on the CPU with time 0.0627569000062067\n",
      "run 814, Training loss: 0.01073991639370268, run on the CPU with time 0.06294699999853037\n",
      "run 815, Training loss: 0.009725559447013603, run on the CPU with time 0.06314829998882487\n",
      "run 816, Training loss: 0.009316748583858664, run on the CPU with time 0.06248819999746047\n",
      "run 817, Training loss: 0.010890947991389443, run on the CPU with time 0.0615445000003092\n",
      "run 818, Training loss: 0.01053155482487372, run on the CPU with time 0.07654459998593666\n",
      "run 819, Training loss: 0.010888578968164935, run on the CPU with time 0.07676970001193695\n",
      "run 820, Training loss: 0.009719682267909362, run on the CPU with time 0.06282410002313554\n",
      "run 821, Training loss: 0.009263231344679794, run on the CPU with time 0.06577129999641329\n",
      "run 822, Training loss: 0.009658945433858951, run on the CPU with time 0.06202149999444373\n",
      "run 823, Training loss: 0.009472285507416184, run on the CPU with time 0.06571739999344572\n",
      "run 824, Training loss: 0.009847855706043034, run on the CPU with time 0.062382400006754324\n",
      "run 825, Training loss: 0.009383434787477283, run on the CPU with time 0.06224649999057874\n",
      "run 826, Training loss: 0.00899244401767977, run on the CPU with time 0.06218159999116324\n",
      "run 827, Training loss: 0.009226102040636099, run on the CPU with time 0.06275470001855865\n",
      "run 828, Training loss: 0.009092159143289213, run on the CPU with time 0.0613794999953825\n",
      "run 829, Training loss: 0.008707820816793669, run on the CPU with time 0.06624709998141043\n",
      "run 830, Training loss: 0.00973986134419895, run on the CPU with time 0.0680002000008244\n",
      "run 831, Training loss: 0.008666337813801047, run on the CPU with time 0.06361559999641031\n",
      "run 832, Training loss: 0.009147207888732242, run on the CPU with time 0.06350220000604168\n",
      "run 833, Training loss: 0.009392055799253285, run on the CPU with time 0.06403999999747612\n",
      "run 834, Training loss: 0.008910126397809522, run on the CPU with time 0.06561769999098033\n",
      "run 835, Training loss: 0.009581469960341399, run on the CPU with time 0.06430050000199117\n",
      "run 836, Training loss: 0.008902377817271784, run on the CPU with time 0.06382589999702759\n",
      "run 837, Training loss: 0.008418438607953828, run on the CPU with time 0.07166160002816468\n",
      "run 838, Training loss: 0.008294530309715562, run on the CPU with time 0.0728069000178948\n",
      "run 839, Training loss: 0.008308880269760266, run on the CPU with time 0.0700485999986995\n",
      "run 840, Training loss: 0.009001827344764023, run on the CPU with time 0.06470919999992475\n",
      "run 841, Training loss: 0.008503333714377897, run on the CPU with time 0.06838809998589568\n",
      "run 842, Training loss: 0.00830269588729028, run on the CPU with time 0.0631301999965217\n",
      "run 843, Training loss: 0.008426305825229395, run on the CPU with time 0.0658040999842342\n",
      "run 844, Training loss: 0.009559921753084795, run on the CPU with time 0.06276550001348369\n",
      "run 845, Training loss: 0.00939233751350912, run on the CPU with time 0.06370779999997467\n",
      "run 846, Training loss: 0.008405171782413328, run on the CPU with time 0.06301959999836981\n",
      "run 847, Training loss: 0.00842847148940729, run on the CPU with time 0.07768049999140203\n",
      "run 848, Training loss: 0.008411356852792035, run on the CPU with time 0.07057009998243302\n",
      "run 849, Training loss: 0.008129146675558084, run on the CPU with time 0.06672069997875951\n",
      "run 850, Training loss: 0.008248389011714607, run on the CPU with time 0.06707140000071377\n",
      "run 851, Training loss: 0.008207499599401754, run on the CPU with time 0.06648839998524636\n",
      "run 852, Training loss: 0.008732080002400009, run on the CPU with time 0.06398160001845099\n",
      "run 853, Training loss: 0.008684959362356246, run on the CPU with time 0.06364549999125302\n",
      "run 854, Training loss: 0.008526101618190295, run on the CPU with time 0.06622910001897253\n",
      "run 855, Training loss: 0.007915907798715953, run on the CPU with time 0.0681930000137072\n",
      "run 856, Training loss: 0.008572966588491743, run on the CPU with time 0.06945339997764677\n",
      "run 857, Training loss: 0.008668611887101592, run on the CPU with time 0.06744010001420975\n",
      "run 858, Training loss: 0.008009893497960134, run on the CPU with time 0.06936110000242479\n",
      "run 859, Training loss: 0.0081494991582903, run on the CPU with time 0.07034350000321865\n",
      "run 860, Training loss: 0.007906876545695758, run on the CPU with time 0.0718263000017032\n",
      "run 861, Training loss: 0.008141927783038806, run on the CPU with time 0.06654870000784285\n",
      "run 862, Training loss: 0.00788211058401926, run on the CPU with time 0.06999659998109564\n",
      "run 863, Training loss: 0.00807503328542225, run on the CPU with time 0.06963529999484308\n",
      "run 864, Training loss: 0.009360852840737524, run on the CPU with time 0.06312580002122559\n",
      "run 865, Training loss: 0.009176424891814928, run on the CPU with time 0.06195820000721142\n",
      "run 866, Training loss: 0.009324927301019092, run on the CPU with time 0.06535759998951107\n",
      "run 867, Training loss: 0.009369671586054293, run on the CPU with time 0.06410469999536872\n",
      "run 868, Training loss: 0.007973889728203754, run on the CPU with time 0.06502690000343136\n",
      "run 869, Training loss: 0.007857741823636388, run on the CPU with time 0.08063740000943653\n",
      "run 870, Training loss: 0.00789659312788651, run on the CPU with time 0.09161510001285933\n",
      "run 871, Training loss: 0.007939929533114827, run on the CPU with time 0.0634274999902118\n",
      "run 872, Training loss: 0.008974929167677394, run on the CPU with time 0.06678689998807386\n",
      "run 873, Training loss: 0.008337236536052924, run on the CPU with time 0.06505719997221604\n",
      "run 874, Training loss: 0.011130411825103114, run on the CPU with time 0.06337750001694076\n",
      "run 875, Training loss: 0.008608051831304859, run on the CPU with time 0.06402759999036789\n",
      "run 876, Training loss: 0.007905857815851711, run on the CPU with time 0.064185199997155\n",
      "run 877, Training loss: 0.008651592408899556, run on the CPU with time 0.06395700000575744\n",
      "run 878, Training loss: 0.008859704407090744, run on the CPU with time 0.07198949999292381\n",
      "run 879, Training loss: 0.00797952741435934, run on the CPU with time 0.07577440000022762\n",
      "run 880, Training loss: 0.008151475085072558, run on the CPU with time 0.068851000018185\n",
      "run 881, Training loss: 0.008020065448098732, run on the CPU with time 0.06487610001931898\n",
      "run 882, Training loss: 0.007898924395240928, run on the CPU with time 0.07414310000604019\n",
      "run 883, Training loss: 0.0077983465606600725, run on the CPU with time 0.06643149998853914\n",
      "run 884, Training loss: 0.007518350064690987, run on the CPU with time 0.0627544000162743\n",
      "run 885, Training loss: 0.007391411440171809, run on the CPU with time 0.06564480002271011\n",
      "run 886, Training loss: 0.008302155236015097, run on the CPU with time 0.0807558000087738\n",
      "run 887, Training loss: 0.00800171884581108, run on the CPU with time 0.06509549997281283\n",
      "run 888, Training loss: 0.007630719468844208, run on the CPU with time 0.063433699979214\n",
      "run 889, Training loss: 0.007881452301940458, run on the CPU with time 0.06450149999000132\n",
      "run 890, Training loss: 0.008188163713467393, run on the CPU with time 0.07195720000891015\n",
      "run 891, Training loss: 0.009509076054250314, run on the CPU with time 0.07199950001086108\n",
      "run 892, Training loss: 0.008010238721776246, run on the CPU with time 0.06536850001430139\n",
      "run 893, Training loss: 0.007875835816957989, run on the CPU with time 0.06470170000102371\n",
      "run 894, Training loss: 0.00787375703327019, run on the CPU with time 0.0718674999952782\n",
      "run 895, Training loss: 0.008507329985414716, run on the CPU with time 0.07290689999354072\n",
      "run 896, Training loss: 0.007626772835982107, run on the CPU with time 0.06933349999599159\n",
      "run 897, Training loss: 0.0078018329656598245, run on the CPU with time 0.06550129997776821\n",
      "run 898, Training loss: 0.007907033984189514, run on the CPU with time 0.06731980000040494\n",
      "run 899, Training loss: 0.007377930648263745, run on the CPU with time 0.06352029999834485\n",
      "run 900, Training loss: 0.0073112682981776415, run on the CPU with time 0.06383800000185147\n",
      "run 901, Training loss: 0.0073136159391176325, run on the CPU with time 0.06281569998827763\n",
      "run 902, Training loss: 0.007620689044283195, run on the CPU with time 0.07980229999520816\n",
      "run 903, Training loss: 0.011179995911449872, run on the CPU with time 0.06720579997636378\n",
      "run 904, Training loss: 0.007683835037856955, run on the CPU with time 0.06969020000542514\n",
      "run 905, Training loss: 0.007642267386175015, run on the CPU with time 0.0649781999818515\n",
      "run 906, Training loss: 0.00714530939320949, run on the CPU with time 0.06514389999210835\n",
      "run 907, Training loss: 0.006727735208749602, run on the CPU with time 0.06555649999063462\n",
      "run 908, Training loss: 0.006812698344120608, run on the CPU with time 0.06484669999917969\n",
      "run 909, Training loss: 0.006844376591669226, run on the CPU with time 0.06353529999614693\n",
      "run 910, Training loss: 0.007799593860346993, run on the CPU with time 0.06233089999295771\n",
      "run 911, Training loss: 0.012975610690508885, run on the CPU with time 0.0626421999768354\n",
      "run 912, Training loss: 0.009813420043164198, run on the CPU with time 0.0694260000018403\n",
      "run 913, Training loss: 0.01193192391444675, run on the CPU with time 0.07086949999211356\n",
      "run 914, Training loss: 0.00896859460223244, run on the CPU with time 0.08067679998930544\n",
      "run 915, Training loss: 0.007853977726402015, run on the CPU with time 0.06581669999286532\n",
      "run 916, Training loss: 0.006880911740220406, run on the CPU with time 0.06517029998940416\n",
      "run 917, Training loss: 0.007152309057048776, run on the CPU with time 0.06513509998330846\n",
      "run 918, Training loss: 0.006990763995351947, run on the CPU with time 0.0653353999950923\n",
      "run 919, Training loss: 0.007050580015956339, run on the CPU with time 0.06903040001634508\n",
      "run 920, Training loss: 0.006843430774328722, run on the CPU with time 0.0651685000048019\n",
      "run 921, Training loss: 0.006897958360390145, run on the CPU with time 0.06382480001775548\n",
      "run 922, Training loss: 0.0064967679833485325, run on the CPU with time 0.08194600002025254\n",
      "run 923, Training loss: 0.006815786157015034, run on the CPU with time 0.06804700000793673\n",
      "run 924, Training loss: 0.006702302510066974, run on the CPU with time 0.06480079999892041\n",
      "run 925, Training loss: 0.006993649937529964, run on the CPU with time 0.06761610001558438\n",
      "run 926, Training loss: 0.007298555671745403, run on the CPU with time 0.06349280002177693\n",
      "run 927, Training loss: 0.006856523789412512, run on the CPU with time 0.06538589997217059\n",
      "run 928, Training loss: 0.006590927853672342, run on the CPU with time 0.06291459998465143\n",
      "run 929, Training loss: 0.006716408737702295, run on the CPU with time 0.06456700002308935\n",
      "run 930, Training loss: 0.0065553858823312276, run on the CPU with time 0.08472539999638684\n",
      "run 931, Training loss: 0.0065046128361824565, run on the CPU with time 0.0781898999994155\n",
      "run 932, Training loss: 0.008008580892452632, run on the CPU with time 0.06538839999120682\n",
      "run 933, Training loss: 0.008178731796049925, run on the CPU with time 0.06772829999681562\n",
      "run 934, Training loss: 0.007434915571304207, run on the CPU with time 0.06438060000073165\n",
      "run 935, Training loss: 0.006825479850389953, run on the CPU with time 0.06385179999051616\n",
      "run 936, Training loss: 0.006951125842302149, run on the CPU with time 0.06342249998124316\n",
      "run 937, Training loss: 0.008177872924980793, run on the CPU with time 0.06333759997505695\n",
      "run 938, Training loss: 0.007597589755790647, run on the CPU with time 0.0635451000125613\n",
      "run 939, Training loss: 0.007250696711625179, run on the CPU with time 0.07809980001184158\n",
      "run 940, Training loss: 0.006768762054112316, run on the CPU with time 0.07174330000998452\n",
      "run 941, Training loss: 0.006601639511063695, run on the CPU with time 0.06893649999983609\n",
      "run 942, Training loss: 0.006996048222274774, run on the CPU with time 0.07611130000441335\n",
      "run 943, Training loss: 0.007780794504585422, run on the CPU with time 0.06781609999598004\n",
      "run 944, Training loss: 0.007656489973570305, run on the CPU with time 0.06447009998373687\n",
      "run 945, Training loss: 0.006590129022846337, run on the CPU with time 0.0647309000196401\n",
      "run 946, Training loss: 0.006632522495188327, run on the CPU with time 0.062170299992430955\n",
      "run 947, Training loss: 0.0067997228994499895, run on the CPU with time 0.063226800004486\n",
      "run 948, Training loss: 0.006922912366413088, run on the CPU with time 0.06327060001785867\n",
      "run 949, Training loss: 0.007450969370918095, run on the CPU with time 0.09149459999753162\n",
      "run 950, Training loss: 0.0072660150519699195, run on the CPU with time 0.0663821000198368\n",
      "run 951, Training loss: 0.0073073461641218855, run on the CPU with time 0.06893919999129139\n",
      "run 952, Training loss: 0.007670899292861577, run on the CPU with time 0.0628226000117138\n",
      "run 953, Training loss: 0.007751811843958091, run on the CPU with time 0.06296429998474196\n",
      "run 954, Training loss: 0.0077730702746934685, run on the CPU with time 0.06237060000421479\n",
      "run 955, Training loss: 0.007983235276664015, run on the CPU with time 0.06264730001566932\n",
      "run 956, Training loss: 0.010779113513787955, run on the CPU with time 0.06415220000781119\n",
      "run 957, Training loss: 0.00921193241215819, run on the CPU with time 0.07339190001948737\n",
      "run 958, Training loss: 0.014208386112427846, run on the CPU with time 0.08205840000300668\n",
      "run 959, Training loss: 0.008185705147281458, run on the CPU with time 0.06580630000098608\n",
      "run 960, Training loss: 0.007368976716481319, run on the CPU with time 0.06409830000484362\n",
      "run 961, Training loss: 0.00653243663943034, run on the CPU with time 0.06391850000363775\n",
      "run 962, Training loss: 0.0067547765571031385, run on the CPU with time 0.06613920000381768\n",
      "run 963, Training loss: 0.006686066794844175, run on the CPU with time 0.06251920000067912\n",
      "run 964, Training loss: 0.006341174263269005, run on the CPU with time 0.06340870002168231\n",
      "run 965, Training loss: 0.006243748498805375, run on the CPU with time 0.06621769999037497\n",
      "run 966, Training loss: 0.006302277339273132, run on the CPU with time 0.07489849999547005\n",
      "run 967, Training loss: 0.006062695422125133, run on the CPU with time 0.06669130001682788\n",
      "run 968, Training loss: 0.00629885615225331, run on the CPU with time 0.06702990000485443\n",
      "run 969, Training loss: 0.006108479432507672, run on the CPU with time 0.07862919999752194\n",
      "run 970, Training loss: 0.006155860179569572, run on the CPU with time 0.06364410000969656\n",
      "run 971, Training loss: 0.006111103705850176, run on the CPU with time 0.06341819997760467\n",
      "run 972, Training loss: 0.0070446167945523155, run on the CPU with time 0.06237550001242198\n",
      "run 973, Training loss: 0.006467912361999465, run on the CPU with time 0.06618940000771545\n",
      "run 974, Training loss: 0.006248473945809316, run on the CPU with time 0.06833629999891855\n",
      "run 975, Training loss: 0.007092860733180053, run on the CPU with time 0.07353269998566248\n",
      "run 976, Training loss: 0.006379147510266524, run on the CPU with time 0.06784810000681318\n",
      "run 977, Training loss: 0.006483426965397401, run on the CPU with time 0.0648356000019703\n",
      "run 978, Training loss: 0.0073630295355211605, run on the CPU with time 0.06101830000989139\n",
      "run 979, Training loss: 0.006932692455699329, run on the CPU with time 0.06394389999331906\n",
      "run 980, Training loss: 0.006375293161677705, run on the CPU with time 0.06238700001267716\n",
      "run 981, Training loss: 0.006435538612673885, run on the CPU with time 0.06424249999690801\n",
      "run 982, Training loss: 0.006478276270420544, run on the CPU with time 0.07699549998505972\n",
      "run 983, Training loss: 0.006721114793882324, run on the CPU with time 0.07821000000694767\n",
      "run 984, Training loss: 0.006164911577202887, run on the CPU with time 0.06390400000964291\n",
      "run 985, Training loss: 0.006183661920526488, run on the CPU with time 0.06621459999587387\n",
      "run 986, Training loss: 0.006122456284091723, run on the CPU with time 0.06725570000708103\n",
      "run 987, Training loss: 0.006043137367073954, run on the CPU with time 0.06241680000675842\n",
      "run 988, Training loss: 0.00624515544093976, run on the CPU with time 0.061786800011759624\n",
      "run 989, Training loss: 0.005953341598135673, run on the CPU with time 0.06265690000145696\n",
      "run 990, Training loss: 0.0059727657311172645, run on the CPU with time 0.06138040000223555\n",
      "run 991, Training loss: 0.006006467120923017, run on the CPU with time 0.06738500000210479\n",
      "run 992, Training loss: 0.005665657232748344, run on the CPU with time 0.08191799998166971\n",
      "run 993, Training loss: 0.005553728092557073, run on the CPU with time 0.06134290000773035\n",
      "run 994, Training loss: 0.005813931822459298, run on the CPU with time 0.06957479999982752\n",
      "run 995, Training loss: 0.006070389782226729, run on the CPU with time 0.06330330000491813\n",
      "run 996, Training loss: 0.006206092514118857, run on the CPU with time 0.06272230000467971\n",
      "run 997, Training loss: 0.005914471832643771, run on the CPU with time 0.06478030001744628\n",
      "run 998, Training loss: 0.005656162945458411, run on the CPU with time 0.06123599997954443\n",
      "run 999, Training loss: 0.005710476555395872, run on the CPU with time 0.06101850001141429\n",
      "run 1000, Training loss: 0.005700902802213519, run on the CPU with time 0.0625741999829188\n",
      "run 1001, Training loss: 0.005783023918576708, run on the CPU with time 0.06346730000223033\n",
      "run 1002, Training loss: 0.0061902151855809445, run on the CPU with time 0.07441000000108033\n",
      "run 1003, Training loss: 0.005633208662567829, run on the CPU with time 0.07834259999799542\n",
      "run 1004, Training loss: 0.005728886869672516, run on the CPU with time 0.06559470001957379\n",
      "run 1005, Training loss: 0.005890012581023092, run on the CPU with time 0.06299169998965226\n",
      "run 1006, Training loss: 0.005722983603932748, run on the CPU with time 0.06312619999516755\n",
      "run 1007, Training loss: 0.005995275133120066, run on the CPU with time 0.06969540001591668\n",
      "run 1008, Training loss: 0.013612396234880593, run on the CPU with time 0.061414700001478195\n",
      "run 1009, Training loss: 0.007042892689456824, run on the CPU with time 0.062318500014953315\n",
      "run 1010, Training loss: 0.005814585202923891, run on the CPU with time 0.061521499999798834\n",
      "run 1011, Training loss: 0.005772937440038236, run on the CPU with time 0.061512299987953156\n",
      "run 1012, Training loss: 0.005759784796232866, run on the CPU with time 0.0650012000114657\n",
      "run 1013, Training loss: 0.005636765470262617, run on the CPU with time 0.062377900001592934\n",
      "run 1014, Training loss: 0.005724943996491758, run on the CPU with time 0.06316849999711849\n",
      "run 1015, Training loss: 0.005627243208106269, run on the CPU with time 0.07376589998602867\n",
      "run 1016, Training loss: 0.006171001210309028, run on the CPU with time 0.06479639999452047\n",
      "run 1017, Training loss: 0.005836340802753429, run on the CPU with time 0.06970399999408983\n",
      "run 1018, Training loss: 0.005740303100107915, run on the CPU with time 0.06651789997704327\n",
      "run 1019, Training loss: 0.0058602106700577266, run on the CPU with time 0.06615639998926781\n",
      "run 1020, Training loss: 0.005942298247034407, run on the CPU with time 0.06319380001514219\n",
      "run 1021, Training loss: 0.00568759119884238, run on the CPU with time 0.06560390000231564\n",
      "run 1022, Training loss: 0.005378889702577991, run on the CPU with time 0.06362500000977889\n",
      "run 1023, Training loss: 0.005480215153180656, run on the CPU with time 0.06430910000926815\n",
      "run 1024, Training loss: 0.005558968950274654, run on the CPU with time 0.0741664000088349\n",
      "run 1025, Training loss: 0.005603461621583186, run on the CPU with time 0.06509630000800826\n",
      "run 1026, Training loss: 0.009021089803321626, run on the CPU with time 0.06905180000467226\n",
      "run 1027, Training loss: 0.006227768871837146, run on the CPU with time 0.07116960000712425\n",
      "run 1028, Training loss: 0.0066784925684756176, run on the CPU with time 0.06298009998863563\n",
      "run 1029, Training loss: 0.008446537499019706, run on the CPU with time 0.06446270001470111\n",
      "run 1030, Training loss: 0.0067933903734559535, run on the CPU with time 0.0646691000147257\n",
      "run 1031, Training loss: 0.005643848557850685, run on the CPU with time 0.06529559998307377\n",
      "run 1032, Training loss: 0.007236840422476896, run on the CPU with time 0.06337819999316707\n",
      "run 1033, Training loss: 0.0072446393653411755, run on the CPU with time 0.06722649998846464\n",
      "run 1034, Training loss: 0.005614258646859195, run on the CPU with time 0.07481709998683073\n",
      "run 1035, Training loss: 0.005610426179026201, run on the CPU with time 0.0652364999987185\n",
      "run 1036, Training loss: 0.005728756443237547, run on the CPU with time 0.06362999998964369\n",
      "run 1037, Training loss: 0.005674614617071877, run on the CPU with time 0.062259400001494214\n",
      "run 1038, Training loss: 0.005486994212366302, run on the CPU with time 0.06314189999829978\n",
      "run 1039, Training loss: 0.005786367066056383, run on the CPU with time 0.06177529998240061\n",
      "run 1040, Training loss: 0.0054602231253573505, run on the CPU with time 0.06213599999318831\n",
      "run 1041, Training loss: 0.005384173623381437, run on the CPU with time 0.06422549998387694\n",
      "run 1042, Training loss: 0.005481129986054095, run on the CPU with time 0.06978630000958219\n",
      "run 1043, Training loss: 0.005106772462419361, run on the CPU with time 0.0758915999904275\n",
      "run 1044, Training loss: 0.0051008614780254325, run on the CPU with time 0.06266329999198206\n",
      "run 1045, Training loss: 0.005387440817007287, run on the CPU with time 0.06507800001418218\n",
      "run 1046, Training loss: 0.005782430629055439, run on the CPU with time 0.062152600003173575\n",
      "run 1047, Training loss: 0.005743454348835671, run on the CPU with time 0.06177239998942241\n",
      "run 1048, Training loss: 0.00573617373063991, run on the CPU with time 0.06154990001232363\n",
      "run 1049, Training loss: 0.0054642836626788436, run on the CPU with time 0.0640228999836836\n",
      "run 1050, Training loss: 0.005467866320131262, run on the CPU with time 0.06480990001000464\n",
      "run 1051, Training loss: 0.005512179677977904, run on the CPU with time 0.06572899999446236\n",
      "run 1052, Training loss: 0.005301041542869908, run on the CPU with time 0.06760829998529516\n",
      "run 1053, Training loss: 0.005167332309197677, run on the CPU with time 0.06737909998628311\n",
      "run 1054, Training loss: 0.005408393542298158, run on the CPU with time 0.0634991999831982\n",
      "run 1055, Training loss: 0.00576183284654028, run on the CPU with time 0.0646623000211548\n",
      "run 1056, Training loss: 0.005993694058005613, run on the CPU with time 0.061253399995621294\n",
      "run 1057, Training loss: 0.008615276541571472, run on the CPU with time 0.0644861999899149\n",
      "run 1058, Training loss: 0.007186146841426803, run on the CPU with time 0.06194750001304783\n",
      "run 1059, Training loss: 0.005641835105118596, run on the CPU with time 0.06161669999710284\n",
      "run 1060, Training loss: 0.0071539936341154815, run on the CPU with time 0.07841689998167567\n",
      "run 1061, Training loss: 0.005788687368674965, run on the CPU with time 0.07398720001219772\n",
      "run 1062, Training loss: 0.0054744366223034874, run on the CPU with time 0.06342079999740236\n",
      "run 1063, Training loss: 0.005187620065143247, run on the CPU with time 0.06467059999704361\n",
      "run 1064, Training loss: 0.005172818386927247, run on the CPU with time 0.06287749999319203\n",
      "run 1065, Training loss: 0.005221578243072145, run on the CPU with time 0.06710349998320453\n",
      "run 1066, Training loss: 0.005248056234003426, run on the CPU with time 0.06176069998764433\n",
      "run 1067, Training loss: 0.005290852327280763, run on the CPU with time 0.06278360000578687\n",
      "run 1068, Training loss: 0.005473331013292244, run on the CPU with time 0.06344239998725243\n",
      "run 1069, Training loss: 0.005347137068879833, run on the CPU with time 0.06778820001636632\n",
      "run 1070, Training loss: 0.005559706210624427, run on the CPU with time 0.06656800000928342\n",
      "run 1071, Training loss: 0.004988745413191447, run on the CPU with time 0.06967349999467842\n",
      "run 1072, Training loss: 0.005351447577149057, run on the CPU with time 0.0633601000008639\n",
      "run 1073, Training loss: 0.00574520437777127, run on the CPU with time 0.06339930000831373\n",
      "run 1074, Training loss: 0.00501824321902611, run on the CPU with time 0.061952599993674085\n",
      "run 1075, Training loss: 0.005212746650266292, run on the CPU with time 0.060979099973337725\n",
      "run 1076, Training loss: 0.005083797652994029, run on the CPU with time 0.0613741000124719\n",
      "run 1077, Training loss: 0.005147113035094332, run on the CPU with time 0.06369010001071729\n",
      "run 1078, Training loss: 0.005021901445169616, run on the CPU with time 0.06463209999492392\n",
      "run 1079, Training loss: 0.005254622588241049, run on the CPU with time 0.07001939998008311\n",
      "run 1080, Training loss: 0.005734980114026587, run on the CPU with time 0.06581390000064857\n",
      "run 1081, Training loss: 0.006513301158768379, run on the CPU with time 0.06413340001017787\n",
      "run 1082, Training loss: 0.005949387131047181, run on the CPU with time 0.06281130001298152\n",
      "run 1083, Training loss: 0.005941939747489497, run on the CPU with time 0.06276800000341609\n",
      "run 1084, Training loss: 0.005036758172156459, run on the CPU with time 0.06399650001549162\n",
      "run 1085, Training loss: 0.00504198693942354, run on the CPU with time 0.06386330001987517\n",
      "run 1086, Training loss: 0.004942157301940659, run on the CPU with time 0.06510269999853335\n",
      "run 1087, Training loss: 0.004717590894274921, run on the CPU with time 0.06447529999422841\n",
      "run 1088, Training loss: 0.004770137086267244, run on the CPU with time 0.062294900009874254\n",
      "run 1089, Training loss: 0.004722348800648681, run on the CPU with time 0.06370060000335798\n",
      "run 1090, Training loss: 0.004840125112836672, run on the CPU with time 0.06461719999788329\n",
      "run 1091, Training loss: 0.0048786510766314515, run on the CPU with time 0.061595499981194735\n",
      "run 1092, Training loss: 0.004835538821167905, run on the CPU with time 0.0617519999796059\n",
      "run 1093, Training loss: 0.004787176396083934, run on the CPU with time 0.06792420000419952\n",
      "run 1094, Training loss: 0.004959648560774936, run on the CPU with time 0.08018379998975433\n",
      "run 1095, Training loss: 0.004989550264144782, run on the CPU with time 0.06846229999791831\n",
      "run 1096, Training loss: 0.005573364948875017, run on the CPU with time 0.06318719999399036\n",
      "run 1097, Training loss: 0.005909361795056611, run on the CPU with time 0.0642468000005465\n",
      "run 1098, Training loss: 0.005510660345581445, run on the CPU with time 0.06521770000108518\n",
      "run 1099, Training loss: 0.005554843295778317, run on the CPU with time 0.061608999996678904\n",
      "run 1100, Training loss: 0.0046825221861416305, run on the CPU with time 0.06360019999556243\n",
      "run 1101, Training loss: 0.005379503658861557, run on the CPU with time 0.06057440000586212\n",
      "run 1102, Training loss: 0.004747647323496164, run on the CPU with time 0.0600611999980174\n",
      "run 1103, Training loss: 0.004670213480689902, run on the CPU with time 0.06045270001050085\n",
      "run 1104, Training loss: 0.004703125569821251, run on the CPU with time 0.06350759998895228\n",
      "run 1105, Training loss: 0.004626530183966018, run on the CPU with time 0.08328560000518337\n",
      "run 1106, Training loss: 0.004644576961238171, run on the CPU with time 0.07398440001998097\n",
      "run 1107, Training loss: 0.004539554320349866, run on the CPU with time 0.06792870000936091\n",
      "run 1108, Training loss: 0.004607803361829032, run on the CPU with time 0.06461380000109784\n",
      "run 1109, Training loss: 0.004868091602375816, run on the CPU with time 0.0635232999920845\n",
      "run 1110, Training loss: 0.0049366726309844206, run on the CPU with time 0.061758499999996275\n",
      "run 1111, Training loss: 0.0053372110166078944, run on the CPU with time 0.06452630000421777\n",
      "run 1112, Training loss: 0.004882357778577981, run on the CPU with time 0.060730099998181686\n",
      "run 1113, Training loss: 0.004573137775349261, run on the CPU with time 0.06168549999711104\n",
      "run 1114, Training loss: 0.004704515437267466, run on the CPU with time 0.06899920001160353\n",
      "run 1115, Training loss: 0.004612892684500283, run on the CPU with time 0.08648140000877902\n",
      "run 1116, Training loss: 0.004678063408672725, run on the CPU with time 0.066868300025817\n",
      "run 1117, Training loss: 0.004570148852442137, run on the CPU with time 0.06501180000486784\n",
      "run 1118, Training loss: 0.004665799846936187, run on the CPU with time 0.06366039998829365\n",
      "run 1119, Training loss: 0.00475389365403151, run on the CPU with time 0.06319190000067465\n",
      "run 1120, Training loss: 0.004669603369124657, run on the CPU with time 0.06397639997885562\n",
      "run 1121, Training loss: 0.0046255179392491385, run on the CPU with time 0.06294020000495948\n",
      "run 1122, Training loss: 0.004565655692501671, run on the CPU with time 0.06502909999107942\n",
      "run 1123, Training loss: 0.004595275008415973, run on the CPU with time 0.06623650001711212\n",
      "run 1124, Training loss: 0.004483950851400467, run on the CPU with time 0.06349760000011884\n",
      "run 1125, Training loss: 0.004421406654133038, run on the CPU with time 0.0616065000067465\n",
      "run 1126, Training loss: 0.004433056790466336, run on the CPU with time 0.061191199987661093\n",
      "run 1127, Training loss: 0.004667937227747065, run on the CPU with time 0.06303630000911653\n",
      "run 1128, Training loss: 0.004645186300331261, run on the CPU with time 0.062096100009512156\n",
      "run 1129, Training loss: 0.004619002094841562, run on the CPU with time 0.06344850000459701\n",
      "run 1130, Training loss: 0.0045703278166580605, run on the CPU with time 0.07253889998537488\n",
      "run 1131, Training loss: 0.004601971317474222, run on the CPU with time 0.07357860001502559\n",
      "run 1132, Training loss: 0.004560021841110607, run on the CPU with time 0.06309429998509586\n",
      "run 1133, Training loss: 0.0047638800778341565, run on the CPU with time 0.061577599990414456\n",
      "run 1134, Training loss: 0.004742689536545764, run on the CPU with time 0.06285419999039732\n",
      "run 1135, Training loss: 0.004482008587695997, run on the CPU with time 0.06483139999909326\n",
      "run 1136, Training loss: 0.004636710486374795, run on the CPU with time 0.06347720001940615\n",
      "run 1137, Training loss: 0.0058901690748329695, run on the CPU with time 0.06208239999250509\n",
      "run 1138, Training loss: 0.004825075609683567, run on the CPU with time 0.06162740002037026\n",
      "run 1139, Training loss: 0.004640848681182516, run on the CPU with time 0.0627047999878414\n",
      "run 1140, Training loss: 0.004603737825924658, run on the CPU with time 0.06552289999672212\n",
      "run 1141, Training loss: 0.004800580485872077, run on the CPU with time 0.06551179999951273\n",
      "run 1142, Training loss: 0.026218442447398873, run on the CPU with time 0.0676040000107605\n",
      "run 1143, Training loss: 0.014629420631999065, run on the CPU with time 0.06732300002477132\n",
      "run 1144, Training loss: 0.008196722354411825, run on the CPU with time 0.06353970000054687\n",
      "run 1145, Training loss: 0.006634308955769732, run on the CPU with time 0.06505549998837523\n",
      "run 1146, Training loss: 0.005408114067077721, run on the CPU with time 0.0612214999855496\n",
      "run 1147, Training loss: 0.005089567283126102, run on the CPU with time 0.06211220001569018\n",
      "run 1148, Training loss: 0.00520520689685575, run on the CPU with time 0.06237839997629635\n",
      "run 1149, Training loss: 0.00911888704365331, run on the CPU with time 0.0630325999809429\n",
      "run 1150, Training loss: 0.007386222933895293, run on the CPU with time 0.07734290001098998\n",
      "run 1151, Training loss: 0.013279909211401405, run on the CPU with time 0.07257069999468513\n",
      "run 1152, Training loss: 0.02521822865184566, run on the CPU with time 0.0663263000024017\n",
      "run 1153, Training loss: 0.00982108236163516, run on the CPU with time 0.06205520001822151\n",
      "run 1154, Training loss: 0.007628360538298942, run on the CPU with time 0.06250880000879988\n",
      "run 1155, Training loss: 0.006498496776277369, run on the CPU with time 0.06367889998364262\n",
      "run 1156, Training loss: 0.006645805111408911, run on the CPU with time 0.06023650002316572\n",
      "run 1157, Training loss: 0.007604196420024065, run on the CPU with time 0.06106829998316243\n",
      "run 1158, Training loss: 0.0063858748741701925, run on the CPU with time 0.06169969998882152\n",
      "run 1159, Training loss: 0.005254624340225498, run on the CPU with time 0.06179080001311377\n",
      "run 1160, Training loss: 0.00510906819333534, run on the CPU with time 0.07765459999791346\n",
      "run 1161, Training loss: 0.005074737721588463, run on the CPU with time 0.08054769999580458\n",
      "run 1162, Training loss: 0.005480254466634836, run on the CPU with time 0.06378379999659956\n",
      "run 1163, Training loss: 0.006309115282699002, run on the CPU with time 0.06499719998100773\n",
      "run 1164, Training loss: 0.005639053711836988, run on the CPU with time 0.06204540000180714\n",
      "run 1165, Training loss: 0.00574050809584812, run on the CPU with time 0.06321910000406206\n",
      "run 1166, Training loss: 0.00533559104766358, run on the CPU with time 0.061369000002741814\n",
      "run 1167, Training loss: 0.005121997782473706, run on the CPU with time 0.06167810002807528\n",
      "run 1168, Training loss: 0.004354332135170063, run on the CPU with time 0.06184090001625009\n",
      "run 1169, Training loss: 0.0044549904943613165, run on the CPU with time 0.06561789999250323\n",
      "run 1170, Training loss: 0.004422178351193328, run on the CPU with time 0.09326019999571145\n",
      "run 1171, Training loss: 0.004446570521527478, run on the CPU with time 0.06486179999774322\n",
      "run 1172, Training loss: 0.004793614267559447, run on the CPU with time 0.07064300001366064\n",
      "run 1173, Training loss: 0.00567862455659038, run on the CPU with time 0.06405909999739379\n",
      "run 1174, Training loss: 0.006468989324507261, run on the CPU with time 0.06367410000530072\n",
      "run 1175, Training loss: 0.008197393155371008, run on the CPU with time 0.06521100000827573\n",
      "run 1176, Training loss: 0.005793874258895151, run on the CPU with time 0.06657299998914823\n",
      "run 1177, Training loss: 0.004436207152321003, run on the CPU with time 0.06361690000630915\n",
      "run 1178, Training loss: 0.004645120403835211, run on the CPU with time 0.06425530000706203\n",
      "run 1179, Training loss: 0.004508059113339352, run on the CPU with time 0.06337750001694076\n",
      "run 1180, Training loss: 0.00486750748929229, run on the CPU with time 0.06396920001134276\n",
      "run 1181, Training loss: 0.0063831043218008495, run on the CPU with time 0.06513770000310615\n",
      "run 1182, Training loss: 0.006422958071131937, run on the CPU with time 0.08530909998808056\n",
      "run 1183, Training loss: 0.004880249040169557, run on the CPU with time 0.06404269998893142\n",
      "run 1184, Training loss: 0.004497093914224851, run on the CPU with time 0.06695059998310171\n",
      "run 1185, Training loss: 0.004423334183097309, run on the CPU with time 0.061872500024037436\n",
      "run 1186, Training loss: 0.004395810859577151, run on the CPU with time 0.0631952999974601\n",
      "run 1187, Training loss: 0.0043924150999042795, run on the CPU with time 0.06335300000500865\n",
      "run 1188, Training loss: 0.0053425075470427555, run on the CPU with time 0.06616290000965819\n",
      "run 1189, Training loss: 0.005412530847719278, run on the CPU with time 0.06682490001549013\n",
      "run 1190, Training loss: 0.004244314316681332, run on the CPU with time 0.06801200000336394\n",
      "run 1191, Training loss: 0.00425980212748982, run on the CPU with time 0.06853960000444204\n",
      "run 1192, Training loss: 0.004951903713729487, run on the CPU with time 0.06696379999630153\n",
      "run 1193, Training loss: 0.004354800663846122, run on the CPU with time 0.06981819999055006\n",
      "run 1194, Training loss: 0.004358588798313111, run on the CPU with time 0.06535829999484122\n",
      "run 1195, Training loss: 0.00545198855278696, run on the CPU with time 0.06374189999769442\n",
      "run 1196, Training loss: 0.0048013238326926845, run on the CPU with time 0.07187489999341778\n",
      "run 1197, Training loss: 0.004253409475230993, run on the CPU with time 0.06845329998759553\n",
      "run 1198, Training loss: 0.004200596525333822, run on the CPU with time 0.06515449998551048\n",
      "run 1199, Training loss: 0.004359277279846455, run on the CPU with time 0.06433850000030361\n",
      "run 1200, Training loss: 0.004426371265435656, run on the CPU with time 0.06323010000050999\n",
      "run 1201, Training loss: 0.0044312252538723194, run on the CPU with time 0.07334549998631701\n",
      "run 1202, Training loss: 0.004701509661655026, run on the CPU with time 0.06976810001651756\n",
      "run 1203, Training loss: 0.06939544954174463, run on the CPU with time 0.06850230001145974\n",
      "run 1204, Training loss: 0.2944867330381054, run on the CPU with time 0.06595749998814426\n",
      "run 1205, Training loss: 0.1574450933313611, run on the CPU with time 0.07064240000909194\n",
      "run 1206, Training loss: 0.05201868695730809, run on the CPU with time 0.07095600001048297\n",
      "run 1207, Training loss: 0.020195156372871927, run on the CPU with time 0.06457069999305531\n",
      "run 1208, Training loss: 0.01678082024287009, run on the CPU with time 0.06355680001433939\n",
      "run 1209, Training loss: 0.014600604172764261, run on the CPU with time 0.06504230000427924\n",
      "run 1210, Training loss: 0.010265353751181941, run on the CPU with time 0.062318400014191866\n",
      "run 1211, Training loss: 0.013314808676791886, run on the CPU with time 0.0698938999848906\n",
      "run 1212, Training loss: 0.008370372789795511, run on the CPU with time 0.06961469998350367\n",
      "run 1213, Training loss: 0.006156816878964575, run on the CPU with time 0.07064410002203658\n",
      "run 1214, Training loss: 0.006201805349635172, run on the CPU with time 0.07568219999666326\n",
      "run 1215, Training loss: 0.006947541760481809, run on the CPU with time 0.06533939999644645\n",
      "run 1216, Training loss: 0.005698199779991145, run on the CPU with time 0.06763030000729486\n",
      "run 1217, Training loss: 0.005552109501282261, run on the CPU with time 0.07030220000888221\n",
      "run 1218, Training loss: 0.005189899124459109, run on the CPU with time 0.0670111000072211\n",
      "run 1219, Training loss: 0.0050045963626334095, run on the CPU with time 0.06766050000442192\n",
      "run 1220, Training loss: 0.005459944409879179, run on the CPU with time 0.0680450999934692\n",
      "run 1221, Training loss: 0.005974767624337057, run on the CPU with time 0.06291639999835752\n",
      "run 1222, Training loss: 0.0058813523589792156, run on the CPU with time 0.07276340000680648\n",
      "run 1223, Training loss: 0.005340688198603775, run on the CPU with time 0.08634790001087822\n",
      "run 1224, Training loss: 0.005292055517466823, run on the CPU with time 0.0737758000032045\n",
      "run 1225, Training loss: 0.004893935613852756, run on the CPU with time 0.06353980000130832\n",
      "run 1226, Training loss: 0.0047814222640061585, run on the CPU with time 0.06612570001743734\n",
      "run 1227, Training loss: 0.00468271784299181, run on the CPU with time 0.06926910000038333\n",
      "run 1228, Training loss: 0.0048563959397142755, run on the CPU with time 0.061330399999860674\n",
      "run 1229, Training loss: 0.004886799024164, run on the CPU with time 0.063685400004033\n",
      "run 1230, Training loss: 0.005071757687892387, run on the CPU with time 0.06264230000670068\n",
      "run 1231, Training loss: 0.005540757477336394, run on the CPU with time 0.0652641000051517\n",
      "run 1232, Training loss: 0.004682409956215204, run on the CPU with time 0.08223269999143668\n",
      "run 1233, Training loss: 0.004584263609351844, run on the CPU with time 0.06838330000755377\n",
      "run 1234, Training loss: 0.004523561553849669, run on the CPU with time 0.06699620001018047\n",
      "run 1235, Training loss: 0.004405693896776277, run on the CPU with time 0.06538359998376109\n",
      "run 1236, Training loss: 0.00424939228264107, run on the CPU with time 0.06347190000815317\n",
      "run 1237, Training loss: 0.004384844818427651, run on the CPU with time 0.06255920001422055\n",
      "run 1238, Training loss: 0.004685393965866586, run on the CPU with time 0.06206309999106452\n",
      "run 1239, Training loss: 0.004127316329157276, run on the CPU with time 0.061786500009475276\n",
      "run 1240, Training loss: 0.004303463515613905, run on the CPU with time 0.07054519999655895\n",
      "run 1241, Training loss: 0.004060017908076671, run on the CPU with time 0.08183209999697283\n",
      "run 1242, Training loss: 0.00424008601853116, run on the CPU with time 0.06790100000216626\n",
      "run 1243, Training loss: 0.004420669944110242, run on the CPU with time 0.06958969999686815\n",
      "run 1244, Training loss: 0.0043526255811395295, run on the CPU with time 0.06521129998145625\n",
      "run 1245, Training loss: 0.004459359306466385, run on the CPU with time 0.06236519999220036\n",
      "run 1246, Training loss: 0.004423344167298638, run on the CPU with time 0.06178300001192838\n",
      "run 1247, Training loss: 0.004124201228312979, run on the CPU with time 0.06389560000388883\n",
      "run 1248, Training loss: 0.004177555841108022, run on the CPU with time 0.06246400001691654\n",
      "run 1249, Training loss: 0.004184215393830858, run on the CPU with time 0.06439990000217222\n",
      "run 1250, Training loss: 0.004168558807313358, run on the CPU with time 0.074830400000792\n",
      "run 1251, Training loss: 0.004041711288366721, run on the CPU with time 0.06535990000702441\n",
      "run 1252, Training loss: 0.003928942602942698, run on the CPU with time 0.06330879998859018\n",
      "run 1253, Training loss: 0.0038426644364252835, run on the CPU with time 0.06351059998269193\n",
      "run 1254, Training loss: 0.0038604856534882196, run on the CPU with time 0.06512559999828227\n",
      "run 1255, Training loss: 0.00387288121141451, run on the CPU with time 0.06360970000969246\n",
      "run 1256, Training loss: 0.003991366678118621, run on the CPU with time 0.06305200001224875\n",
      "run 1257, Training loss: 0.0040645129026167775, run on the CPU with time 0.06238710001343861\n",
      "run 1258, Training loss: 0.003984210612940263, run on the CPU with time 0.06409530001110397\n",
      "run 1259, Training loss: 0.004124293656257743, run on the CPU with time 0.09851169999456033\n",
      "run 1260, Training loss: 0.003963365731363989, run on the CPU with time 0.06719790000352077\n",
      "run 1261, Training loss: 0.004498769610654563, run on the CPU with time 0.06649380002636462\n",
      "run 1262, Training loss: 0.004442873002897778, run on the CPU with time 0.0632340999727603\n",
      "run 1263, Training loss: 0.003949613792991097, run on the CPU with time 0.06253850000211969\n",
      "run 1264, Training loss: 0.0038601842999923973, run on the CPU with time 0.06261700001778081\n",
      "run 1265, Training loss: 0.0038246860695918174, run on the CPU with time 0.0626361999893561\n",
      "run 1266, Training loss: 0.0038170812347247686, run on the CPU with time 0.06347829999867827\n",
      "run 1267, Training loss: 0.00378384107792623, run on the CPU with time 0.06806359998881817\n",
      "run 1268, Training loss: 0.0036741482621767897, run on the CPU with time 0.0813639999832958\n",
      "run 1269, Training loss: 0.0037429733646356248, run on the CPU with time 0.06547460000729188\n",
      "run 1270, Training loss: 0.0037195554638112132, run on the CPU with time 0.0661582000029739\n",
      "run 1271, Training loss: 0.0037074446341152493, run on the CPU with time 0.06334049999713898\n",
      "run 1272, Training loss: 0.0036828731723785907, run on the CPU with time 0.061917200015159324\n",
      "run 1273, Training loss: 0.0036507435636436146, run on the CPU with time 0.06359259999589995\n",
      "run 1274, Training loss: 0.003706038430259055, run on the CPU with time 0.0639274999848567\n",
      "run 1275, Training loss: 0.0035955815290270206, run on the CPU with time 0.062236899975687265\n",
      "run 1276, Training loss: 0.003640707748920911, run on the CPU with time 0.07389830000465736\n",
      "run 1277, Training loss: 0.0036334202066593043, run on the CPU with time 0.0703429999994114\n",
      "run 1278, Training loss: 0.003887180392418734, run on the CPU with time 0.07686120001017116\n",
      "run 1279, Training loss: 0.005246294676114551, run on the CPU with time 0.0642220999870915\n",
      "run 1280, Training loss: 0.0049011780615811325, run on the CPU with time 0.0685005999985151\n",
      "run 1281, Training loss: 0.008504897409875412, run on the CPU with time 0.06277859999681823\n",
      "run 1282, Training loss: 0.005349107067740988, run on the CPU with time 0.06546069998876192\n",
      "run 1283, Training loss: 0.004930848386985334, run on the CPU with time 0.06264719998580404\n",
      "run 1284, Training loss: 0.005871258098208769, run on the CPU with time 0.061813200009055436\n",
      "run 1285, Training loss: 0.0039306244577429345, run on the CPU with time 0.06235220000962727\n",
      "run 1286, Training loss: 0.003977599293020003, run on the CPU with time 0.06469520000973716\n",
      "run 1287, Training loss: 0.0037375840642859906, run on the CPU with time 0.07166029998916201\n",
      "run 1288, Training loss: 0.0037603383002781563, run on the CPU with time 0.0652371000032872\n",
      "run 1289, Training loss: 0.003718819531389851, run on the CPU with time 0.06359899998642504\n",
      "run 1290, Training loss: 0.004017939878923988, run on the CPU with time 0.06997290000435896\n",
      "run 1291, Training loss: 0.005257467262113509, run on the CPU with time 0.06324610000592656\n",
      "run 1292, Training loss: 0.004736413017500044, run on the CPU with time 0.061822299991035834\n",
      "run 1293, Training loss: 0.005677290238699326, run on the CPU with time 0.06219500000588596\n",
      "run 1294, Training loss: 0.00492749140328008, run on the CPU with time 0.06372400000691414\n",
      "run 1295, Training loss: 0.004206328341388144, run on the CPU with time 0.08134299999801442\n",
      "run 1296, Training loss: 0.004193217899988998, run on the CPU with time 0.07842430000891909\n",
      "run 1297, Training loss: 0.004366597294442313, run on the CPU with time 0.06745140001294203\n",
      "run 1298, Training loss: 0.0040531071686101235, run on the CPU with time 0.06550570001127198\n",
      "run 1299, Training loss: 0.003892870136504908, run on the CPU with time 0.06654050000361167\n",
      "run 1300, Training loss: 0.003975352009398524, run on the CPU with time 0.06262489999062382\n",
      "run 1301, Training loss: 0.0043552369297355075, run on the CPU with time 0.06310249998932704\n",
      "run 1302, Training loss: 0.004089777310516431, run on the CPU with time 0.06265310000162572\n",
      "run 1303, Training loss: 0.003845635855528103, run on the CPU with time 0.06206159997964278\n",
      "run 1304, Training loss: 0.0040305942904854996, run on the CPU with time 0.06256369999027811\n",
      "run 1305, Training loss: 0.0036455406720051543, run on the CPU with time 0.06320209999103099\n",
      "run 1306, Training loss: 0.003777121077953118, run on the CPU with time 0.06669969999347813\n",
      "run 1307, Training loss: 0.0037904865097847175, run on the CPU with time 0.08955599999171682\n",
      "run 1308, Training loss: 0.0036563350950018503, run on the CPU with time 0.07303679999313317\n",
      "run 1309, Training loss: 0.003742512722055835, run on the CPU with time 0.06341709999833256\n",
      "run 1310, Training loss: 0.0037511928656551227, run on the CPU with time 0.06660610000835732\n",
      "run 1311, Training loss: 0.0038028006291080437, run on the CPU with time 0.06341909998445772\n",
      "run 1312, Training loss: 0.0037278341946975243, run on the CPU with time 0.06360869997297414\n",
      "run 1313, Training loss: 0.003928921703217466, run on the CPU with time 0.06946850000531413\n",
      "run 1314, Training loss: 0.0040443443923405455, run on the CPU with time 0.06323870000778697\n",
      "run 1315, Training loss: 0.004395835437489123, run on the CPU with time 0.06665399999474175\n",
      "run 1316, Training loss: 0.003977144631260836, run on the CPU with time 0.06636540000909008\n",
      "run 1317, Training loss: 0.004032250417301177, run on the CPU with time 0.06692860001930967\n",
      "run 1318, Training loss: 0.004133630559292876, run on the CPU with time 0.07026810001116246\n",
      "run 1319, Training loss: 0.0036909461132547056, run on the CPU with time 0.06636619998607785\n",
      "run 1320, Training loss: 0.003798375917143527, run on the CPU with time 0.06445130001520738\n",
      "run 1321, Training loss: 0.004028958530927247, run on the CPU with time 0.06133550000959076\n",
      "run 1322, Training loss: 0.004406694333143109, run on the CPU with time 0.06226460001198575\n",
      "run 1323, Training loss: 0.004217988471712239, run on the CPU with time 0.0637189999979455\n",
      "run 1324, Training loss: 0.0038892363870218103, run on the CPU with time 0.06702129999757744\n",
      "run 1325, Training loss: 0.0037952805353349754, run on the CPU with time 0.07749180000973865\n",
      "run 1326, Training loss: 0.0037179538339594067, run on the CPU with time 0.0653545000241138\n",
      "run 1327, Training loss: 0.003725994372242977, run on the CPU with time 0.06666690000565723\n",
      "run 1328, Training loss: 0.0037635501587911595, run on the CPU with time 0.06605349999153987\n",
      "run 1329, Training loss: 0.003709089828639249, run on the CPU with time 0.06973050002125092\n",
      "run 1330, Training loss: 0.0037463328607951884, run on the CPU with time 0.06714889997965656\n",
      "run 1331, Training loss: 0.0036643860837846827, run on the CPU with time 0.06456170001183636\n",
      "run 1332, Training loss: 0.0035048015062453816, run on the CPU with time 0.06362880000961013\n",
      "run 1333, Training loss: 0.003487258677804758, run on the CPU with time 0.0628267000138294\n",
      "run 1334, Training loss: 0.0036293618407481436, run on the CPU with time 0.0725586999906227\n",
      "run 1335, Training loss: 0.0038100583873033017, run on the CPU with time 0.06989139999495819\n",
      "run 1336, Training loss: 0.003461597068764439, run on the CPU with time 0.06742400000803173\n",
      "run 1337, Training loss: 0.003521601890514086, run on the CPU with time 0.06506659998558462\n",
      "run 1338, Training loss: 0.0034558373016559266, run on the CPU with time 0.06379270000616089\n",
      "run 1339, Training loss: 0.0035021356158805163, run on the CPU with time 0.06810539998696186\n",
      "run 1340, Training loss: 0.00363758050953038, run on the CPU with time 0.06602540001040325\n",
      "run 1341, Training loss: 0.003917705487798561, run on the CPU with time 0.06361159999505617\n",
      "run 1342, Training loss: 0.0036710031700997866, run on the CPU with time 0.06390089998603798\n",
      "run 1343, Training loss: 0.003591047888568772, run on the CPU with time 0.06448760000057518\n",
      "run 1344, Training loss: 0.0034781914109771606, run on the CPU with time 0.06297790000098757\n",
      "run 1345, Training loss: 0.0033301095795733007, run on the CPU with time 0.07224289997247979\n",
      "run 1346, Training loss: 0.003280601376371289, run on the CPU with time 0.07565660000545904\n",
      "run 1347, Training loss: 0.003251985825905153, run on the CPU with time 0.06452399998670444\n",
      "run 1348, Training loss: 0.0034677557485834273, run on the CPU with time 0.06466140001430176\n",
      "run 1349, Training loss: 0.003405155401825058, run on the CPU with time 0.06364899998879991\n",
      "run 1350, Training loss: 0.003298084702162834, run on the CPU with time 0.06244790001073852\n",
      "run 1351, Training loss: 0.003260181707181883, run on the CPU with time 0.06417159998090938\n",
      "run 1352, Training loss: 0.0032662465276768094, run on the CPU with time 0.06046099998638965\n",
      "run 1353, Training loss: 0.00333699112736874, run on the CPU with time 0.06033700000261888\n",
      "run 1354, Training loss: 0.003299294779807579, run on the CPU with time 0.07138670000131242\n",
      "run 1355, Training loss: 0.0032158711593100716, run on the CPU with time 0.062133300001733005\n",
      "run 1356, Training loss: 0.0031776303954757167, run on the CPU with time 0.06164080000598915\n",
      "run 1357, Training loss: 0.0032239112495021384, run on the CPU with time 0.07085709998500533\n",
      "run 1358, Training loss: 0.0031701100704429504, run on the CPU with time 0.06884990000980906\n",
      "run 1359, Training loss: 0.003312052720205181, run on the CPU with time 0.06425320002017543\n",
      "run 1360, Training loss: 0.0031855282277154567, run on the CPU with time 0.06292409999878146\n",
      "run 1361, Training loss: 0.0031576116491289045, run on the CPU with time 0.06179520001751371\n",
      "run 1362, Training loss: 0.003222137707052752, run on the CPU with time 0.061340799991739914\n",
      "run 1363, Training loss: 0.003201643615929325, run on the CPU with time 0.06379580000066198\n",
      "run 1364, Training loss: 0.0031600482034264132, run on the CPU with time 0.06146150000859052\n",
      "run 1365, Training loss: 0.0031409294677029144, run on the CPU with time 0.06468400001176633\n",
      "run 1366, Training loss: 0.003234538401540538, run on the CPU with time 0.07329000000027008\n",
      "run 1367, Training loss: 0.003444671820034273, run on the CPU with time 0.06599780000397004\n",
      "run 1368, Training loss: 0.0032053848180856387, run on the CPU with time 0.06684200000017881\n",
      "run 1369, Training loss: 0.003197345855667002, run on the CPU with time 0.06208410000544973\n",
      "run 1370, Training loss: 0.0032296501216478647, run on the CPU with time 0.06205869998666458\n",
      "run 1371, Training loss: 0.003125977344726297, run on the CPU with time 0.06330840001464821\n",
      "run 1372, Training loss: 0.0030730762431630866, run on the CPU with time 0.06109840000863187\n",
      "run 1373, Training loss: 0.0031322301366758024, run on the CPU with time 0.06190109997987747\n",
      "run 1374, Training loss: 0.0030899676782692865, run on the CPU with time 0.06412789999740198\n",
      "run 1375, Training loss: 0.0031104615138081663, run on the CPU with time 0.06426149999606423\n",
      "run 1376, Training loss: 0.0031303125306625258, run on the CPU with time 0.06660690001444891\n",
      "run 1377, Training loss: 0.0030880568924740973, run on the CPU with time 0.06179459998384118\n",
      "run 1378, Training loss: 0.003113487435356629, run on the CPU with time 0.06503920000977814\n",
      "run 1379, Training loss: 0.003105005990735001, run on the CPU with time 0.0638154000043869\n",
      "run 1380, Training loss: 0.003105414858278395, run on the CPU with time 0.06165839999448508\n",
      "run 1381, Training loss: 0.0031375285153361883, run on the CPU with time 0.07042830000864342\n",
      "run 1382, Training loss: 0.0032532069421987134, run on the CPU with time 0.06634779999149032\n",
      "run 1383, Training loss: 0.0032209227946375244, run on the CPU with time 0.06523310000193305\n",
      "run 1384, Training loss: 0.0035623612122435587, run on the CPU with time 0.06252279999898747\n",
      "run 1385, Training loss: 0.003385663892657414, run on the CPU with time 0.06149220000952482\n",
      "run 1386, Training loss: 0.00333628702765881, run on the CPU with time 0.06162249998305924\n",
      "run 1387, Training loss: 0.0032570450180421837, run on the CPU with time 0.06217300001299009\n",
      "run 1388, Training loss: 0.0032138975673164662, run on the CPU with time 0.06108230000245385\n",
      "run 1389, Training loss: 0.0031241351464731536, run on the CPU with time 0.06304040001123212\n",
      "run 1390, Training loss: 0.0031513339609690856, run on the CPU with time 0.06430249998811632\n",
      "run 1391, Training loss: 0.0031006832837275314, run on the CPU with time 0.07891539999400266\n",
      "run 1392, Training loss: 0.003100626350549812, run on the CPU with time 0.06445330000133254\n",
      "run 1393, Training loss: 0.003019173897072588, run on the CPU with time 0.06180339999264106\n",
      "run 1394, Training loss: 0.003035063205011697, run on the CPU with time 0.06287500000325963\n",
      "run 1395, Training loss: 0.00300367602348243, run on the CPU with time 0.06566630001179874\n",
      "run 1396, Training loss: 0.003017954623580656, run on the CPU with time 0.06272679998073727\n",
      "run 1397, Training loss: 0.002986092238411815, run on the CPU with time 0.06078729999717325\n",
      "run 1398, Training loss: 0.003000023589728781, run on the CPU with time 0.06032479999703355\n",
      "run 1399, Training loss: 0.0029844742501154544, run on the CPU with time 0.08449039998231456\n",
      "run 1400, Training loss: 0.003002991756048604, run on the CPU with time 0.06578760000411421\n",
      "run 1401, Training loss: 0.0029862776983381164, run on the CPU with time 0.06212089999462478\n",
      "run 1402, Training loss: 0.0031207368973727254, run on the CPU with time 0.06220939999911934\n",
      "run 1403, Training loss: 0.0029639151045227083, run on the CPU with time 0.0645414000027813\n",
      "run 1404, Training loss: 0.0029651735550776884, run on the CPU with time 0.062422099988907576\n",
      "run 1405, Training loss: 0.0031162206734403628, run on the CPU with time 0.06150149999302812\n",
      "run 1406, Training loss: 0.0029633183106356725, run on the CPU with time 0.062800799991237\n",
      "run 1407, Training loss: 0.0031394781005060807, run on the CPU with time 0.06409460000577383\n",
      "run 1408, Training loss: 0.003190517958897875, run on the CPU with time 0.07029040000634268\n",
      "run 1409, Training loss: 0.0033782573214425197, run on the CPU with time 0.06526229999144562\n",
      "run 1410, Training loss: 0.0032520517740605016, run on the CPU with time 0.06342760002007708\n",
      "run 1411, Training loss: 0.0032711578263181517, run on the CPU with time 0.06474800000432879\n",
      "run 1412, Training loss: 0.003210819526099261, run on the CPU with time 0.06266039999900386\n",
      "run 1413, Training loss: 0.0031270124298482024, run on the CPU with time 0.0626398999884259\n",
      "run 1414, Training loss: 0.0031045243531645445, run on the CPU with time 0.06126240000594407\n",
      "run 1415, Training loss: 0.003079969200176823, run on the CPU with time 0.061607500014360994\n",
      "run 1416, Training loss: 0.003169040082138963, run on the CPU with time 0.06196319998707622\n",
      "run 1417, Training loss: 0.0031006492745787413, run on the CPU with time 0.0646134999988135\n",
      "run 1418, Training loss: 0.003264443155917847, run on the CPU with time 0.08214300000690855\n",
      "run 1419, Training loss: 0.003146120569991498, run on the CPU with time 0.06563029999961145\n",
      "run 1420, Training loss: 0.003138696285631423, run on the CPU with time 0.0637097000144422\n",
      "run 1421, Training loss: 0.0031031323297859422, run on the CPU with time 0.06410279998090118\n",
      "run 1422, Training loss: 0.003184715406545861, run on the CPU with time 0.06436920000123791\n",
      "run 1423, Training loss: 0.0032862451795319264, run on the CPU with time 0.06315950001589954\n",
      "run 1424, Training loss: 0.003366484835648655, run on the CPU with time 0.06316019999212585\n",
      "run 1425, Training loss: 0.003168538451559884, run on the CPU with time 0.06142930002533831\n",
      "run 1426, Training loss: 0.003046993861392945, run on the CPU with time 0.06221249999362044\n",
      "run 1427, Training loss: 0.003083970832416195, run on the CPU with time 0.06231789998128079\n",
      "run 1428, Training loss: 0.0030686479982581327, run on the CPU with time 0.0615809999871999\n",
      "run 1429, Training loss: 0.0030441878429635174, run on the CPU with time 0.06879240000853315\n",
      "run 1430, Training loss: 0.0032204620346998458, run on the CPU with time 0.06988860000274144\n",
      "run 1431, Training loss: 0.0029920285276602955, run on the CPU with time 0.06401060000644065\n",
      "run 1432, Training loss: 0.0029626610019496463, run on the CPU with time 0.06351639999775216\n",
      "run 1433, Training loss: 0.002958440682744946, run on the CPU with time 0.06301820001681335\n",
      "run 1434, Training loss: 0.0029026105027614075, run on the CPU with time 0.07584859998314641\n",
      "run 1435, Training loss: 0.0029774237122513693, run on the CPU with time 0.066815600002883\n",
      "run 1436, Training loss: 0.0030943506106268616, run on the CPU with time 0.06371660000877455\n",
      "run 1437, Training loss: 0.0029958357129626994, run on the CPU with time 0.06196079999790527\n",
      "run 1438, Training loss: 0.0030867384680525654, run on the CPU with time 0.06336460000602528\n",
      "run 1439, Training loss: 0.00320361467536052, run on the CPU with time 0.06186469999374822\n",
      "run 1440, Training loss: 0.0030422667798120527, run on the CPU with time 0.061303399997996166\n",
      "run 1441, Training loss: 0.0031113527109317314, run on the CPU with time 0.06334110000170767\n",
      "run 1442, Training loss: 0.0030836177034705707, run on the CPU with time 0.0701384000130929\n",
      "run 1443, Training loss: 0.003149657918468372, run on the CPU with time 0.0638409000239335\n",
      "run 1444, Training loss: 0.0031379041508030653, run on the CPU with time 0.06395849998807535\n",
      "run 1445, Training loss: 0.0030644063909262926, run on the CPU with time 0.06401179998647422\n",
      "run 1446, Training loss: 0.00303563194016037, run on the CPU with time 0.06468929999391548\n",
      "run 1447, Training loss: 0.0031164391532787373, run on the CPU with time 0.06215160002466291\n",
      "run 1448, Training loss: 0.003180245062586354, run on the CPU with time 0.06197959999553859\n",
      "run 1449, Training loss: 0.0029399008201752706, run on the CPU with time 0.06150549999438226\n",
      "run 1450, Training loss: 0.003141481667989865, run on the CPU with time 0.06343219999689609\n",
      "run 1451, Training loss: 0.003049802319120235, run on the CPU with time 0.06137569999555126\n",
      "run 1452, Training loss: 0.0031362846551060515, run on the CPU with time 0.08161559997824952\n",
      "run 1453, Training loss: 0.002923914026177954, run on the CPU with time 0.06858869999996386\n",
      "run 1454, Training loss: 0.002884793464115567, run on the CPU with time 0.06324320001294836\n",
      "run 1455, Training loss: 0.002878233278434808, run on the CPU with time 0.06088619999354705\n",
      "run 1456, Training loss: 0.0028408837599933827, run on the CPU with time 0.06184670000220649\n",
      "run 1457, Training loss: 0.0029838556132745, run on the CPU with time 0.06338970002252609\n",
      "run 1458, Training loss: 0.0029456849368712443, run on the CPU with time 0.06369849998736754\n",
      "run 1459, Training loss: 0.002930951249759966, run on the CPU with time 0.07221049998770468\n",
      "run 1460, Training loss: 0.002842860384209251, run on the CPU with time 0.06179199999314733\n",
      "run 1461, Training loss: 0.002842427926357645, run on the CPU with time 0.06399920000694692\n",
      "run 1462, Training loss: 0.0028395815707468004, run on the CPU with time 0.06161720000091009\n",
      "run 1463, Training loss: 0.0028692561238792473, run on the CPU with time 0.061576099978992715\n",
      "run 1464, Training loss: 0.0028421083933525635, run on the CPU with time 0.0627208000223618\n",
      "run 1465, Training loss: 0.0029065632054963235, run on the CPU with time 0.07264259998919442\n",
      "run 1466, Training loss: 0.002895001174659807, run on the CPU with time 0.07060870001441799\n",
      "run 1467, Training loss: 0.002782867321829227, run on the CPU with time 0.06390159999136813\n",
      "run 1468, Training loss: 0.0028754470544465056, run on the CPU with time 0.06412979998276569\n",
      "run 1469, Training loss: 0.0028375411218307402, run on the CPU with time 0.06864399998448789\n",
      "run 1470, Training loss: 0.0028111957374759105, run on the CPU with time 0.06416270000045188\n",
      "run 1471, Training loss: 0.0027958089017457413, run on the CPU with time 0.0688611000077799\n",
      "run 1472, Training loss: 0.002736643449249889, run on the CPU with time 0.0666635999805294\n",
      "run 1473, Training loss: 0.0028004055541135707, run on the CPU with time 0.06290039999294095\n",
      "run 1474, Training loss: 0.0027722319431962785, run on the CPU with time 0.061325799993937835\n",
      "run 1475, Training loss: 0.0028231972296701067, run on the CPU with time 0.06735820000176318\n",
      "run 1476, Training loss: 0.0028175704339942474, run on the CPU with time 0.07473010002286173\n",
      "run 1477, Training loss: 0.002841886623222804, run on the CPU with time 0.06417009999859147\n",
      "run 1478, Training loss: 0.00285438928761075, run on the CPU with time 0.06304440001258627\n",
      "run 1479, Training loss: 0.0028541714254639704, run on the CPU with time 0.06134469999233261\n",
      "run 1480, Training loss: 0.002986954350490123, run on the CPU with time 0.06078030000207946\n",
      "run 1481, Training loss: 0.0029520639012017374, run on the CPU with time 0.06184680000296794\n",
      "run 1482, Training loss: 0.0028077747115854766, run on the CPU with time 0.06087029998889193\n",
      "run 1483, Training loss: 0.0027763846034014766, run on the CPU with time 0.060430899990024045\n",
      "run 1484, Training loss: 0.0027070716338047614, run on the CPU with time 0.06152369998744689\n",
      "run 1485, Training loss: 0.0027882036947465333, run on the CPU with time 0.07768159999977797\n",
      "run 1486, Training loss: 0.0027507074394220996, run on the CPU with time 0.07305119998636656\n",
      "run 1487, Training loss: 0.0030214099628871984, run on the CPU with time 0.06343410001136363\n",
      "run 1488, Training loss: 0.002812203008745293, run on the CPU with time 0.06453559998772107\n",
      "run 1489, Training loss: 0.002779586429559541, run on the CPU with time 0.062450500001432374\n",
      "run 1490, Training loss: 0.0027056207581402057, run on the CPU with time 0.06025819998467341\n",
      "run 1491, Training loss: 0.002734354600331492, run on the CPU with time 0.05986449998454191\n",
      "run 1492, Training loss: 0.0027549808248295447, run on the CPU with time 0.06121210000128485\n",
      "run 1493, Training loss: 0.002729153316150504, run on the CPU with time 0.07162000000244007\n",
      "run 1494, Training loss: 0.002740060948417522, run on the CPU with time 0.0744037999829743\n",
      "run 1495, Training loss: 0.0027128915885854935, run on the CPU with time 0.06209819999639876\n",
      "run 1496, Training loss: 0.0028168706424449657, run on the CPU with time 0.06149089999962598\n",
      "run 1497, Training loss: 0.002895881697556681, run on the CPU with time 0.06285269997897558\n",
      "run 1498, Training loss: 0.0028181705630479633, run on the CPU with time 0.06148699999903329\n",
      "run 1499, Training loss: 0.00278524144272193, run on the CPU with time 0.061251700011780486\n",
      "run 1500, Training loss: 0.0028237921181719435, run on the CPU with time 0.061356899997917935\n",
      "run 1501, Training loss: 0.002789191422396636, run on the CPU with time 0.06310559998382814\n",
      "run 1502, Training loss: 0.0027276134756737684, run on the CPU with time 0.06167279998771846\n",
      "run 1503, Training loss: 0.0027128092031820086, run on the CPU with time 0.07002809998812154\n",
      "run 1504, Training loss: 0.002683115445093294, run on the CPU with time 0.08628350001526996\n",
      "run 1505, Training loss: 0.0026801669734678314, run on the CPU with time 0.06905779999215156\n",
      "run 1506, Training loss: 0.0026917947036880917, run on the CPU with time 0.06323470000643283\n",
      "run 1507, Training loss: 0.0026699896157052453, run on the CPU with time 0.06210900002042763\n",
      "run 1508, Training loss: 0.0026680324832506646, run on the CPU with time 0.06246759998612106\n",
      "run 1509, Training loss: 0.0026440760897954037, run on the CPU with time 0.06152230000589043\n",
      "run 1510, Training loss: 0.002626841798138974, run on the CPU with time 0.06038599999737926\n",
      "run 1511, Training loss: 0.002649532722584395, run on the CPU with time 0.06055860000196844\n",
      "run 1512, Training loss: 0.002618011044317179, run on the CPU with time 0.06380570001783781\n",
      "run 1513, Training loss: 0.002613060311443085, run on the CPU with time 0.07961270000669174\n",
      "run 1514, Training loss: 0.002590262198845685, run on the CPU with time 0.06483170000137761\n",
      "run 1515, Training loss: 0.002612252011154355, run on the CPU with time 0.0632661999843549\n",
      "run 1516, Training loss: 0.0026367315347835592, run on the CPU with time 0.06194400001550093\n",
      "run 1517, Training loss: 0.002629505980505862, run on the CPU with time 0.06590879999566823\n",
      "run 1518, Training loss: 0.0026044774597308294, run on the CPU with time 0.06133959998260252\n",
      "run 1519, Training loss: 0.002593618087716591, run on the CPU with time 0.06310669999220408\n",
      "run 1520, Training loss: 0.0025957322581565347, run on the CPU with time 0.06045720001566224\n",
      "run 1521, Training loss: 0.002786293342201547, run on the CPU with time 0.0761484999966342\n",
      "run 1522, Training loss: 0.0026521976465697992, run on the CPU with time 0.07118289999198169\n",
      "run 1523, Training loss: 0.0026268833959792654, run on the CPU with time 0.06373249998432584\n",
      "run 1524, Training loss: 0.002670048365226566, run on the CPU with time 0.06158300000242889\n",
      "run 1525, Training loss: 0.0026440871247111566, run on the CPU with time 0.06263230001786724\n",
      "run 1526, Training loss: 0.0025959158265984363, run on the CPU with time 0.06549029998132028\n",
      "run 1527, Training loss: 0.0025922745897498153, run on the CPU with time 0.06383769999956712\n",
      "run 1528, Training loss: 0.0025739923901379703, run on the CPU with time 0.06236619999981485\n",
      "run 1529, Training loss: 0.0028043647238519044, run on the CPU with time 0.06193450000137091\n",
      "run 1530, Training loss: 0.0029456959698687902, run on the CPU with time 0.06965049999416806\n",
      "run 1531, Training loss: 0.0025646944310623934, run on the CPU with time 0.06456280002021231\n",
      "run 1532, Training loss: 0.002611835448029028, run on the CPU with time 0.06518969999160618\n",
      "run 1533, Training loss: 0.002569514499654443, run on the CPU with time 0.06259099999442697\n",
      "run 1534, Training loss: 0.0026033460966904056, run on the CPU with time 0.061805699981050566\n",
      "run 1535, Training loss: 0.0025697461593980815, run on the CPU with time 0.06376240000827238\n",
      "run 1536, Training loss: 0.0025667874814561484, run on the CPU with time 0.06234110001241788\n",
      "run 1537, Training loss: 0.0025482564787539146, run on the CPU with time 0.06354410000494681\n",
      "run 1538, Training loss: 0.00256398189711614, run on the CPU with time 0.06439700000919402\n",
      "run 1539, Training loss: 0.0025691740953334373, run on the CPU with time 0.07662769997841679\n",
      "run 1540, Training loss: 0.002550701453583315, run on the CPU with time 0.06447620000108145\n",
      "run 1541, Training loss: 0.0025306068356175356, run on the CPU with time 0.06393170001683757\n",
      "run 1542, Training loss: 0.002534630811715033, run on the CPU with time 0.06330359997809865\n",
      "run 1543, Training loss: 0.00252128176454624, run on the CPU with time 0.06313610001234338\n",
      "run 1544, Training loss: 0.002573332783272649, run on the CPU with time 0.06204520000028424\n",
      "run 1545, Training loss: 0.002523552450756225, run on the CPU with time 0.061614799982635304\n",
      "run 1546, Training loss: 0.002536667304494503, run on the CPU with time 0.06289670002297498\n",
      "run 1547, Training loss: 0.0025143261143553406, run on the CPU with time 0.06405899999663234\n",
      "run 1548, Training loss: 0.002526129312503723, run on the CPU with time 0.060651799984043464\n",
      "run 1549, Training loss: 0.002521413740694036, run on the CPU with time 0.06798379999236204\n",
      "run 1550, Training loss: 0.002534300069435796, run on the CPU with time 0.07174399998621084\n",
      "run 1551, Training loss: 0.0025180730016886215, run on the CPU with time 0.06672880001133308\n",
      "run 1552, Training loss: 0.0025102535719104873, run on the CPU with time 0.0631825000164099\n",
      "run 1553, Training loss: 0.0025326044965450737, run on the CPU with time 0.0624627000070177\n",
      "run 1554, Training loss: 0.0025109701577445197, run on the CPU with time 0.060893099987879395\n",
      "run 1555, Training loss: 0.002498692810331704, run on the CPU with time 0.06131009999080561\n",
      "run 1556, Training loss: 0.0025209587103083983, run on the CPU with time 0.061371599993435666\n",
      "run 1557, Training loss: 0.002486363459849434, run on the CPU with time 0.06652390002273023\n",
      "run 1558, Training loss: 0.002516956551169807, run on the CPU with time 0.06855289998929948\n",
      "run 1559, Training loss: 0.0024856780491642316, run on the CPU with time 0.06318260001717135\n",
      "run 1560, Training loss: 0.0024991762815096247, run on the CPU with time 0.06270130001939833\n",
      "run 1561, Training loss: 0.002497624591019716, run on the CPU with time 0.06256540000322275\n",
      "run 1562, Training loss: 0.0025019220753165427, run on the CPU with time 0.06168459999025799\n",
      "run 1563, Training loss: 0.0024778389210537584, run on the CPU with time 0.07567240000935271\n",
      "run 1564, Training loss: 0.0024656613846838643, run on the CPU with time 0.0655432999774348\n",
      "run 1565, Training loss: 0.0024676219168694303, run on the CPU with time 0.06231789998128079\n",
      "run 1566, Training loss: 0.002483982675502458, run on the CPU with time 0.061850199999753386\n",
      "run 1567, Training loss: 0.002488659816233187, run on the CPU with time 0.0662300999974832\n",
      "run 1568, Training loss: 0.0024646719729802995, run on the CPU with time 0.06265649999841116\n",
      "run 1569, Training loss: 0.0024997271364554765, run on the CPU with time 0.06182839997927658\n",
      "run 1570, Training loss: 0.0024727871859795414, run on the CPU with time 0.06369879998965189\n",
      "run 1571, Training loss: 0.0024749709596190686, run on the CPU with time 0.07110810000449419\n",
      "run 1572, Training loss: 0.00246375854684712, run on the CPU with time 0.06360710001899861\n",
      "run 1573, Training loss: 0.0024559730716811664, run on the CPU with time 0.06417130000772886\n",
      "run 1574, Training loss: 0.002449282630219717, run on the CPU with time 0.06340249997447245\n",
      "run 1575, Training loss: 0.0024783217145754447, run on the CPU with time 0.06271800000104122\n",
      "run 1576, Training loss: 0.0024701610718197613, run on the CPU with time 0.062189399992348626\n",
      "run 1577, Training loss: 0.0024608273249627512, run on the CPU with time 0.06478760001482442\n",
      "run 1578, Training loss: 0.0024591308971470094, run on the CPU with time 0.06286869998439215\n",
      "run 1579, Training loss: 0.0024575900500746106, run on the CPU with time 0.0766983000212349\n",
      "run 1580, Training loss: 0.0024215941365003926, run on the CPU with time 0.06622070001321845\n",
      "run 1581, Training loss: 0.0024289834610085976, run on the CPU with time 0.07353160000639036\n",
      "run 1582, Training loss: 0.002454014245449798, run on the CPU with time 0.06282560000545345\n",
      "run 1583, Training loss: 0.002441021248920482, run on the CPU with time 0.06566019999445416\n",
      "run 1584, Training loss: 0.0024632219813859346, run on the CPU with time 0.06143460000748746\n",
      "run 1585, Training loss: 0.002454886175242295, run on the CPU with time 0.06142440001713112\n",
      "run 1586, Training loss: 0.0024748551222728565, run on the CPU with time 0.06194039998808876\n",
      "run 1587, Training loss: 0.002561678045259958, run on the CPU with time 0.0620432999858167\n",
      "run 1588, Training loss: 0.0027065679176964544, run on the CPU with time 0.07895610001287423\n",
      "run 1589, Training loss: 0.0028578705680874093, run on the CPU with time 0.06716740000410937\n",
      "run 1590, Training loss: 0.002799695217452774, run on the CPU with time 0.06536490001599304\n",
      "run 1591, Training loss: 0.0026629797290462936, run on the CPU with time 0.06271249998826534\n",
      "run 1592, Training loss: 0.0025777639120034028, run on the CPU with time 0.06215949999750592\n",
      "run 1593, Training loss: 0.002572584105655551, run on the CPU with time 0.062201499997172505\n",
      "run 1594, Training loss: 0.0025876575664204376, run on the CPU with time 0.061561299982713535\n",
      "run 1595, Training loss: 0.0025580099685943093, run on the CPU with time 0.06113539999932982\n",
      "run 1596, Training loss: 0.0024587593697519465, run on the CPU with time 0.06382189999567345\n",
      "run 1597, Training loss: 0.002452957819840859, run on the CPU with time 0.06600190000608563\n",
      "run 1598, Training loss: 0.0024135487619787455, run on the CPU with time 0.07354029998532496\n",
      "run 1599, Training loss: 0.0024104038221676918, run on the CPU with time 0.062186600000131875\n",
      "run 1600, Training loss: 0.002506251045931342, run on the CPU with time 0.06907720002345741\n",
      "run 1601, Training loss: 0.0023782299688636242, run on the CPU with time 0.06257770000956953\n",
      "run 1602, Training loss: 0.002382112364284694, run on the CPU with time 0.06148079998092726\n",
      "run 1603, Training loss: 0.0023746428414597176, run on the CPU with time 0.06125420000171289\n",
      "run 1604, Training loss: 0.0023782845710229594, run on the CPU with time 0.06127649999689311\n",
      "run 1605, Training loss: 0.0023682252169898925, run on the CPU with time 0.061232299980474636\n",
      "run 1606, Training loss: 0.0023828679901436605, run on the CPU with time 0.07153909999760799\n",
      "run 1607, Training loss: 0.002372877653397154, run on the CPU with time 0.06912989998818375\n",
      "run 1608, Training loss: 0.0023772093340415847, run on the CPU with time 0.06167920000734739\n",
      "run 1609, Training loss: 0.002488262222190811, run on the CPU with time 0.06257460001506843\n",
      "run 1610, Training loss: 0.0025778806511714885, run on the CPU with time 0.0624316000030376\n",
      "run 1611, Training loss: 0.0026602752713105558, run on the CPU with time 0.06094919997849502\n",
      "run 1612, Training loss: 0.0025748285415995104, run on the CPU with time 0.06285419999039732\n",
      "run 1613, Training loss: 0.0026669438678337347, run on the CPU with time 0.07206949999090284\n",
      "run 1614, Training loss: 0.0028227241929430005, run on the CPU with time 0.06562820001272485\n",
      "run 1615, Training loss: 0.002860195064832541, run on the CPU with time 0.06273999999393709\n",
      "run 1616, Training loss: 0.002653906395492165, run on the CPU with time 0.06322589999763295\n",
      "run 1617, Training loss: 0.002935314754921075, run on the CPU with time 0.06507359998067841\n",
      "run 1618, Training loss: 0.003791174081693911, run on the CPU with time 0.061690200003795326\n",
      "run 1619, Training loss: 0.002844605116776868, run on the CPU with time 0.060783499997342005\n",
      "run 1620, Training loss: 0.0025536272121164734, run on the CPU with time 0.062155299994628876\n",
      "run 1621, Training loss: 0.002623995340863158, run on the CPU with time 0.06445939998957328\n",
      "run 1622, Training loss: 0.0026164236927467177, run on the CPU with time 0.07038290001219139\n",
      "run 1623, Training loss: 0.0024872862023006415, run on the CPU with time 0.06318160000955686\n",
      "run 1624, Training loss: 0.0024999349864877085, run on the CPU with time 0.0653081999917049\n",
      "run 1625, Training loss: 0.00265158807371997, run on the CPU with time 0.06429329997627065\n",
      "run 1626, Training loss: 0.002639599330987866, run on the CPU with time 0.06208960001822561\n",
      "run 1627, Training loss: 0.0025363683436642696, run on the CPU with time 0.061224999983096495\n",
      "run 1628, Training loss: 0.0025362194669221274, run on the CPU with time 0.061130399990361184\n",
      "run 1629, Training loss: 0.0024440926002253865, run on the CPU with time 0.06352480000350624\n",
      "run 1630, Training loss: 0.0024210093908054245, run on the CPU with time 0.0625409999920521\n",
      "run 1631, Training loss: 0.002383673478933898, run on the CPU with time 0.07249839999713004\n",
      "run 1632, Training loss: 0.0023935051523784005, run on the CPU with time 0.06505599999218248\n",
      "run 1633, Training loss: 0.002739209107213273, run on the CPU with time 0.06334680001600645\n",
      "run 1634, Training loss: 0.0023543781143830116, run on the CPU with time 0.06628629998886026\n",
      "run 1635, Training loss: 0.0023527056929610393, run on the CPU with time 0.06114679999882355\n",
      "run 1636, Training loss: 0.0023102039897573627, run on the CPU with time 0.06113720001303591\n",
      "run 1637, Training loss: 0.002322015015604186, run on the CPU with time 0.06284739999682643\n",
      "run 1638, Training loss: 0.0023127895410818217, run on the CPU with time 0.06268309999722987\n",
      "run 1639, Training loss: 0.0022848187002554453, run on the CPU with time 0.07330509999883361\n",
      "run 1640, Training loss: 0.002289123242900876, run on the CPU with time 0.06627489998936653\n",
      "run 1641, Training loss: 0.002299414964032952, run on the CPU with time 0.062057600007392466\n",
      "run 1642, Training loss: 0.0022983335137145003, run on the CPU with time 0.0630931000050623\n",
      "run 1643, Training loss: 0.0023207397704192606, run on the CPU with time 0.06374919999507256\n",
      "run 1644, Training loss: 0.0022798699665475975, run on the CPU with time 0.06377989999600686\n",
      "run 1645, Training loss: 0.0022829522955548367, run on the CPU with time 0.06215779998456128\n",
      "run 1646, Training loss: 0.0023106241712494836, run on the CPU with time 0.06069129999377765\n",
      "run 1647, Training loss: 0.0023613342341956343, run on the CPU with time 0.06483570000273176\n",
      "run 1648, Training loss: 0.0022949050545734775, run on the CPU with time 0.0805732999870088\n",
      "run 1649, Training loss: 0.0022782878597023557, run on the CPU with time 0.06493480000062846\n",
      "run 1650, Training loss: 0.0022633129857819187, run on the CPU with time 0.06302609998965636\n",
      "run 1651, Training loss: 0.002278040879966945, run on the CPU with time 0.06301069998880848\n",
      "run 1652, Training loss: 0.002487446874147281, run on the CPU with time 0.06211659999098629\n",
      "run 1653, Training loss: 0.0023146177708192476, run on the CPU with time 0.061619299987796694\n",
      "run 1654, Training loss: 0.0023027594660313547, run on the CPU with time 0.061312700010603294\n",
      "run 1655, Training loss: 0.002260526597224684, run on the CPU with time 0.06782060000114143\n",
      "run 1656, Training loss: 0.0022675681102555245, run on the CPU with time 0.06787970001460053\n",
      "run 1657, Training loss: 0.002251248240777799, run on the CPU with time 0.06379839999135584\n",
      "run 1658, Training loss: 0.0022597583861418322, run on the CPU with time 0.06258579998393543\n",
      "run 1659, Training loss: 0.0024684962083649058, run on the CPU with time 0.061865199997555465\n",
      "run 1660, Training loss: 0.0022560474298767407, run on the CPU with time 0.06182609999086708\n",
      "run 1661, Training loss: 0.0022478666947625407, run on the CPU with time 0.06271199998445809\n",
      "run 1662, Training loss: 0.0022483594380901194, run on the CPU with time 0.06324170000152662\n",
      "run 1663, Training loss: 0.002244297374008139, run on the CPU with time 0.06980029999976978\n",
      "run 1664, Training loss: 0.002244120804243721, run on the CPU with time 0.07302049998543225\n",
      "run 1665, Training loss: 0.0022601954808289353, run on the CPU with time 0.07107120001455769\n",
      "run 1666, Training loss: 0.0022676384741895494, run on the CPU with time 0.06447290000505745\n",
      "run 1667, Training loss: 0.0022842354221872733, run on the CPU with time 0.06445920001715422\n",
      "run 1668, Training loss: 0.0022402188687754626, run on the CPU with time 0.06336669999291189\n",
      "run 1669, Training loss: 0.002245279711529623, run on the CPU with time 0.06303559997468255\n",
      "run 1670, Training loss: 0.002238687835581872, run on the CPU with time 0.06203309999546036\n",
      "run 1671, Training loss: 0.0022506965915619565, run on the CPU with time 0.06135949998861179\n",
      "run 1672, Training loss: 0.002220080107640983, run on the CPU with time 0.06442489998880774\n",
      "run 1673, Training loss: 0.002240313687732189, run on the CPU with time 0.06275630000163801\n",
      "run 1674, Training loss: 0.0022269192490388047, run on the CPU with time 0.06596030000946485\n",
      "run 1675, Training loss: 0.002220029107212957, run on the CPU with time 0.07699460000731051\n",
      "run 1676, Training loss: 0.0022162843810499245, run on the CPU with time 0.06368039999506436\n",
      "run 1677, Training loss: 0.002207706848676852, run on the CPU with time 0.061466699989978224\n",
      "run 1678, Training loss: 0.0022181999087809924, run on the CPU with time 0.07025060002342798\n",
      "run 1679, Training loss: 0.0022074216975438918, run on the CPU with time 0.06581470000674017\n",
      "run 1680, Training loss: 0.0022216321250942365, run on the CPU with time 0.06377079998492263\n",
      "run 1681, Training loss: 0.002204866503069008, run on the CPU with time 0.06167580001056194\n",
      "run 1682, Training loss: 0.0022048994147388094, run on the CPU with time 0.06158919999143109\n",
      "run 1683, Training loss: 0.002200978464415212, run on the CPU with time 0.06146779999835417\n",
      "run 1684, Training loss: 0.0022232593493082607, run on the CPU with time 0.06467970000812784\n",
      "run 1685, Training loss: 0.0023186869744677096, run on the CPU with time 0.06690349997370504\n",
      "run 1686, Training loss: 0.0022421039261230776, run on the CPU with time 0.08003260000259615\n",
      "run 1687, Training loss: 0.002207800381388965, run on the CPU with time 0.06236499999067746\n",
      "run 1688, Training loss: 0.0022291430247026833, run on the CPU with time 0.06426029998692684\n",
      "run 1689, Training loss: 0.0021922819285960446, run on the CPU with time 0.06418380001559854\n",
      "run 1690, Training loss: 0.0021982497624280354, run on the CPU with time 0.062392599997110665\n",
      "run 1691, Training loss: 0.00220205912707818, run on the CPU with time 0.062278400000650436\n",
      "run 1692, Training loss: 0.002187151136141355, run on the CPU with time 0.06206299999030307\n",
      "run 1693, Training loss: 0.002191604899814014, run on the CPU with time 0.06109829997876659\n",
      "run 1694, Training loss: 0.0021796084005142225, run on the CPU with time 0.06102280001505278\n",
      "run 1695, Training loss: 0.002170392468410269, run on the CPU with time 0.07789530002628453\n",
      "run 1696, Training loss: 0.0021734168008895887, run on the CPU with time 0.07235579998814501\n",
      "run 1697, Training loss: 0.002183443683464826, run on the CPU with time 0.06987169999047183\n",
      "run 1698, Training loss: 0.00217938699987082, run on the CPU with time 0.06736779998755082\n",
      "run 1699, Training loss: 0.002171878614171874, run on the CPU with time 0.06494999999995343\n",
      "run 1700, Training loss: 0.0021571194647218694, run on the CPU with time 0.06177559998468496\n",
      "run 1701, Training loss: 0.0021603563174060333, run on the CPU with time 0.061359699990134686\n",
      "run 1702, Training loss: 0.0021548231247090206, run on the CPU with time 0.06259479999425821\n",
      "run 1703, Training loss: 0.002154988874562233, run on the CPU with time 0.062330499989911914\n",
      "run 1704, Training loss: 0.002164491059639576, run on the CPU with time 0.06778380001196638\n",
      "run 1705, Training loss: 0.0021572748069460927, run on the CPU with time 0.06193569998140447\n",
      "run 1706, Training loss: 0.002159099617777181, run on the CPU with time 0.07819000000017695\n",
      "run 1707, Training loss: 0.0021762590401604854, run on the CPU with time 0.06380569998873398\n",
      "run 1708, Training loss: 0.0021755989418703724, run on the CPU with time 0.06306570000015199\n",
      "run 1709, Training loss: 0.002155783831073098, run on the CPU with time 0.06310579998535104\n",
      "run 1710, Training loss: 0.002149010410159297, run on the CPU with time 0.06445169998914935\n",
      "run 1711, Training loss: 0.002172225382541497, run on the CPU with time 0.06134019998717122\n",
      "run 1712, Training loss: 0.002145411178141578, run on the CPU with time 0.06133060000138357\n",
      "run 1713, Training loss: 0.0021499523089997556, run on the CPU with time 0.06233149999752641\n",
      "run 1714, Training loss: 0.0021312555623643255, run on the CPU with time 0.07189890000154264\n",
      "run 1715, Training loss: 0.002140013174952897, run on the CPU with time 0.0635855000000447\n",
      "run 1716, Training loss: 0.002139050797929733, run on the CPU with time 0.07388400001218542\n",
      "run 1717, Training loss: 0.0021429805344351733, run on the CPU with time 0.06521529998281039\n",
      "run 1718, Training loss: 0.0021285190342248163, run on the CPU with time 0.0682970000198111\n",
      "run 1719, Training loss: 0.002171265974042895, run on the CPU with time 0.06530720001319423\n",
      "run 1720, Training loss: 0.0021778663826726653, run on the CPU with time 0.06347779999487102\n",
      "run 1721, Training loss: 0.002169195099453315, run on the CPU with time 0.062074100016616285\n",
      "run 1722, Training loss: 0.002121689996916261, run on the CPU with time 0.06440629999269731\n",
      "run 1723, Training loss: 0.0021264248778937725, run on the CPU with time 0.06142449998878874\n",
      "run 1724, Training loss: 0.002118221697905524, run on the CPU with time 0.06351929999073036\n",
      "run 1725, Training loss: 0.002148775907963599, run on the CPU with time 0.06165069999406114\n",
      "run 1726, Training loss: 0.0021170021068883218, run on the CPU with time 0.07129500000155531\n",
      "run 1727, Training loss: 0.0021143966550219127, run on the CPU with time 0.08218130000750534\n",
      "run 1728, Training loss: 0.002114048124746081, run on the CPU with time 0.0631835000240244\n",
      "run 1729, Training loss: 0.002113962239772346, run on the CPU with time 0.06282319998717867\n",
      "run 1730, Training loss: 0.002103578467160167, run on the CPU with time 0.06393110001226887\n",
      "run 1731, Training loss: 0.002108901872204363, run on the CPU with time 0.0620436999888625\n",
      "run 1732, Training loss: 0.0021065029347400095, run on the CPU with time 0.06116839998867363\n",
      "run 1733, Training loss: 0.002107959522192263, run on the CPU with time 0.06177859997842461\n",
      "run 1734, Training loss: 0.002118720134736081, run on the CPU with time 0.0628654999891296\n",
      "run 1735, Training loss: 0.0021015113119078293, run on the CPU with time 0.06474470000830479\n",
      "run 1736, Training loss: 0.0021102942711546677, run on the CPU with time 0.06595889999880455\n",
      "run 1737, Training loss: 0.0021004907856959935, run on the CPU with time 0.06524699999135919\n",
      "run 1738, Training loss: 0.002114017552230507, run on the CPU with time 0.06531679999898188\n",
      "run 1739, Training loss: 0.002098936071177542, run on the CPU with time 0.06275250000180677\n",
      "run 1740, Training loss: 0.002120343408155763, run on the CPU with time 0.062207399983890355\n",
      "run 1741, Training loss: 0.00211106274060016, run on the CPU with time 0.06049099998199381\n",
      "run 1742, Training loss: 0.0020967699473575605, run on the CPU with time 0.06242879998171702\n",
      "run 1743, Training loss: 0.002079369642888196, run on the CPU with time 0.06138329999521375\n",
      "run 1744, Training loss: 0.002090116164816374, run on the CPU with time 0.06706620001932606\n",
      "run 1745, Training loss: 0.0020857391898144585, run on the CPU with time 0.06778300000587478\n",
      "run 1746, Training loss: 0.0020999820971734483, run on the CPU with time 0.0793872999784071\n",
      "run 1747, Training loss: 0.0020752863215007396, run on the CPU with time 0.06455100001767278\n",
      "run 1748, Training loss: 0.0020873736400972122, run on the CPU with time 0.06385919998865575\n",
      "run 1749, Training loss: 0.0020784887998491863, run on the CPU with time 0.0704394000058528\n",
      "run 1750, Training loss: 0.0020880073700523514, run on the CPU with time 0.0665580999921076\n",
      "run 1751, Training loss: 0.0020768400698381647, run on the CPU with time 0.06055039999773726\n",
      "run 1752, Training loss: 0.0020743830761851064, run on the CPU with time 0.06028599999262951\n",
      "run 1753, Training loss: 0.002067009486067532, run on the CPU with time 0.06045660001109354\n",
      "run 1754, Training loss: 0.002067247284602755, run on the CPU with time 0.06425460000173189\n",
      "run 1755, Training loss: 0.0020755375271784776, run on the CPU with time 0.06391249998705462\n",
      "run 1756, Training loss: 0.0020539413617536246, run on the CPU with time 0.06812589999753982\n",
      "run 1757, Training loss: 0.0020673954891125587, run on the CPU with time 0.07356779999099672\n",
      "run 1758, Training loss: 0.002067912952092725, run on the CPU with time 0.06653489999007434\n",
      "run 1759, Training loss: 0.002055866407119373, run on the CPU with time 0.06463619999703951\n",
      "run 1760, Training loss: 0.0020529373415312207, run on the CPU with time 0.06270969999604858\n",
      "run 1761, Training loss: 0.0020665188567363657, run on the CPU with time 0.06363949997466989\n",
      "run 1762, Training loss: 0.002043898925880554, run on the CPU with time 0.06269459999748506\n",
      "run 1763, Training loss: 0.0020438860591246505, run on the CPU with time 0.06232070000260137\n",
      "run 1764, Training loss: 0.002046001864602493, run on the CPU with time 0.0627346999826841\n",
      "run 1765, Training loss: 0.0020763346769275483, run on the CPU with time 0.06655610000598244\n",
      "run 1766, Training loss: 0.0020597272980963184, run on the CPU with time 0.0641096000035759\n",
      "run 1767, Training loss: 0.0020482917809038718, run on the CPU with time 0.0620995000062976\n",
      "run 1768, Training loss: 0.002175281888643936, run on the CPU with time 0.06223579999641515\n",
      "run 1769, Training loss: 0.0020521052332531492, run on the CPU with time 0.0632789000228513\n",
      "run 1770, Training loss: 0.002045184299326634, run on the CPU with time 0.06373389999498613\n",
      "run 1771, Training loss: 0.0020358242687176574, run on the CPU with time 0.07178739999653772\n",
      "run 1772, Training loss: 0.0020302820316415323, run on the CPU with time 0.07773889999953099\n",
      "run 1773, Training loss: 0.0020336418961629864, run on the CPU with time 0.06725579997873865\n",
      "run 1774, Training loss: 0.002091523188573774, run on the CPU with time 0.06323450000490993\n",
      "run 1775, Training loss: 0.0020604948354461653, run on the CPU with time 0.06133080000290647\n",
      "run 1776, Training loss: 0.0021130001214756207, run on the CPU with time 0.06259479999425821\n",
      "run 1777, Training loss: 0.002044186456193777, run on the CPU with time 0.060835499985842034\n",
      "run 1778, Training loss: 0.002035790374793578, run on the CPU with time 0.06050849999883212\n",
      "run 1779, Training loss: 0.002038760141956366, run on the CPU with time 0.06255459997919388\n",
      "run 1780, Training loss: 0.0020394852302905004, run on the CPU with time 0.06784850000985898\n",
      "run 1781, Training loss: 0.0020296275247925553, run on the CPU with time 0.07179990000440739\n",
      "run 1782, Training loss: 0.0020162690158361907, run on the CPU with time 0.08140229998389259\n",
      "run 1783, Training loss: 0.00201581641449593, run on the CPU with time 0.06477470000390895\n",
      "run 1784, Training loss: 0.002012986101908609, run on the CPU with time 0.06318409999948926\n",
      "run 1785, Training loss: 0.00201144223974552, run on the CPU with time 0.06327879999298602\n",
      "run 1786, Training loss: 0.0020167527883049015, run on the CPU with time 0.0632573000038974\n",
      "run 1787, Training loss: 0.002024794014340097, run on the CPU with time 0.06235910000395961\n",
      "run 1788, Training loss: 0.002022468981819905, run on the CPU with time 0.0617803999921307\n",
      "run 1789, Training loss: 0.002008507134054195, run on the CPU with time 0.06518020000657998\n",
      "run 1790, Training loss: 0.0021286605435307136, run on the CPU with time 0.06610279998858459\n",
      "run 1791, Training loss: 0.0020782202478254806, run on the CPU with time 0.09202999999979511\n",
      "run 1792, Training loss: 0.0020148032657084382, run on the CPU with time 0.06938199998694472\n",
      "run 1793, Training loss: 0.0020137172228053466, run on the CPU with time 0.07340729999123141\n",
      "run 1794, Training loss: 0.002001151232583321, run on the CPU with time 0.0694552999921143\n",
      "run 1795, Training loss: 0.001995189500766256, run on the CPU with time 0.06529650001903065\n",
      "run 1796, Training loss: 0.0019972622328747426, run on the CPU with time 0.0649140999885276\n",
      "run 1797, Training loss: 0.0019997070184961723, run on the CPU with time 0.06753639999078587\n",
      "run 1798, Training loss: 0.001990984328122894, run on the CPU with time 0.06762980000348762\n",
      "run 1799, Training loss: 0.0019896051349033686, run on the CPU with time 0.07938959999592043\n",
      "run 1800, Training loss: 0.0019905916448227467, run on the CPU with time 0.09168610000051558\n",
      "run 1801, Training loss: 0.001979584326892853, run on the CPU with time 0.08149000001139939\n",
      "run 1802, Training loss: 0.001988433163867078, run on the CPU with time 0.06355269998311996\n",
      "run 1803, Training loss: 0.0019741161697311325, run on the CPU with time 0.06366600000183098\n",
      "run 1804, Training loss: 0.0019970777737564113, run on the CPU with time 0.061839699978008866\n",
      "run 1805, Training loss: 0.0019904613711828876, run on the CPU with time 0.06235170000582002\n",
      "run 1806, Training loss: 0.0019670915565008977, run on the CPU with time 0.061853099992731586\n",
      "run 1807, Training loss: 0.001971404267665507, run on the CPU with time 0.06191719998605549\n",
      "run 1808, Training loss: 0.002250096227296374, run on the CPU with time 0.06524610001360998\n",
      "run 1809, Training loss: 0.002044541941061404, run on the CPU with time 0.09462079999502748\n",
      "run 1810, Training loss: 0.0019979075035239063, run on the CPU with time 0.07056220000959001\n",
      "run 1811, Training loss: 0.0019951618543762543, run on the CPU with time 0.06455410001217388\n",
      "run 1812, Training loss: 0.0019708401254981502, run on the CPU with time 0.0643804999999702\n",
      "run 1813, Training loss: 0.0019802323541212403, run on the CPU with time 0.06892099999822676\n",
      "run 1814, Training loss: 0.001966063806189182, run on the CPU with time 0.06355349998921156\n",
      "run 1815, Training loss: 0.0019691168288938406, run on the CPU with time 0.06293169999844395\n",
      "run 1816, Training loss: 0.001972473260353912, run on the CPU with time 0.06571700001950376\n",
      "run 1817, Training loss: 0.0019857186308813238, run on the CPU with time 0.07360580001841299\n",
      "run 1818, Training loss: 0.002009707669044887, run on the CPU with time 0.06387559999711812\n",
      "run 1819, Training loss: 0.002034853807444134, run on the CPU with time 0.06687729997793213\n",
      "run 1820, Training loss: 0.0020322645818272246, run on the CPU with time 0.06591480001225136\n",
      "run 1821, Training loss: 0.002032896235058698, run on the CPU with time 0.06640819998574443\n",
      "run 1822, Training loss: 0.002027963657945458, run on the CPU with time 0.06412590001127683\n",
      "run 1823, Training loss: 0.0019679114276765506, run on the CPU with time 0.06620850000763312\n",
      "run 1824, Training loss: 0.001936026118280345, run on the CPU with time 0.06430249998811632\n",
      "run 1825, Training loss: 0.0019460057939630299, run on the CPU with time 0.07098399999085814\n",
      "run 1826, Training loss: 0.0019429950052819384, run on the CPU with time 0.06950610000058077\n",
      "run 1827, Training loss: 0.0019307626327975992, run on the CPU with time 0.06768330000340939\n",
      "run 1828, Training loss: 0.001936900039700876, run on the CPU with time 0.07150310001452453\n",
      "run 1829, Training loss: 0.001930174191891803, run on the CPU with time 0.06775930000003427\n",
      "run 1830, Training loss: 0.0019621191489436157, run on the CPU with time 0.06298210000386462\n",
      "run 1831, Training loss: 0.0019405202483705414, run on the CPU with time 0.06487900001229718\n",
      "run 1832, Training loss: 0.0019489757843654265, run on the CPU with time 0.0634775000216905\n",
      "run 1833, Training loss: 0.0019415062862787058, run on the CPU with time 0.06288079998921603\n",
      "run 1834, Training loss: 0.0019232622305439277, run on the CPU with time 0.062346599996089935\n",
      "run 1835, Training loss: 0.0019135263200785796, run on the CPU with time 0.06241240000235848\n",
      "run 1836, Training loss: 0.001956288987738927, run on the CPU with time 0.06551310000941157\n",
      "run 1837, Training loss: 0.001955919520696625, run on the CPU with time 0.06740070000523701\n",
      "run 1838, Training loss: 0.0019312428449418141, run on the CPU with time 0.06910319998860359\n",
      "run 1839, Training loss: 0.0019311886159068142, run on the CPU with time 0.07092919998103753\n",
      "run 1840, Training loss: 0.001920719581936613, run on the CPU with time 0.07369799999287352\n",
      "run 1841, Training loss: 0.0019195390424680558, run on the CPU with time 0.06202589999884367\n",
      "run 1842, Training loss: 0.001921390183666848, run on the CPU with time 0.06379300000844523\n",
      "run 1843, Training loss: 0.0019111080991106892, run on the CPU with time 0.06337210000492632\n",
      "run 1844, Training loss: 0.0019029986487690952, run on the CPU with time 0.06344070000341162\n",
      "run 1845, Training loss: 0.0019186944756339388, run on the CPU with time 0.06502589999581687\n",
      "run 1846, Training loss: 0.001935717551697797, run on the CPU with time 0.06446739999228157\n",
      "run 1847, Training loss: 0.0019152360763242044, run on the CPU with time 0.0637470000074245\n",
      "run 1848, Training loss: 0.0019122065726895183, run on the CPU with time 0.06360199998016469\n",
      "run 1849, Training loss: 0.0019314901498463852, run on the CPU with time 0.06148049997864291\n",
      "run 1850, Training loss: 0.0019223467387167726, run on the CPU with time 0.06202549999579787\n",
      "run 1851, Training loss: 0.0018997785344915675, run on the CPU with time 0.06123180000577122\n",
      "run 1852, Training loss: 0.0018999594018995678, run on the CPU with time 0.06437149998964742\n",
      "run 1853, Training loss: 0.0018982267761285501, run on the CPU with time 0.07933629999752156\n",
      "run 1854, Training loss: 0.001895075437679142, run on the CPU with time 0.07862640000530519\n",
      "run 1855, Training loss: 0.0018964241338174112, run on the CPU with time 0.06885810001404025\n",
      "run 1856, Training loss: 0.0018970524836649102, run on the CPU with time 0.06864439998753369\n",
      "run 1857, Training loss: 0.0018905032871936616, run on the CPU with time 0.062168700009351596\n",
      "run 1858, Training loss: 0.0019119163874579085, run on the CPU with time 0.061553899984573945\n",
      "run 1859, Training loss: 0.0018953231641551777, run on the CPU with time 0.06164329999592155\n",
      "run 1860, Training loss: 0.0018842538779029962, run on the CPU with time 0.061581999994814396\n",
      "run 1861, Training loss: 0.0018925581361103965, run on the CPU with time 0.06778869999106973\n",
      "run 1862, Training loss: 0.0018874302109576423, run on the CPU with time 0.0850538999948185\n",
      "run 1863, Training loss: 0.0018886671957178889, run on the CPU with time 0.07983190001687035\n",
      "run 1864, Training loss: 0.0018792657380055804, run on the CPU with time 0.06572110002161935\n",
      "run 1865, Training loss: 0.00187804369475502, run on the CPU with time 0.0623724999895785\n",
      "run 1866, Training loss: 0.0018912638362962753, run on the CPU with time 0.06410330001381226\n",
      "run 1867, Training loss: 0.0018828929393583332, run on the CPU with time 0.06181990000186488\n",
      "run 1868, Training loss: 0.0018975740265556272, run on the CPU with time 0.06126249997760169\n",
      "run 1869, Training loss: 0.0018726050515190517, run on the CPU with time 0.06272730001364835\n",
      "run 1870, Training loss: 0.0018741984124062583, run on the CPU with time 0.06516140000894666\n",
      "run 1871, Training loss: 0.0018759991592642935, run on the CPU with time 0.06238180000218563\n",
      "run 1872, Training loss: 0.0018782214165019634, run on the CPU with time 0.06188210000982508\n",
      "run 1873, Training loss: 0.0018671240990939127, run on the CPU with time 0.06280929999775253\n",
      "run 1874, Training loss: 0.0018709668521875178, run on the CPU with time 0.06450990002485923\n",
      "run 1875, Training loss: 0.0018738191525864583, run on the CPU with time 0.07625650000409223\n",
      "run 1876, Training loss: 0.001888513475635343, run on the CPU with time 0.07216270000208169\n",
      "run 1877, Training loss: 0.0018794494346366264, run on the CPU with time 0.06191739998757839\n",
      "run 1878, Training loss: 0.0018503480850170526, run on the CPU with time 0.0624002999975346\n",
      "run 1879, Training loss: 0.0018516308699438675, run on the CPU with time 0.06101089998264797\n",
      "run 1880, Training loss: 0.0018677121110323987, run on the CPU with time 0.06197229999816045\n",
      "run 1881, Training loss: 0.0018677756088436581, run on the CPU with time 0.06296999999904074\n",
      "run 1882, Training loss: 0.0018542787234764545, run on the CPU with time 0.06319599997368641\n",
      "run 1883, Training loss: 0.00186747578806138, run on the CPU with time 0.06426660000579432\n",
      "run 1884, Training loss: 0.001851707287212114, run on the CPU with time 0.06565810000756755\n",
      "run 1885, Training loss: 0.0018591597554570233, run on the CPU with time 0.06692979999934323\n",
      "run 1886, Training loss: 0.0018607330732100474, run on the CPU with time 0.06375929998466745\n",
      "run 1887, Training loss: 0.0019383159013655544, run on the CPU with time 0.06329540000297129\n",
      "run 1888, Training loss: 0.0019823926918516835, run on the CPU with time 0.06127919998834841\n",
      "run 1889, Training loss: 0.0019153577893369154, run on the CPU with time 0.062269799993373454\n",
      "run 1890, Training loss: 0.0018653019951836375, run on the CPU with time 0.06237949998467229\n",
      "run 1891, Training loss: 0.001839037097868806, run on the CPU with time 0.06167910000658594\n",
      "run 1892, Training loss: 0.0018432729094373908, run on the CPU with time 0.0719902999990154\n",
      "run 1893, Training loss: 0.001861671052492139, run on the CPU with time 0.06616240000585094\n",
      "run 1894, Training loss: 0.0018747164191815749, run on the CPU with time 0.0622672000026796\n",
      "run 1895, Training loss: 0.001877260144366036, run on the CPU with time 0.06363379998947494\n",
      "run 1896, Training loss: 0.0018629420203069458, run on the CPU with time 0.06210119999013841\n",
      "run 1897, Training loss: 0.001842770959113047, run on the CPU with time 0.06226850001257844\n",
      "run 1898, Training loss: 0.0018530210261815227, run on the CPU with time 0.061782100005075336\n",
      "run 1899, Training loss: 0.0018515210976395545, run on the CPU with time 0.062105099990731105\n",
      "run 1900, Training loss: 0.001828266299641903, run on the CPU with time 0.06527930000447668\n",
      "run 1901, Training loss: 0.0018191726728797551, run on the CPU with time 0.07112849998520687\n",
      "run 1902, Training loss: 0.0018232597056670452, run on the CPU with time 0.06234040000708774\n",
      "run 1903, Training loss: 0.0018271342498006893, run on the CPU with time 0.06212350001442246\n",
      "run 1904, Training loss: 0.001831209973897785, run on the CPU with time 0.061879799992311746\n",
      "run 1905, Training loss: 0.0018160447139631618, run on the CPU with time 0.062171899975510314\n",
      "run 1906, Training loss: 0.0018117894482036884, run on the CPU with time 0.06040650000795722\n",
      "run 1907, Training loss: 0.0018268906283827329, run on the CPU with time 0.0728440000093542\n",
      "run 1908, Training loss: 0.0018308594744832424, run on the CPU with time 0.080188400024781\n",
      "run 1909, Training loss: 0.0018248040648441847, run on the CPU with time 0.07352159998845309\n",
      "run 1910, Training loss: 0.0018259366655415347, run on the CPU with time 0.06474780000280589\n",
      "run 1911, Training loss: 0.0018218285056884104, run on the CPU with time 0.06301949999760836\n",
      "run 1912, Training loss: 0.0018344090408804318, run on the CPU with time 0.0627358999918215\n",
      "run 1913, Training loss: 0.0018260803578992967, run on the CPU with time 0.061550799990072846\n",
      "run 1914, Training loss: 0.0018069923421452669, run on the CPU with time 0.06091589998686686\n",
      "run 1915, Training loss: 0.001812039096826497, run on the CPU with time 0.0639841000083834\n",
      "run 1916, Training loss: 0.0018256341218313371, run on the CPU with time 0.06129409998538904\n",
      "run 1917, Training loss: 0.0018434681499704972, run on the CPU with time 0.061264999996637926\n",
      "run 1918, Training loss: 0.0018230337840081615, run on the CPU with time 0.07065869998768903\n",
      "run 1919, Training loss: 0.0017963072217753507, run on the CPU with time 0.08385930000804365\n",
      "run 1920, Training loss: 0.001816365292920223, run on the CPU with time 0.06569620000664145\n",
      "run 1921, Training loss: 0.0017952664503933524, run on the CPU with time 0.06208250002237037\n",
      "run 1922, Training loss: 0.0017906314854785291, run on the CPU with time 0.061283100018044934\n",
      "run 1923, Training loss: 0.0018270675543383484, run on the CPU with time 0.061710999987553805\n",
      "run 1924, Training loss: 0.0018036612736812624, run on the CPU with time 0.06104609998874366\n",
      "run 1925, Training loss: 0.0017960387414363637, run on the CPU with time 0.06112240001675673\n",
      "run 1926, Training loss: 0.0017877924193188928, run on the CPU with time 0.06098400001064874\n",
      "run 1927, Training loss: 0.0018043914441725727, run on the CPU with time 0.06745510001201183\n",
      "run 1928, Training loss: 0.00178609877131583, run on the CPU with time 0.06785870000021532\n",
      "run 1929, Training loss: 0.0018099064405594782, run on the CPU with time 0.07858920001308434\n",
      "run 1930, Training loss: 0.0017855875773172805, run on the CPU with time 0.06582970000454225\n",
      "run 1931, Training loss: 0.0017793839183551344, run on the CPU with time 0.07676189998164773\n",
      "run 1932, Training loss: 0.0017829265878059563, run on the CPU with time 0.060644300014246255\n",
      "run 1933, Training loss: 0.0017841485723196953, run on the CPU with time 0.061346800008323044\n",
      "run 1934, Training loss: 0.0017774071704479866, run on the CPU with time 0.06161800000700168\n",
      "run 1935, Training loss: 0.0017784216716377572, run on the CPU with time 0.06004959999700077\n",
      "run 1936, Training loss: 0.0017886649042008105, run on the CPU with time 0.06065800000214949\n",
      "run 1937, Training loss: 0.0017704212447411424, run on the CPU with time 0.06892089999746531\n",
      "run 1938, Training loss: 0.0017724592583941887, run on the CPU with time 0.07089949998771772\n",
      "run 1939, Training loss: 0.001773240676497649, run on the CPU with time 0.06312149998848327\n",
      "run 1940, Training loss: 0.0017725124197154813, run on the CPU with time 0.06239020000793971\n",
      "run 1941, Training loss: 0.0017625376766690435, run on the CPU with time 0.06306749998475425\n",
      "run 1942, Training loss: 0.0017816714429169554, run on the CPU with time 0.06288489999133162\n",
      "run 1943, Training loss: 0.0017605994869055311, run on the CPU with time 0.060947999998461455\n",
      "run 1944, Training loss: 0.0017598137455272743, run on the CPU with time 0.060730400000466034\n",
      "run 1945, Training loss: 0.001762030244439798, run on the CPU with time 0.060740699991583824\n",
      "run 1946, Training loss: 0.0017671353337143294, run on the CPU with time 0.06311440002173185\n",
      "run 1947, Training loss: 0.0017850568191285921, run on the CPU with time 0.08174170000711456\n",
      "run 1948, Training loss: 0.0017786743344582977, run on the CPU with time 0.06482239998877048\n",
      "run 1949, Training loss: 0.0017578583084700355, run on the CPU with time 0.0665238999936264\n",
      "run 1950, Training loss: 0.001753292126867543, run on the CPU with time 0.07124130000011064\n",
      "run 1951, Training loss: 0.001758456316945905, run on the CPU with time 0.06318530000862665\n",
      "run 1952, Training loss: 0.001760516633141213, run on the CPU with time 0.06177249999018386\n",
      "run 1953, Training loss: 0.0017673437723301918, run on the CPU with time 0.06140910001704469\n",
      "run 1954, Training loss: 0.001751631760883108, run on the CPU with time 0.06129260000307113\n",
      "run 1955, Training loss: 0.001754886161110004, run on the CPU with time 0.06137280000257306\n",
      "run 1956, Training loss: 0.0017523915160299194, run on the CPU with time 0.06219209998380393\n",
      "run 1957, Training loss: 0.0017485027356914625, run on the CPU with time 0.0709295000124257\n",
      "run 1958, Training loss: 0.0017435552246586833, run on the CPU with time 0.08485859999200329\n",
      "run 1959, Training loss: 0.0017656487455083565, run on the CPU with time 0.0651709999947343\n",
      "run 1960, Training loss: 0.0017440527180128966, run on the CPU with time 0.06154299998888746\n",
      "run 1961, Training loss: 0.0017530781355162618, run on the CPU with time 0.062488700001267716\n",
      "run 1962, Training loss: 0.0017382035305096432, run on the CPU with time 0.06089079999946989\n",
      "run 1963, Training loss: 0.0017385863023016348, run on the CPU with time 0.061188199993921444\n",
      "run 1964, Training loss: 0.001757977721651762, run on the CPU with time 0.06110419999458827\n",
      "run 1965, Training loss: 0.0017309747226218778, run on the CPU with time 0.06222399999387562\n",
      "run 1966, Training loss: 0.0017393481032658283, run on the CPU with time 0.07410530000925064\n",
      "run 1967, Training loss: 0.0017322764079091774, run on the CPU with time 0.06358099999488331\n",
      "run 1968, Training loss: 0.0017267531733523357, run on the CPU with time 0.0647774999961257\n",
      "run 1969, Training loss: 0.001724948457161769, run on the CPU with time 0.06453740000142716\n",
      "run 1970, Training loss: 0.0017200318276247178, run on the CPU with time 0.06290970000554807\n",
      "run 1971, Training loss: 0.0017268904908136888, run on the CPU with time 0.061189400003058836\n",
      "run 1972, Training loss: 0.0017264139809412883, run on the CPU with time 0.06113660000846721\n",
      "run 1973, Training loss: 0.0017146636463704372, run on the CPU with time 0.06412019999697804\n",
      "run 1974, Training loss: 0.0017384022681984459, run on the CPU with time 0.060876900010043755\n",
      "run 1975, Training loss: 0.0017325543766756627, run on the CPU with time 0.06216100000892766\n",
      "run 1976, Training loss: 0.0017201148100535978, run on the CPU with time 0.07374550000531599\n",
      "run 1977, Training loss: 0.0017380525877921504, run on the CPU with time 0.07599579999805428\n",
      "run 1978, Training loss: 0.0017341118012237447, run on the CPU with time 0.06545530000585131\n",
      "run 1979, Training loss: 0.0017245331691810861, run on the CPU with time 0.06204220000654459\n",
      "run 1980, Training loss: 0.0017176444990830285, run on the CPU with time 0.06145149999065325\n",
      "run 1981, Training loss: 0.0017237886135593396, run on the CPU with time 0.0631957000005059\n",
      "run 1982, Training loss: 0.0017182719151605853, run on the CPU with time 0.06295659998431802\n",
      "run 1983, Training loss: 0.0017131962855108378, run on the CPU with time 0.062173400016035885\n",
      "run 1984, Training loss: 0.0017261713376766155, run on the CPU with time 0.061586700001498684\n",
      "run 1985, Training loss: 0.001709548199644153, run on the CPU with time 0.06304690000251867\n",
      "run 1986, Training loss: 0.0017245913569984789, run on the CPU with time 0.06316700001480058\n",
      "run 1987, Training loss: 0.001709346790481835, run on the CPU with time 0.07719839998753741\n",
      "run 1988, Training loss: 0.001711895101603163, run on the CPU with time 0.06909510001423769\n",
      "run 1989, Training loss: 0.0017047402015040545, run on the CPU with time 0.06668419999186881\n",
      "run 1990, Training loss: 0.0017718256407533773, run on the CPU with time 0.062429399986285716\n",
      "run 1991, Training loss: 0.0017309500561995348, run on the CPU with time 0.06745860000955872\n",
      "run 1992, Training loss: 0.0017248340530469166, run on the CPU with time 0.06685299999662675\n",
      "run 1993, Training loss: 0.001701047445501238, run on the CPU with time 0.061075500008882955\n",
      "run 1994, Training loss: 0.001699898417361758, run on the CPU with time 0.061396900011459365\n",
      "run 1995, Training loss: 0.0017050948965003376, run on the CPU with time 0.06135489998268895\n",
      "run 1996, Training loss: 0.0017122013891242784, run on the CPU with time 0.07150170000386424\n",
      "run 1997, Training loss: 0.0016958917723968626, run on the CPU with time 0.06835079999291338\n",
      "run 1998, Training loss: 0.0016944382390647662, run on the CPU with time 0.06819520000135526\n",
      "run 1999, Training loss: 0.0016864689082550202, run on the CPU with time 0.06253579998156056\n",
      "run 2000, Training loss: 0.0016806027632397175, run on the CPU with time 0.06217869999818504\n",
      "run 2001, Training loss: 0.0016861734437043196, run on the CPU with time 0.06362760000047274\n",
      "run 2002, Training loss: 0.0016916706616029841, run on the CPU with time 0.06109239999204874\n",
      "run 2003, Training loss: 0.0016831696227944286, run on the CPU with time 0.06988300001830794\n",
      "run 2004, Training loss: 0.0016904814528640021, run on the CPU with time 0.07272299999021925\n",
      "run 2005, Training loss: 0.0017016476327130063, run on the CPU with time 0.06235640001250431\n",
      "run 2006, Training loss: 0.0016812786337554412, run on the CPU with time 0.0651052999892272\n",
      "run 2007, Training loss: 0.0016804346625576728, run on the CPU with time 0.06389809999382123\n",
      "run 2008, Training loss: 0.001682498892213599, run on the CPU with time 0.07031569999526255\n",
      "run 2009, Training loss: 0.0016878962707943918, run on the CPU with time 0.062272999988636\n",
      "run 2010, Training loss: 0.001682111045048435, run on the CPU with time 0.060979100002441555\n",
      "run 2011, Training loss: 0.001679321116336558, run on the CPU with time 0.06125379999866709\n",
      "run 2012, Training loss: 0.0016774309456443668, run on the CPU with time 0.07297159999143332\n",
      "run 2013, Training loss: 0.0016876197002023798, run on the CPU with time 0.0773711999936495\n",
      "run 2014, Training loss: 0.0016734712026267185, run on the CPU with time 0.06870730000082403\n",
      "run 2015, Training loss: 0.0016799014659640802, run on the CPU with time 0.06339590001152828\n",
      "run 2016, Training loss: 0.001671274010649623, run on the CPU with time 0.062366900005145\n",
      "run 2017, Training loss: 0.0016683887283513534, run on the CPU with time 0.06145279997144826\n",
      "run 2018, Training loss: 0.0016743331289696718, run on the CPU with time 0.06228909999481402\n",
      "run 2019, Training loss: 0.001678227857998784, run on the CPU with time 0.061096099991118535\n",
      "run 2020, Training loss: 0.001670446298984726, run on the CPU with time 0.06178420002106577\n",
      "run 2021, Training loss: 0.001665610556268472, run on the CPU with time 0.06212779998895712\n",
      "run 2022, Training loss: 0.0016645018151559105, run on the CPU with time 0.06396900000981987\n",
      "run 2023, Training loss: 0.001662450955547518, run on the CPU with time 0.07597340000211261\n",
      "run 2024, Training loss: 0.001662602837313898, run on the CPU with time 0.06354970001848415\n",
      "run 2025, Training loss: 0.001660752021284266, run on the CPU with time 0.0629674999800045\n",
      "run 2026, Training loss: 0.0016657450250932015, run on the CPU with time 0.06406949998927303\n",
      "run 2027, Training loss: 0.0016546698285806502, run on the CPU with time 0.061745200015138835\n",
      "run 2028, Training loss: 0.0016645118870656007, run on the CPU with time 0.06160489999456331\n",
      "run 2029, Training loss: 0.001667386284812396, run on the CPU with time 0.062348300009034574\n",
      "run 2030, Training loss: 0.0016505518191314134, run on the CPU with time 0.0687984999967739\n",
      "run 2031, Training loss: 0.0016491887623983944, run on the CPU with time 0.06338579999282956\n",
      "run 2032, Training loss: 0.0016468695511734537, run on the CPU with time 0.06761709999409504\n",
      "run 2033, Training loss: 0.0016502887207686648, run on the CPU with time 0.06584029999794438\n",
      "run 2034, Training loss: 0.0016479918566553598, run on the CPU with time 0.06125999998766929\n",
      "run 2035, Training loss: 0.0016471341073735278, run on the CPU with time 0.061857599997892976\n",
      "run 2036, Training loss: 0.0016380148544902278, run on the CPU with time 0.06259230000432581\n",
      "run 2037, Training loss: 0.00165765963128334, run on the CPU with time 0.06243659998290241\n",
      "run 2038, Training loss: 0.0016549675250478852, run on the CPU with time 0.07145270000910386\n",
      "run 2039, Training loss: 0.00164441937496039, run on the CPU with time 0.06718970002839342\n",
      "run 2040, Training loss: 0.0016599658099452923, run on the CPU with time 0.06393069998011924\n",
      "run 2041, Training loss: 0.001647103702502632, run on the CPU with time 0.0635513000015635\n",
      "run 2042, Training loss: 0.0016488575818419287, run on the CPU with time 0.06191020002006553\n",
      "run 2043, Training loss: 0.0016380464405466972, run on the CPU with time 0.06167190000996925\n",
      "run 2044, Training loss: 0.0016452718179524792, run on the CPU with time 0.06046580002293922\n",
      "run 2045, Training loss: 0.0016413464775144927, run on the CPU with time 0.06171770000946708\n",
      "run 2046, Training loss: 0.001651566479533399, run on the CPU with time 0.0686310000019148\n",
      "run 2047, Training loss: 0.0016479571869819087, run on the CPU with time 0.07791990000987425\n",
      "run 2048, Training loss: 0.0016312152282229031, run on the CPU with time 0.06242170001496561\n",
      "run 2049, Training loss: 0.0016300764474429096, run on the CPU with time 0.06177860000752844\n",
      "run 2050, Training loss: 0.0016279701620409137, run on the CPU with time 0.06061420001788065\n",
      "run 2051, Training loss: 0.0016530681609848133, run on the CPU with time 0.06178719998570159\n",
      "run 2052, Training loss: 0.0016443342037250326, run on the CPU with time 0.059610199998132885\n",
      "run 2053, Training loss: 0.0016184942934738833, run on the CPU with time 0.060461999994004145\n",
      "run 2054, Training loss: 0.0016196828285253353, run on the CPU with time 0.06029949997900985\n",
      "run 2055, Training loss: 0.001632580419457306, run on the CPU with time 0.060735899984138086\n",
      "run 2056, Training loss: 0.0016184093119095118, run on the CPU with time 0.06015860001207329\n",
      "run 2057, Training loss: 0.001618912421823056, run on the CPU with time 0.05999660000088625\n",
      "run 2058, Training loss: 0.0016102783901839179, run on the CPU with time 0.059990999987348914\n",
      "run 2059, Training loss: 0.001618361716761931, run on the CPU with time 0.06129579999833368\n",
      "run 2060, Training loss: 0.00161846364737192, run on the CPU with time 0.05972749998909421\n",
      "run 2061, Training loss: 0.0016239598451647907, run on the CPU with time 0.05987009999807924\n",
      "run 2062, Training loss: 0.0016157952928386459, run on the CPU with time 0.060537700017448515\n",
      "run 2063, Training loss: 0.0016210472575039602, run on the CPU with time 0.0615107000048738\n",
      "run 2064, Training loss: 0.0016229721659328789, run on the CPU with time 0.06096030000480823\n",
      "run 2065, Training loss: 0.0016264318862588632, run on the CPU with time 0.06208829997922294\n",
      "run 2066, Training loss: 0.0016266240448616868, run on the CPU with time 0.06097430002409965\n",
      "run 2067, Training loss: 0.0016208262493653985, run on the CPU with time 0.06126149999909103\n",
      "run 2068, Training loss: 0.0016189419977970167, run on the CPU with time 0.06128829999943264\n",
      "run 2069, Training loss: 0.0016115748101252724, run on the CPU with time 0.060829400026705116\n",
      "run 2070, Training loss: 0.0016085718023305163, run on the CPU with time 0.06155600000056438\n",
      "run 2071, Training loss: 0.0016019193910654973, run on the CPU with time 0.06194179999874905\n",
      "run 2072, Training loss: 0.0016017388026573992, run on the CPU with time 0.06200219999300316\n",
      "run 2073, Training loss: 0.0016054956537945228, run on the CPU with time 0.062057600007392466\n",
      "run 2074, Training loss: 0.0016008333120473915, run on the CPU with time 0.06329849999747239\n",
      "run 2075, Training loss: 0.0015979549826376817, run on the CPU with time 0.06850009999470785\n",
      "run 2076, Training loss: 0.0015944797147478145, run on the CPU with time 0.06609069998376071\n",
      "run 2077, Training loss: 0.0015952036140581846, run on the CPU with time 0.06200070001068525\n",
      "run 2078, Training loss: 0.0015922899227007292, run on the CPU with time 0.06277309998404235\n",
      "run 2079, Training loss: 0.001594729554199148, run on the CPU with time 0.06283350000740029\n",
      "run 2080, Training loss: 0.0015989834983801384, run on the CPU with time 0.061650200019357726\n",
      "run 2081, Training loss: 0.0015982263277030805, run on the CPU with time 0.06206699999165721\n",
      "run 2082, Training loss: 0.001584313747348047, run on the CPU with time 0.0615133999963291\n",
      "run 2083, Training loss: 0.00158857004594624, run on the CPU with time 0.06815730000380427\n",
      "run 2084, Training loss: 0.0015926731923669153, run on the CPU with time 0.06536199999391101\n",
      "run 2085, Training loss: 0.001587826173636131, run on the CPU with time 0.0619015000120271\n",
      "run 2086, Training loss: 0.0015992992372379047, run on the CPU with time 0.06210400001145899\n",
      "run 2087, Training loss: 0.0015823274902107766, run on the CPU with time 0.06501469999784604\n",
      "run 2088, Training loss: 0.001650614476966439, run on the CPU with time 0.06252569999196567\n",
      "run 2089, Training loss: 0.0015926396410742944, run on the CPU with time 0.06385800000862218\n",
      "run 2090, Training loss: 0.0015894422359203665, run on the CPU with time 0.06396810000296682\n",
      "run 2091, Training loss: 0.001599214659389955, run on the CPU with time 0.06307790000573732\n",
      "run 2092, Training loss: 0.0015858842542829552, run on the CPU with time 0.06449089999659918\n",
      "run 2093, Training loss: 0.0015879459650022909, run on the CPU with time 0.0648174999805633\n",
      "run 2094, Training loss: 0.0015759799269620667, run on the CPU with time 0.06482879997929558\n",
      "run 2095, Training loss: 0.0015774985115223734, run on the CPU with time 0.06316760001936927\n",
      "run 2096, Training loss: 0.001575985992200334, run on the CPU with time 0.06311200000345707\n",
      "run 2097, Training loss: 0.0015766015982742168, run on the CPU with time 0.06769470000290312\n",
      "run 2098, Training loss: 0.001572225151374534, run on the CPU with time 0.06986190000316128\n",
      "run 2099, Training loss: 0.001573188789188862, run on the CPU with time 0.06487669999478385\n",
      "run 2100, Training loss: 0.0018741162754436532, run on the CPU with time 0.06171730000642128\n",
      "run 2101, Training loss: 0.0016481308081314307, run on the CPU with time 0.0617089000006672\n",
      "run 2102, Training loss: 0.0016084631475678179, run on the CPU with time 0.060919800016563386\n",
      "run 2103, Training loss: 0.001583659591084473, run on the CPU with time 0.06294740000157617\n",
      "run 2104, Training loss: 0.0015629740549643015, run on the CPU with time 0.06231370000750758\n",
      "run 2105, Training loss: 0.0016641056924775264, run on the CPU with time 0.06255800000508316\n",
      "run 2106, Training loss: 0.0018080562976220708, run on the CPU with time 0.06275390001246706\n",
      "run 2107, Training loss: 0.0015602136073126034, run on the CPU with time 0.07609170000068843\n",
      "run 2108, Training loss: 0.0015824003975880756, run on the CPU with time 0.06424779997905716\n",
      "run 2109, Training loss: 0.0015718817384119822, run on the CPU with time 0.06306210000184365\n",
      "run 2110, Training loss: 0.0015765840470769697, run on the CPU with time 0.06399840000085533\n",
      "run 2111, Training loss: 0.0015603067101355092, run on the CPU with time 0.06245890000718646\n",
      "run 2112, Training loss: 0.001562159853314039, run on the CPU with time 0.06235819999710657\n",
      "run 2113, Training loss: 0.0015546242269920186, run on the CPU with time 0.062204399990150705\n",
      "run 2114, Training loss: 0.00155323373606238, run on the CPU with time 0.06251320001319982\n",
      "run 2115, Training loss: 0.0015747131982193836, run on the CPU with time 0.06396880000829697\n",
      "run 2116, Training loss: 0.001568392550358443, run on the CPU with time 0.062499099993146956\n",
      "run 2117, Training loss: 0.0015704961542227962, run on the CPU with time 0.06212049999157898\n",
      "run 2118, Training loss: 0.0015530177624565444, run on the CPU with time 0.06125570001313463\n",
      "run 2119, Training loss: 0.0015538941433822567, run on the CPU with time 0.06632139999419451\n",
      "run 2120, Training loss: 0.0015593970367759043, run on the CPU with time 0.06924719997914508\n",
      "run 2121, Training loss: 0.0015423442722640795, run on the CPU with time 0.0699404000188224\n",
      "run 2122, Training loss: 0.0015543323413691145, run on the CPU with time 0.06347640001331456\n",
      "run 2123, Training loss: 0.0015399456500745146, run on the CPU with time 0.06260679999832064\n",
      "run 2124, Training loss: 0.001539727255162275, run on the CPU with time 0.07109549999586307\n",
      "run 2125, Training loss: 0.001543424833107565, run on the CPU with time 0.06258139997953549\n",
      "run 2126, Training loss: 0.0015418246550490784, run on the CPU with time 0.06060619998606853\n",
      "run 2127, Training loss: 0.0015467786324072884, run on the CPU with time 0.062050499982433394\n",
      "run 2128, Training loss: 0.0015398997734471529, run on the CPU with time 0.06167610001284629\n",
      "run 2129, Training loss: 0.0015393256668837488, run on the CPU with time 0.07030339998891577\n",
      "run 2130, Training loss: 0.0015405899721372407, run on the CPU with time 0.07017659998382442\n",
      "run 2131, Training loss: 0.0017650035223182798, run on the CPU with time 0.07210150000173599\n",
      "run 2132, Training loss: 0.0015529322656882207, run on the CPU with time 0.06174409997765906\n",
      "run 2133, Training loss: 0.0015331385982625017, run on the CPU with time 0.06376800002180971\n",
      "run 2134, Training loss: 0.0015439788714336994, run on the CPU with time 0.06314539999584667\n",
      "run 2135, Training loss: 0.0015370368436296387, run on the CPU with time 0.06265429998165928\n",
      "run 2136, Training loss: 0.0015303835856188511, run on the CPU with time 0.061389399983454496\n",
      "run 2137, Training loss: 0.001533658694560555, run on the CPU with time 0.07241310001700185\n",
      "run 2138, Training loss: 0.0018261947542090307, run on the CPU with time 0.06675970001379028\n",
      "run 2139, Training loss: 0.0015334186948166462, run on the CPU with time 0.070973399997456\n",
      "run 2140, Training loss: 0.001533105869880687, run on the CPU with time 0.06434479999006726\n",
      "run 2141, Training loss: 0.0015616975589761173, run on the CPU with time 0.06427479998092167\n",
      "run 2142, Training loss: 0.001530839485316061, run on the CPU with time 0.06360809999750927\n",
      "run 2143, Training loss: 0.0015239779582109556, run on the CPU with time 0.06618940000771545\n",
      "run 2144, Training loss: 0.0015304364096002908, run on the CPU with time 0.060319599986542016\n",
      "run 2145, Training loss: 0.0015283347058522685, run on the CPU with time 0.062120200018398464\n",
      "run 2146, Training loss: 0.0015266767378885892, run on the CPU with time 0.05995120000443421\n",
      "run 2147, Training loss: 0.0015270238895688884, run on the CPU with time 0.06210559999453835\n",
      "run 2148, Training loss: 0.0015283159068282906, run on the CPU with time 0.07136600001831539\n",
      "run 2149, Training loss: 0.0015238208471352911, run on the CPU with time 0.06432880001375452\n",
      "run 2150, Training loss: 0.001508311099347669, run on the CPU with time 0.06257010000990704\n",
      "run 2151, Training loss: 0.0015193919977470863, run on the CPU with time 0.06275459998869337\n",
      "run 2152, Training loss: 0.0015175516046424904, run on the CPU with time 0.06197290000272915\n",
      "run 2153, Training loss: 0.0015058106356512079, run on the CPU with time 0.061724500003037974\n",
      "run 2154, Training loss: 0.0015122951275611354, run on the CPU with time 0.06151749999844469\n",
      "run 2155, Training loss: 0.0015032327468030746, run on the CPU with time 0.0640148000093177\n",
      "run 2156, Training loss: 0.0015055833529252348, run on the CPU with time 0.06140459998277947\n",
      "run 2157, Training loss: 0.0015041481034793172, run on the CPU with time 0.06185269998968579\n",
      "run 2158, Training loss: 0.0015069813107733022, run on the CPU with time 0.06460990000050515\n",
      "run 2159, Training loss: 0.0014997176080111206, run on the CPU with time 0.07826939999358729\n",
      "run 2160, Training loss: 0.00150401653247007, run on the CPU with time 0.0664272999856621\n",
      "run 2161, Training loss: 0.0015163371997583784, run on the CPU with time 0.06368950000614859\n",
      "run 2162, Training loss: 0.0015175238114104352, run on the CPU with time 0.06327919999603182\n",
      "run 2163, Training loss: 0.0015108058864081448, run on the CPU with time 0.06412140000611544\n",
      "run 2164, Training loss: 0.0015085225533270701, run on the CPU with time 0.061986799992155284\n",
      "run 2165, Training loss: 0.001499185760390405, run on the CPU with time 0.06377060001250356\n",
      "run 2166, Training loss: 0.0014904863418699031, run on the CPU with time 0.06147129999590106\n",
      "run 2167, Training loss: 0.001506738888631596, run on the CPU with time 0.06308659998467192\n",
      "run 2168, Training loss: 0.0014962212873135948, run on the CPU with time 0.07199180001043715\n",
      "run 2169, Training loss: 0.0015028977097244934, run on the CPU with time 0.06089270001393743\n",
      "run 2170, Training loss: 0.0015034353323492475, run on the CPU with time 0.06290119999903254\n",
      "run 2171, Training loss: 0.0014881459560208234, run on the CPU with time 0.06481999999959953\n",
      "run 2172, Training loss: 0.001775094452552201, run on the CPU with time 0.06400899999425747\n",
      "run 2173, Training loss: 0.0015239772602771238, run on the CPU with time 0.06407059999764897\n",
      "run 2174, Training loss: 0.0015144263339276552, run on the CPU with time 0.06259089999366552\n",
      "run 2175, Training loss: 0.0014917871823523787, run on the CPU with time 0.06320299999788404\n",
      "run 2176, Training loss: 0.0014948608511159282, run on the CPU with time 0.06253779999678954\n",
      "run 2177, Training loss: 0.0014870008762624242, run on the CPU with time 0.060939199989661574\n",
      "run 2178, Training loss: 0.0014922774003935046, run on the CPU with time 0.06187739997403696\n",
      "run 2179, Training loss: 0.0015064395960176955, run on the CPU with time 0.06457919999957085\n",
      "run 2180, Training loss: 0.0014949749360146764, run on the CPU with time 0.0627260000037495\n",
      "run 2181, Training loss: 0.001499071033479347, run on the CPU with time 0.061505199992097914\n",
      "run 2182, Training loss: 0.0014973094604845921, run on the CPU with time 0.06072629999835044\n",
      "run 2183, Training loss: 0.0014946969059317118, run on the CPU with time 0.06303970000590198\n",
      "run 2184, Training loss: 0.0014813385651161132, run on the CPU with time 0.07278720001340844\n",
      "run 2185, Training loss: 0.001479032940354004, run on the CPU with time 0.06807320000370964\n",
      "run 2186, Training loss: 0.0014840626475316557, run on the CPU with time 0.06449709998560138\n",
      "run 2187, Training loss: 0.0014758790489312642, run on the CPU with time 0.0624596000125166\n",
      "run 2188, Training loss: 0.0014801681685970504, run on the CPU with time 0.06218849998549558\n",
      "run 2189, Training loss: 0.0014719142343396511, run on the CPU with time 0.06402900000102818\n",
      "run 2190, Training loss: 0.0014727671521541196, run on the CPU with time 0.06292309999116696\n",
      "run 2191, Training loss: 0.0014739910960832442, run on the CPU with time 0.06110160000389442\n",
      "run 2192, Training loss: 0.0014684438823182558, run on the CPU with time 0.061712299997452646\n",
      "run 2193, Training loss: 0.0015921272324869113, run on the CPU with time 0.06304790001013316\n",
      "run 2194, Training loss: 0.001471690399789209, run on the CPU with time 0.07536610000533983\n",
      "run 2195, Training loss: 0.0014707703778350895, run on the CPU with time 0.076319899992086\n",
      "run 2196, Training loss: 0.0014715608590367166, run on the CPU with time 0.06295759999193251\n",
      "run 2197, Training loss: 0.0014764545323321893, run on the CPU with time 0.06129099999088794\n",
      "run 2198, Training loss: 0.001462507697016752, run on the CPU with time 0.06205659999977797\n",
      "run 2199, Training loss: 0.0014654243614297064, run on the CPU with time 0.06411270002718084\n",
      "run 2200, Training loss: 0.0014675231900996137, run on the CPU with time 0.06204229997820221\n",
      "run 2201, Training loss: 0.0014683147087370427, run on the CPU with time 0.06137539999326691\n",
      "run 2202, Training loss: 0.0014637001648555848, run on the CPU with time 0.06093279999913648\n",
      "run 2203, Training loss: 0.001462239829363005, run on the CPU with time 0.06356830001459457\n",
      "run 2204, Training loss: 0.0014560496508651836, run on the CPU with time 0.06846420001238585\n",
      "run 2205, Training loss: 0.0014553063187287825, run on the CPU with time 0.074621500010835\n",
      "run 2206, Training loss: 0.001459624734971757, run on the CPU with time 0.06337600000551902\n",
      "run 2207, Training loss: 0.0014633149327974852, run on the CPU with time 0.0684698999975808\n",
      "run 2208, Training loss: 0.0014537319288964764, run on the CPU with time 0.06239919998915866\n",
      "run 2209, Training loss: 0.0014624143894939772, run on the CPU with time 0.06156349999946542\n",
      "run 2210, Training loss: 0.0014591764365005392, run on the CPU with time 0.06183289998443797\n",
      "run 2211, Training loss: 0.0014567750366536942, run on the CPU with time 0.061278400011360645\n",
      "run 2212, Training loss: 0.0014481769754969389, run on the CPU with time 0.06281969998963177\n",
      "run 2213, Training loss: 0.0014459487871756045, run on the CPU with time 0.06542249998892657\n",
      "run 2214, Training loss: 0.0014595373893826035, run on the CPU with time 0.06700529999216087\n",
      "run 2215, Training loss: 0.0014578983263246218, run on the CPU with time 0.06879080002545379\n",
      "run 2216, Training loss: 0.001447319689164446, run on the CPU with time 0.06329950000508688\n",
      "run 2217, Training loss: 0.00144827682987935, run on the CPU with time 0.06177719999686815\n",
      "run 2218, Training loss: 0.0014483352416639471, run on the CPU with time 0.06183469999814406\n",
      "run 2219, Training loss: 0.001437235062820202, run on the CPU with time 0.06409780000103638\n",
      "run 2220, Training loss: 0.0014366941339341627, run on the CPU with time 0.06411200002185069\n",
      "run 2221, Training loss: 0.001612214758510659, run on the CPU with time 0.06100350001361221\n",
      "run 2222, Training loss: 0.0015917227748352824, run on the CPU with time 0.061746499995933846\n",
      "run 2223, Training loss: 0.0016289338098431471, run on the CPU with time 0.061808900005416945\n",
      "run 2224, Training loss: 0.0015548928946373052, run on the CPU with time 0.06232169998111203\n",
      "run 2225, Training loss: 0.0014845418763632717, run on the CPU with time 0.07831310000619851\n",
      "run 2226, Training loss: 0.0014476925981315699, run on the CPU with time 0.07085550000192598\n",
      "run 2227, Training loss: 0.001443415818026882, run on the CPU with time 0.06289409997407347\n",
      "run 2228, Training loss: 0.0014329986752603541, run on the CPU with time 0.0623693999950774\n",
      "run 2229, Training loss: 0.0017112739954344843, run on the CPU with time 0.0634868000051938\n",
      "run 2230, Training loss: 0.0014370074874684427, run on the CPU with time 0.06416829998488538\n",
      "run 2231, Training loss: 0.0014310202477976087, run on the CPU with time 0.06046030001016334\n",
      "run 2232, Training loss: 0.0014372142001362095, run on the CPU with time 0.06094329999177717\n",
      "run 2233, Training loss: 0.0014327662682758686, run on the CPU with time 0.06673160000354983\n",
      "run 2234, Training loss: 0.0014333289008143103, run on the CPU with time 0.06550150000839494\n",
      "run 2235, Training loss: 0.0014286837934791533, run on the CPU with time 0.0672160999965854\n",
      "run 2236, Training loss: 0.0015550717276875564, run on the CPU with time 0.0641077000182122\n",
      "run 2237, Training loss: 0.0014441844414580952, run on the CPU with time 0.06199149999883957\n",
      "run 2238, Training loss: 0.001434775444332362, run on the CPU with time 0.06122589998994954\n",
      "run 2239, Training loss: 0.0014283816346009686, run on the CPU with time 0.06266339999274351\n",
      "run 2240, Training loss: 0.0014298929504796185, run on the CPU with time 0.0631299999949988\n",
      "run 2241, Training loss: 0.0014290182827442716, run on the CPU with time 0.061745000013615936\n",
      "run 2242, Training loss: 0.0014311183703698176, run on the CPU with time 0.06433029999607243\n",
      "run 2243, Training loss: 0.001421071486203635, run on the CPU with time 0.06551129999570549\n",
      "run 2244, Training loss: 0.0014218825546611863, run on the CPU with time 0.06853119999868795\n",
      "run 2245, Training loss: 0.0014224596786417268, run on the CPU with time 0.06222850002814084\n",
      "run 2246, Training loss: 0.0014158569420529106, run on the CPU with time 0.0656639999942854\n",
      "run 2247, Training loss: 0.001420488130944167, run on the CPU with time 0.06327360001159832\n",
      "run 2248, Training loss: 0.0014190770587778057, run on the CPU with time 0.061687200010055676\n",
      "run 2249, Training loss: 0.0014621047469766133, run on the CPU with time 0.06103769998298958\n",
      "run 2250, Training loss: 0.0014164147004273465, run on the CPU with time 0.061956799996551126\n",
      "run 2251, Training loss: 0.0014172957731220363, run on the CPU with time 0.061826500023016706\n",
      "run 2252, Training loss: 0.0014236629120039817, run on the CPU with time 0.06463249999796972\n",
      "run 2253, Training loss: 0.0014140350146176802, run on the CPU with time 0.06943929998669773\n",
      "run 2254, Training loss: 0.001419420846849045, run on the CPU with time 0.06811510000261478\n",
      "run 2255, Training loss: 0.0014091205813615076, run on the CPU with time 0.06292050000047311\n",
      "run 2256, Training loss: 0.001417360636696685, run on the CPU with time 0.06210410001222044\n",
      "run 2257, Training loss: 0.0014140319583186118, run on the CPU with time 0.06485660001635551\n",
      "run 2258, Training loss: 0.0014048560796337288, run on the CPU with time 0.0628380999842193\n",
      "run 2259, Training loss: 0.0014102598840757062, run on the CPU with time 0.06143659999361262\n",
      "run 2260, Training loss: 0.0013987776759418838, run on the CPU with time 0.060281399986706674\n",
      "run 2261, Training loss: 0.0014022477016128092, run on the CPU with time 0.0610512999992352\n",
      "run 2262, Training loss: 0.0014824317460641562, run on the CPU with time 0.06333720000111498\n",
      "run 2263, Training loss: 0.0014023057633577, run on the CPU with time 0.07452429999830201\n",
      "run 2264, Training loss: 0.0014158962599388112, run on the CPU with time 0.06952590000582859\n",
      "run 2265, Training loss: 0.0014118770825337957, run on the CPU with time 0.07171869999729097\n",
      "run 2266, Training loss: 0.0013992199518146331, run on the CPU with time 0.0634332999761682\n",
      "run 2267, Training loss: 0.0013947767721467906, run on the CPU with time 0.062198300001909956\n",
      "run 2268, Training loss: 0.0013974127963799468, run on the CPU with time 0.06272320001153275\n",
      "run 2269, Training loss: 0.001393216467113234, run on the CPU with time 0.06314769998425618\n",
      "run 2270, Training loss: 0.0013911156860227823, run on the CPU with time 0.061352699995040894\n",
      "run 2271, Training loss: 0.001390992751310643, run on the CPU with time 0.06228179999743588\n",
      "run 2272, Training loss: 0.0014071623585054608, run on the CPU with time 0.06538369998452254\n",
      "run 2273, Training loss: 0.0013925420995457732, run on the CPU with time 0.07353260001400486\n",
      "run 2274, Training loss: 0.0014046226476652505, run on the CPU with time 0.06485340002109297\n",
      "run 2275, Training loss: 0.0013966123626398092, run on the CPU with time 0.06290819999412633\n",
      "run 2276, Training loss: 0.0013857314609181644, run on the CPU with time 0.06274240001221187\n",
      "run 2277, Training loss: 0.0014013155788151462, run on the CPU with time 0.06571399999666028\n",
      "run 2278, Training loss: 0.001386381556684765, run on the CPU with time 0.062450299999909475\n",
      "run 2279, Training loss: 0.001385702656162638, run on the CPU with time 0.06144779999158345\n",
      "run 2280, Training loss: 0.001386340218124877, run on the CPU with time 0.06651550001697615\n",
      "run 2281, Training loss: 0.0013872096093440302, run on the CPU with time 0.08339640000485815\n",
      "run 2282, Training loss: 0.0013875180777342228, run on the CPU with time 0.06577429999015294\n",
      "run 2283, Training loss: 0.0013809798848259643, run on the CPU with time 0.06580250000115484\n",
      "run 2284, Training loss: 0.0013839365171820232, run on the CPU with time 0.06291779997991398\n",
      "run 2285, Training loss: 0.0013825335205184333, run on the CPU with time 0.06250250001903623\n",
      "run 2286, Training loss: 0.0015085663696068382, run on the CPU with time 0.06068599998252466\n",
      "run 2287, Training loss: 0.0013919226550073787, run on the CPU with time 0.06171620002714917\n",
      "run 2288, Training loss: 0.0013812040497379547, run on the CPU with time 0.061770200001774356\n",
      "run 2289, Training loss: 0.0014977873999503738, run on the CPU with time 0.07659509999211878\n",
      "run 2290, Training loss: 0.001387775366707832, run on the CPU with time 0.06600359998992644\n",
      "run 2291, Training loss: 0.0013775370948761437, run on the CPU with time 0.06726989999879152\n",
      "run 2292, Training loss: 0.0013749345866844206, run on the CPU with time 0.06195500001194887\n",
      "run 2293, Training loss: 0.001643930178397568, run on the CPU with time 0.06184909999137744\n",
      "run 2294, Training loss: 0.0013786117583301596, run on the CPU with time 0.06123750002007\n",
      "run 2295, Training loss: 0.001385059503610889, run on the CPU with time 0.06101430000853725\n",
      "run 2296, Training loss: 0.0013940587765318923, run on the CPU with time 0.06133799999952316\n",
      "run 2297, Training loss: 0.0013752316986814217, run on the CPU with time 0.06549599999561906\n",
      "run 2298, Training loss: 0.0013694471948176876, run on the CPU with time 0.07638010001392104\n",
      "run 2299, Training loss: 0.0013686694994579408, run on the CPU with time 0.06439039998804219\n",
      "run 2300, Training loss: 0.0013720062032172625, run on the CPU with time 0.0637779000098817\n",
      "run 2301, Training loss: 0.0013676854901281, run on the CPU with time 0.06420789999538101\n",
      "run 2302, Training loss: 0.001368578610708937, run on the CPU with time 0.062410199985606596\n",
      "run 2303, Training loss: 0.0013703150679992343, run on the CPU with time 0.08000019998871721\n",
      "run 2304, Training loss: 0.00137110921747411, run on the CPU with time 0.06539259999408387\n",
      "run 2305, Training loss: 0.0013607019966002554, run on the CPU with time 0.06178419999196194\n",
      "run 2306, Training loss: 0.001362162683587733, run on the CPU with time 0.06962429999839514\n",
      "run 2307, Training loss: 0.0016289872083275854, run on the CPU with time 0.06711689999792725\n",
      "run 2308, Training loss: 0.00136514799709072, run on the CPU with time 0.06373389999498613\n",
      "run 2309, Training loss: 0.0013649338022250132, run on the CPU with time 0.0631578000029549\n",
      "run 2310, Training loss: 0.0013632719949121317, run on the CPU with time 0.06413759998395108\n",
      "run 2311, Training loss: 0.0013670533609745855, run on the CPU with time 0.06219430000055581\n",
      "run 2312, Training loss: 0.0013581144797461257, run on the CPU with time 0.06205539999064058\n",
      "run 2313, Training loss: 0.0013616990521603096, run on the CPU with time 0.06218930002069101\n",
      "run 2314, Training loss: 0.0013565702092653903, run on the CPU with time 0.060924200020963326\n",
      "run 2315, Training loss: 0.0013754733942650174, run on the CPU with time 0.061270899983355775\n",
      "run 2316, Training loss: 0.001353443247221135, run on the CPU with time 0.06287800002610311\n",
      "run 2317, Training loss: 0.0016204506808373314, run on the CPU with time 0.0776280999998562\n",
      "run 2318, Training loss: 0.0013552287057047414, run on the CPU with time 0.06637390001560561\n",
      "run 2319, Training loss: 0.0013551030660230688, run on the CPU with time 0.0627686999796424\n",
      "run 2320, Training loss: 0.0013493562427439347, run on the CPU with time 0.06322430001455359\n",
      "run 2321, Training loss: 0.0013472818238369655, run on the CPU with time 0.06311359998653643\n",
      "run 2322, Training loss: 0.0013540098011700055, run on the CPU with time 0.06193990001338534\n",
      "run 2323, Training loss: 0.0013653081356262025, run on the CPU with time 0.06246619997546077\n",
      "run 2324, Training loss: 0.0013619279373828744, run on the CPU with time 0.060619199997745454\n",
      "run 2325, Training loss: 0.0013501705247273839, run on the CPU with time 0.06136780002270825\n",
      "run 2326, Training loss: 0.0013426398931518303, run on the CPU with time 0.060699299996485934\n",
      "run 2327, Training loss: 0.0013480513462458144, run on the CPU with time 0.06495050000376068\n",
      "run 2328, Training loss: 0.0013448468449827158, run on the CPU with time 0.08216930000344291\n",
      "run 2329, Training loss: 0.0013533865132062189, run on the CPU with time 0.06271989998640493\n",
      "run 2330, Training loss: 0.0013457337494249993, run on the CPU with time 0.06209240001044236\n",
      "run 2331, Training loss: 0.0013427507617796602, run on the CPU with time 0.06226479998440482\n",
      "run 2332, Training loss: 0.0013433287186093035, run on the CPU with time 0.06427000000257976\n",
      "run 2333, Training loss: 0.0013485358377114277, run on the CPU with time 0.06421430001500994\n",
      "run 2334, Training loss: 0.0013459445485337214, run on the CPU with time 0.06232049997197464\n",
      "run 2335, Training loss: 0.0013416395533237268, run on the CPU with time 0.06232969998382032\n",
      "run 2336, Training loss: 0.0013372062886694701, run on the CPU with time 0.0646658000187017\n",
      "run 2337, Training loss: 0.001331127434390179, run on the CPU with time 0.06447680000565015\n",
      "run 2338, Training loss: 0.0013288085849622307, run on the CPU with time 0.06393440000829287\n",
      "run 2339, Training loss: 0.0013536128148023802, run on the CPU with time 0.0633289999968838\n",
      "run 2340, Training loss: 0.0013344777589240535, run on the CPU with time 0.06372730000293814\n",
      "run 2341, Training loss: 0.001329193763112099, run on the CPU with time 0.06193209998309612\n",
      "run 2342, Training loss: 0.0013313968114247968, run on the CPU with time 0.06316590000642464\n",
      "run 2343, Training loss: 0.001325627728517082, run on the CPU with time 0.0657927000138443\n",
      "run 2344, Training loss: 0.0013390765295877248, run on the CPU with time 0.06547639999189414\n",
      "run 2345, Training loss: 0.0013292077977728862, run on the CPU with time 0.06138850000570528\n",
      "run 2346, Training loss: 0.0013298960540189663, run on the CPU with time 0.06649480000487529\n",
      "run 2347, Training loss: 0.0013346111704198517, run on the CPU with time 0.0623697999981232\n",
      "run 2348, Training loss: 0.001322375333389093, run on the CPU with time 0.06847359999665059\n",
      "run 2349, Training loss: 0.0013295682168311693, run on the CPU with time 0.06351479998556897\n",
      "run 2350, Training loss: 0.0013227050645086406, run on the CPU with time 0.06227789999684319\n",
      "run 2351, Training loss: 0.0013275871134035035, run on the CPU with time 0.06327730001066811\n",
      "run 2352, Training loss: 0.0013196826764297757, run on the CPU with time 0.06299129998660646\n",
      "run 2353, Training loss: 0.0013171625941305882, run on the CPU with time 0.06304699997417629\n",
      "run 2354, Training loss: 0.0013316366702466357, run on the CPU with time 0.061957099998835474\n",
      "run 2355, Training loss: 0.001317563502313781, run on the CPU with time 0.06276440000510775\n",
      "run 2356, Training loss: 0.0013260669270742007, run on the CPU with time 0.06236529999296181\n",
      "run 2357, Training loss: 0.0013119189968247983, run on the CPU with time 0.061143800005083904\n",
      "run 2358, Training loss: 0.0013265710429881106, run on the CPU with time 0.0631578000029549\n",
      "run 2359, Training loss: 0.001314239835350732, run on the CPU with time 0.0636233999975957\n",
      "run 2360, Training loss: 0.0013171388099594466, run on the CPU with time 0.06498719999217428\n",
      "run 2361, Training loss: 0.0013163107168855442, run on the CPU with time 0.06207179999910295\n",
      "run 2362, Training loss: 0.001313711027284047, run on the CPU with time 0.0617711000086274\n",
      "run 2363, Training loss: 0.001328683427808838, run on the CPU with time 0.06260390000534244\n",
      "run 2364, Training loss: 0.0013185528073914941, run on the CPU with time 0.06281860001035966\n",
      "run 2365, Training loss: 0.0014223681740325198, run on the CPU with time 0.06173250000574626\n",
      "run 2366, Training loss: 0.0013237971213873772, run on the CPU with time 0.06614549999358132\n",
      "run 2367, Training loss: 0.0013131428988179489, run on the CPU with time 0.0627946000022348\n",
      "run 2368, Training loss: 0.001307050851226615, run on the CPU with time 0.06347759999334812\n",
      "run 2369, Training loss: 0.0013516298918561502, run on the CPU with time 0.06228620000183582\n",
      "run 2370, Training loss: 0.001338020577201281, run on the CPU with time 0.06260060000931844\n",
      "run 2371, Training loss: 0.0013075846878398971, run on the CPU with time 0.0619752999919001\n",
      "run 2372, Training loss: 0.0013067370123198171, run on the CPU with time 0.06537299999035895\n",
      "run 2373, Training loss: 0.0013011055741861293, run on the CPU with time 0.06365670001832768\n",
      "run 2374, Training loss: 0.0013041598312239248, run on the CPU with time 0.06976339998072945\n",
      "run 2375, Training loss: 0.001305603447558083, run on the CPU with time 0.0669546999852173\n",
      "run 2376, Training loss: 0.001300630091943524, run on the CPU with time 0.06322430001455359\n",
      "run 2377, Training loss: 0.001416734361290847, run on the CPU with time 0.06135579998954199\n",
      "run 2378, Training loss: 0.0013043513540072706, run on the CPU with time 0.06255010000313632\n",
      "run 2379, Training loss: 0.00130290897802545, run on the CPU with time 0.06240950000938028\n",
      "run 2380, Training loss: 0.0012989696690056008, run on the CPU with time 0.06366730001172982\n",
      "run 2381, Training loss: 0.0012993094371764032, run on the CPU with time 0.061563000024762005\n",
      "run 2382, Training loss: 0.0013051763165011917, run on the CPU with time 0.0626674999948591\n",
      "run 2383, Training loss: 0.0013006028805896427, run on the CPU with time 0.08087599999271333\n",
      "run 2384, Training loss: 0.001290811101147359, run on the CPU with time 0.06346460001077503\n",
      "run 2385, Training loss: 0.001301177096981767, run on the CPU with time 0.06418859999394044\n",
      "run 2386, Training loss: 0.0012967596838610585, run on the CPU with time 0.06250260001979768\n",
      "run 2387, Training loss: 0.001292993028817529, run on the CPU with time 0.06670559998019598\n",
      "run 2388, Training loss: 0.001296189247328915, run on the CPU with time 0.06208650002372451\n",
      "run 2389, Training loss: 0.0012930253916155461, run on the CPU with time 0.061928799987072125\n",
      "run 2390, Training loss: 0.0012964020787793826, run on the CPU with time 0.06072619999758899\n",
      "run 2391, Training loss: 0.0012869868737073954, run on the CPU with time 0.06244380000862293\n",
      "run 2392, Training loss: 0.0012969470341142202, run on the CPU with time 0.060993999999482185\n",
      "run 2393, Training loss: 0.001302155571267411, run on the CPU with time 0.06410199997480959\n",
      "run 2394, Training loss: 0.0012925812246976421, run on the CPU with time 0.06614499998977408\n",
      "run 2395, Training loss: 0.001288287004122553, run on the CPU with time 0.08651749999262393\n",
      "run 2396, Training loss: 0.0012870956578725865, run on the CPU with time 0.06604429997969419\n",
      "run 2397, Training loss: 0.001292082489552823, run on the CPU with time 0.06221410000580363\n",
      "run 2398, Training loss: 0.0013350895642361138, run on the CPU with time 0.0666641999850981\n",
      "run 2399, Training loss: 0.0012834872439808267, run on the CPU with time 0.06326799999806099\n",
      "run 2400, Training loss: 0.0012794540530938485, run on the CPU with time 0.0622929000237491\n",
      "run 2401, Training loss: 0.0012809955158776243, run on the CPU with time 0.060809999995399266\n",
      "run 2402, Training loss: 0.001275107872524214, run on the CPU with time 0.06161450000945479\n",
      "run 2403, Training loss: 0.00127902912724742, run on the CPU with time 0.06308379999245517\n",
      "run 2404, Training loss: 0.0012776485278498677, run on the CPU with time 0.07001409999793395\n",
      "run 2405, Training loss: 0.0012755900660956533, run on the CPU with time 0.06772720001754351\n",
      "run 2406, Training loss: 0.0012732237027938192, run on the CPU with time 0.0645795000018552\n",
      "run 2407, Training loss: 0.00127795685022482, run on the CPU with time 0.06310649999068119\n",
      "run 2408, Training loss: 0.001272833641004664, run on the CPU with time 0.06393730000127107\n",
      "run 2409, Training loss: 0.001274017038337082, run on the CPU with time 0.06396679999306798\n",
      "run 2410, Training loss: 0.0013301131918772378, run on the CPU with time 0.062013099988689646\n",
      "run 2411, Training loss: 0.001279428044455761, run on the CPU with time 0.06278540001949295\n",
      "run 2412, Training loss: 0.0012703742930501572, run on the CPU with time 0.06024689998594113\n",
      "run 2413, Training loss: 0.0012741966828550423, run on the CPU with time 0.06167910000658594\n",
      "run 2414, Training loss: 0.0012699928928423209, run on the CPU with time 0.07397810000111349\n",
      "run 2415, Training loss: 0.0012818976321299985, run on the CPU with time 0.07195479999063537\n",
      "run 2416, Training loss: 0.001286912297853798, run on the CPU with time 0.06335449998732656\n",
      "run 2417, Training loss: 0.0012691189635502683, run on the CPU with time 0.06334369999240153\n",
      "run 2418, Training loss: 0.0012724995425246147, run on the CPU with time 0.06271950001246296\n",
      "run 2419, Training loss: 0.0012656749568122905, run on the CPU with time 0.0637873999949079\n",
      "run 2420, Training loss: 0.0012679040893668902, run on the CPU with time 0.06266589998267591\n",
      "run 2421, Training loss: 0.0012620173820654269, run on the CPU with time 0.06054830001085065\n",
      "run 2422, Training loss: 0.001270619814617517, run on the CPU with time 0.06784380000317469\n",
      "run 2423, Training loss: 0.0012650205077741042, run on the CPU with time 0.0672140000096988\n",
      "run 2424, Training loss: 0.0012658475682573308, run on the CPU with time 0.06277489999774843\n",
      "run 2425, Training loss: 0.001267387968189062, run on the CPU with time 0.061204400000860915\n",
      "run 2426, Training loss: 0.0012674360396803504, run on the CPU with time 0.06120789999840781\n",
      "run 2427, Training loss: 0.0012627167014712044, run on the CPU with time 0.06574499999987893\n",
      "run 2428, Training loss: 0.0012594230492223605, run on the CPU with time 0.060961199982557446\n",
      "run 2429, Training loss: 0.0012557017071892253, run on the CPU with time 0.06192639999790117\n",
      "run 2430, Training loss: 0.0012586144061722073, run on the CPU with time 0.06560960001661442\n",
      "run 2431, Training loss: 0.0012521491616181183, run on the CPU with time 0.062497699982486665\n",
      "run 2432, Training loss: 0.0012594411261406177, run on the CPU with time 0.0612550999794621\n",
      "run 2433, Training loss: 0.0012786467314898882, run on the CPU with time 0.06742790000862442\n",
      "run 2434, Training loss: 0.0012577682886330877, run on the CPU with time 0.0646749000006821\n",
      "run 2435, Training loss: 0.0012491987160088188, run on the CPU with time 0.06316759999026544\n",
      "run 2436, Training loss: 0.0012598418333998945, run on the CPU with time 0.06228240000200458\n",
      "run 2437, Training loss: 0.0012558153173317392, run on the CPU with time 0.062374600005568936\n",
      "run 2438, Training loss: 0.0012494735622154125, run on the CPU with time 0.06206339999334887\n",
      "run 2439, Training loss: 0.001247074340327262, run on the CPU with time 0.06204200000502169\n",
      "run 2440, Training loss: 0.0012548868558290203, run on the CPU with time 0.06197079998673871\n",
      "run 2441, Training loss: 0.0012457531535801536, run on the CPU with time 0.061377500009257346\n",
      "run 2442, Training loss: 0.0012537730097723066, run on the CPU with time 0.06202330000814982\n",
      "run 2443, Training loss: 0.001441707082267385, run on the CPU with time 0.06902789999730885\n",
      "run 2444, Training loss: 0.0012581716081926557, run on the CPU with time 0.07264550001127645\n",
      "run 2445, Training loss: 0.0012486060487307523, run on the CPU with time 0.06184519999078475\n",
      "run 2446, Training loss: 0.0012462829235036308, run on the CPU with time 0.061029400007100776\n",
      "run 2447, Training loss: 0.0012422890182868039, run on the CPU with time 0.06365540000842884\n",
      "run 2448, Training loss: 0.0012448745486009019, run on the CPU with time 0.06128319998970255\n",
      "run 2449, Training loss: 0.001244586693726226, run on the CPU with time 0.0616749000037089\n",
      "run 2450, Training loss: 0.0012366393588970162, run on the CPU with time 0.0608955999778118\n",
      "run 2451, Training loss: 0.0012376539412798592, run on the CPU with time 0.06269990000873804\n",
      "run 2452, Training loss: 0.0012364909243263934, run on the CPU with time 0.06308329998864792\n",
      "run 2453, Training loss: 0.001237074604101839, run on the CPU with time 0.06305060000158846\n",
      "run 2454, Training loss: 0.0012333194086915516, run on the CPU with time 0.07634540001163259\n",
      "run 2455, Training loss: 0.001243815842132211, run on the CPU with time 0.07999389999895357\n",
      "run 2456, Training loss: 0.0012507506045618688, run on the CPU with time 0.06292029999895021\n",
      "run 2457, Training loss: 0.001243647062801756, run on the CPU with time 0.06192480001482181\n",
      "run 2458, Training loss: 0.001236526710520345, run on the CPU with time 0.062350899999728426\n",
      "run 2459, Training loss: 0.0012402415108226706, run on the CPU with time 0.06243520000134595\n",
      "run 2460, Training loss: 0.0012343309541889044, run on the CPU with time 0.061837099987315014\n",
      "run 2461, Training loss: 0.0012326832847479223, run on the CPU with time 0.06372920001740567\n",
      "run 2462, Training loss: 0.0012526216338632037, run on the CPU with time 0.06303920000209473\n",
      "run 2463, Training loss: 0.0012324087240399835, run on the CPU with time 0.06285149999894202\n",
      "run 2464, Training loss: 0.0012351745579838858, run on the CPU with time 0.07086340000387281\n",
      "run 2465, Training loss: 0.0012314659260399905, run on the CPU with time 0.07201629999326542\n",
      "run 2466, Training loss: 0.0012287151386606804, run on the CPU with time 0.06243049999466166\n",
      "run 2467, Training loss: 0.0012326045501553876, run on the CPU with time 0.06272280000848696\n",
      "run 2468, Training loss: 0.0012259242399357555, run on the CPU with time 0.0635541999945417\n",
      "run 2469, Training loss: 0.0012262991144697563, run on the CPU with time 0.06288739998126402\n",
      "run 2470, Training loss: 0.0012281047802720092, run on the CPU with time 0.06920220001484267\n",
      "run 2471, Training loss: 0.0012270882539847992, run on the CPU with time 0.06426909999572672\n",
      "run 2472, Training loss: 0.0012396798002142035, run on the CPU with time 0.06133839997346513\n",
      "run 2473, Training loss: 0.001221958176690663, run on the CPU with time 0.06441610000911169\n",
      "run 2474, Training loss: 0.0012240713480754163, run on the CPU with time 0.06458929998916574\n",
      "run 2475, Training loss: 0.0012249813964775637, run on the CPU with time 0.06392870002309792\n",
      "run 2476, Training loss: 0.001219386176010382, run on the CPU with time 0.0634806000161916\n",
      "run 2477, Training loss: 0.0012238669141714292, run on the CPU with time 0.06353230000240728\n",
      "run 2478, Training loss: 0.0012181963754053207, run on the CPU with time 0.06337519999942742\n",
      "run 2479, Training loss: 0.0012153658257871443, run on the CPU with time 0.06175819999771193\n",
      "run 2480, Training loss: 0.0012254863741955804, run on the CPU with time 0.06299249999574386\n",
      "run 2481, Training loss: 0.0012347687205130403, run on the CPU with time 0.06252519998815842\n",
      "run 2482, Training loss: 0.001274114369748118, run on the CPU with time 0.07055959998979233\n",
      "run 2483, Training loss: 0.0012199333423103037, run on the CPU with time 0.06402400002116337\n",
      "run 2484, Training loss: 0.0012175586455586281, run on the CPU with time 0.06397199997445568\n",
      "run 2485, Training loss: 0.0012138034288496932, run on the CPU with time 0.06494720000773668\n",
      "run 2486, Training loss: 0.0012125499788618816, run on the CPU with time 0.06622280000010505\n",
      "run 2487, Training loss: 0.001213314532403522, run on the CPU with time 0.06251369998790324\n",
      "run 2488, Training loss: 0.0012162631688459606, run on the CPU with time 0.06276629999047145\n",
      "run 2489, Training loss: 0.001218243079215013, run on the CPU with time 0.06888460001209751\n",
      "run 2490, Training loss: 0.0012141702590848912, run on the CPU with time 0.06355649998295121\n",
      "run 2491, Training loss: 0.001220635402238589, run on the CPU with time 0.0661244000075385\n",
      "run 2492, Training loss: 0.0012098114043427896, run on the CPU with time 0.06287950000842102\n",
      "run 2493, Training loss: 0.0012089445991891395, run on the CPU with time 0.0633254999993369\n",
      "run 2494, Training loss: 0.0012092150913800156, run on the CPU with time 0.06297940001240931\n",
      "run 2495, Training loss: 0.0012150951188893734, run on the CPU with time 0.0612431000045035\n",
      "run 2496, Training loss: 0.0012248556509307077, run on the CPU with time 0.06305399999837391\n",
      "run 2497, Training loss: 0.0012112010667888998, run on the CPU with time 0.061971799994353205\n",
      "run 2498, Training loss: 0.0012153686818650881, run on the CPU with time 0.06558840000070632\n",
      "run 2499, Training loss: 0.00125804909277411, run on the CPU with time 0.06363689998397604\n",
      "run 2500, Training loss: 0.0012180984795453365, run on the CPU with time 0.06297070000437088\n",
      "run 2501, Training loss: 0.0012043687465351964, run on the CPU with time 0.06266160000814125\n",
      "run 2502, Training loss: 0.0012050721296955917, run on the CPU with time 0.06256740001845174\n",
      "run 2503, Training loss: 0.0012050627077273516, run on the CPU with time 0.06237859997781925\n",
      "run 2504, Training loss: 0.0012039338815322315, run on the CPU with time 0.061188000021502376\n",
      "run 2505, Training loss: 0.0012093759649856523, run on the CPU with time 0.06083189998753369\n",
      "run 2506, Training loss: 0.0011998870709000833, run on the CPU with time 0.06376199997612275\n",
      "run 2507, Training loss: 0.0012013367681488903, run on the CPU with time 0.06218400000943802\n",
      "run 2508, Training loss: 0.0011954519505624457, run on the CPU with time 0.08648549998179078\n",
      "run 2509, Training loss: 0.0011979401932463091, run on the CPU with time 0.07125420001102611\n",
      "run 2510, Training loss: 0.001201442545888395, run on the CPU with time 0.06219619998591952\n",
      "run 2511, Training loss: 0.0011945573631568218, run on the CPU with time 0.06311109999660403\n",
      "run 2512, Training loss: 0.0011939746935571416, run on the CPU with time 0.061949700000695884\n",
      "run 2513, Training loss: 0.0011934223901665642, run on the CPU with time 0.06296700000530109\n",
      "run 2514, Training loss: 0.0011979240381730382, run on the CPU with time 0.06092920000082813\n",
      "run 2515, Training loss: 0.0011919803686230974, run on the CPU with time 0.06266429999959655\n",
      "run 2516, Training loss: 0.0011992000464074821, run on the CPU with time 0.06090000001131557\n",
      "run 2517, Training loss: 0.0011945611469855067, run on the CPU with time 0.07058289999258704\n",
      "run 2518, Training loss: 0.0011904381103291103, run on the CPU with time 0.0669177999952808\n",
      "run 2519, Training loss: 0.0011906113966357555, run on the CPU with time 0.06201520000468008\n",
      "run 2520, Training loss: 0.0011918970199820416, run on the CPU with time 0.06270070001482964\n",
      "run 2521, Training loss: 0.0011923254470192742, run on the CPU with time 0.0661575999984052\n",
      "run 2522, Training loss: 0.001195174179090166, run on the CPU with time 0.07015839999075979\n",
      "run 2523, Training loss: 0.0011916709267428484, run on the CPU with time 0.06344960001297295\n",
      "run 2524, Training loss: 0.0011840789507583725, run on the CPU with time 0.06325549999019131\n",
      "run 2525, Training loss: 0.0011928609480078608, run on the CPU with time 0.06283259997144341\n",
      "run 2526, Training loss: 0.0011886389400884085, run on the CPU with time 0.06287150000571273\n",
      "run 2527, Training loss: 0.0011863398873670535, run on the CPU with time 0.06272250000620261\n",
      "run 2528, Training loss: 0.0011817582901461389, run on the CPU with time 0.06223750000935979\n",
      "run 2529, Training loss: 0.0011868894143844955, run on the CPU with time 0.06608279998181388\n",
      "run 2530, Training loss: 0.0011824623521533794, run on the CPU with time 0.06644029999733903\n",
      "run 2531, Training loss: 0.0011890790906777097, run on the CPU with time 0.06261019999510609\n",
      "run 2532, Training loss: 0.0011851381154304942, run on the CPU with time 0.06182880001142621\n",
      "run 2533, Training loss: 0.0011869437633405058, run on the CPU with time 0.061435500014340505\n",
      "run 2534, Training loss: 0.001180244978340025, run on the CPU with time 0.06295890000183135\n",
      "run 2535, Training loss: 0.001182527352053016, run on the CPU with time 0.06540220000897534\n",
      "run 2536, Training loss: 0.0011832572267105042, run on the CPU with time 0.06322509999154136\n",
      "run 2537, Training loss: 0.0011863688196172006, run on the CPU with time 0.07029119998333044\n",
      "run 2538, Training loss: 0.0011808585747944149, run on the CPU with time 0.06616349998512305\n",
      "run 2539, Training loss: 0.0011762078972506888, run on the CPU with time 0.06363560000318103\n",
      "run 2540, Training loss: 0.0011744738863559302, run on the CPU with time 0.06416219999664463\n",
      "run 2541, Training loss: 0.0011784128326822114, run on the CPU with time 0.06429229999775998\n",
      "run 2542, Training loss: 0.001181224439443957, run on the CPU with time 0.06253809999907389\n",
      "run 2543, Training loss: 0.0011806359424636785, run on the CPU with time 0.06326610001269728\n",
      "run 2544, Training loss: 0.00117684593410972, run on the CPU with time 0.06307079998077825\n",
      "run 2545, Training loss: 0.0011750065137510484, run on the CPU with time 0.06181380001362413\n",
      "run 2546, Training loss: 0.0011737065114919626, run on the CPU with time 0.06317470001522452\n",
      "run 2547, Training loss: 0.0011736224943888374, run on the CPU with time 0.06169099998078309\n",
      "run 2548, Training loss: 0.0011670371220216939, run on the CPU with time 0.062017599993851036\n",
      "run 2549, Training loss: 0.0011715633420284245, run on the CPU with time 0.06845639998209663\n",
      "run 2550, Training loss: 0.001175873005294389, run on the CPU with time 0.06311920000007376\n",
      "run 2551, Training loss: 0.001171256483873797, run on the CPU with time 0.06732050000573508\n",
      "run 2552, Training loss: 0.0011643574425100021, run on the CPU with time 0.062223699991591275\n",
      "run 2553, Training loss: 0.0011788317186239344, run on the CPU with time 0.06525610000244342\n",
      "run 2554, Training loss: 0.001169679895445535, run on the CPU with time 0.06307800000649877\n",
      "run 2555, Training loss: 0.0011680331276279917, run on the CPU with time 0.06756039999891073\n",
      "run 2556, Training loss: 0.0011688807058486748, run on the CPU with time 0.06363709998549893\n",
      "run 2557, Training loss: 0.0011715992212306115, run on the CPU with time 0.06413979997159913\n",
      "run 2558, Training loss: 0.0011660118970956484, run on the CPU with time 0.06231969999498688\n",
      "run 2559, Training loss: 0.0011723993649039501, run on the CPU with time 0.061127399996621534\n",
      "run 2560, Training loss: 0.0011702591831470439, run on the CPU with time 0.06106109998654574\n",
      "run 2561, Training loss: 0.0011709330071673983, run on the CPU with time 0.06168969999998808\n",
      "run 2562, Training loss: 0.0011634315289649584, run on the CPU with time 0.061690300004556775\n",
      "run 2563, Training loss: 0.0011564416854856641, run on the CPU with time 0.06271140000899322\n",
      "run 2564, Training loss: 0.001162100653428669, run on the CPU with time 0.08727150000049733\n",
      "run 2565, Training loss: 0.0011663500785785304, run on the CPU with time 0.06267429998843\n",
      "run 2566, Training loss: 0.0011604963306374636, run on the CPU with time 0.06499400001484901\n",
      "run 2567, Training loss: 0.0011678247988377486, run on the CPU with time 0.0630634999834001\n",
      "run 2568, Training loss: 0.0011591051935366439, run on the CPU with time 0.06637719998252578\n",
      "run 2569, Training loss: 0.0011563606128964404, run on the CPU with time 0.06200509998598136\n",
      "run 2570, Training loss: 0.0011529549577971921, run on the CPU with time 0.061979400023119524\n",
      "run 2571, Training loss: 0.0012642895900750195, run on the CPU with time 0.06161279999651015\n",
      "run 2572, Training loss: 0.0011950078075725205, run on the CPU with time 0.06829799999832176\n",
      "run 2573, Training loss: 0.001161918767080682, run on the CPU with time 0.06833999999798834\n",
      "run 2574, Training loss: 0.0011611997059679364, run on the CPU with time 0.06524630001513287\n",
      "run 2575, Training loss: 0.0011558234627459156, run on the CPU with time 0.061927200003992766\n",
      "run 2576, Training loss: 0.0011513745531953068, run on the CPU with time 0.06664260002435185\n",
      "run 2577, Training loss: 0.0011633749356736768, run on the CPU with time 0.06448830000590533\n",
      "run 2578, Training loss: 0.0011498127391942861, run on the CPU with time 0.061936200014315546\n",
      "run 2579, Training loss: 0.001150430178015747, run on the CPU with time 0.06289380000089295\n",
      "run 2580, Training loss: 0.0011506563461311586, run on the CPU with time 0.06259079999290407\n",
      "run 2581, Training loss: 0.0011529499540549957, run on the CPU with time 0.0658224000071641\n",
      "run 2582, Training loss: 0.0012028841266080482, run on the CPU with time 0.061882899986812845\n",
      "run 2583, Training loss: 0.0011477872249418975, run on the CPU with time 0.06320750000304542\n",
      "run 2584, Training loss: 0.0011510833279647738, run on the CPU with time 0.06613250001100823\n",
      "run 2585, Training loss: 0.001144531251140341, run on the CPU with time 0.06386030002613552\n",
      "run 2586, Training loss: 0.0011682386322883593, run on the CPU with time 0.06372659999760799\n",
      "run 2587, Training loss: 0.0011468762916840866, run on the CPU with time 0.06134590000147\n",
      "run 2588, Training loss: 0.0011415519987084818, run on the CPU with time 0.06216229998972267\n",
      "run 2589, Training loss: 0.0011477851745439692, run on the CPU with time 0.06611929999780841\n",
      "run 2590, Training loss: 0.001143588443995792, run on the CPU with time 0.0650049000105355\n",
      "run 2591, Training loss: 0.0011472919307958694, run on the CPU with time 0.0618980000144802\n",
      "run 2592, Training loss: 0.0011382804787320889, run on the CPU with time 0.06610019999789074\n",
      "run 2593, Training loss: 0.0011386069567048583, run on the CPU with time 0.06903579999925569\n",
      "run 2594, Training loss: 0.001137635513607, run on the CPU with time 0.0626044999808073\n",
      "run 2595, Training loss: 0.0011400642732703338, run on the CPU with time 0.06413980000070296\n",
      "run 2596, Training loss: 0.0011351468960955654, run on the CPU with time 0.06127800000831485\n",
      "run 2597, Training loss: 0.0011429076504348567, run on the CPU with time 0.06207020001602359\n",
      "run 2598, Training loss: 0.0011373132743756286, run on the CPU with time 0.06296079998719506\n",
      "run 2599, Training loss: 0.0011406389355463696, run on the CPU with time 0.06325890001608059\n",
      "run 2600, Training loss: 0.0011339595238413578, run on the CPU with time 0.06852739999885671\n",
      "run 2601, Training loss: 0.0011358061608585889, run on the CPU with time 0.06229529998381622\n",
      "run 2602, Training loss: 0.001139673031535825, run on the CPU with time 0.060955399996601045\n",
      "run 2603, Training loss: 0.001135766177983235, run on the CPU with time 0.06093439998221584\n",
      "run 2604, Training loss: 0.0011363552579000083, run on the CPU with time 0.0650753999943845\n",
      "run 2605, Training loss: 0.0011329944066925566, run on the CPU with time 0.06632769998395815\n",
      "run 2606, Training loss: 0.0011314760775845156, run on the CPU with time 0.07296560000395402\n",
      "run 2607, Training loss: 0.0012304488131617704, run on the CPU with time 0.06555589998606592\n",
      "run 2608, Training loss: 0.001151589630502382, run on the CPU with time 0.0624004999990575\n",
      "run 2609, Training loss: 0.0011358694241525055, run on the CPU with time 0.06089349999092519\n",
      "run 2610, Training loss: 0.001135032517578855, run on the CPU with time 0.06131620000815019\n",
      "run 2611, Training loss: 0.0011271817694333466, run on the CPU with time 0.0646061000006739\n",
      "run 2612, Training loss: 0.0011266182577938624, run on the CPU with time 0.06077869998989627\n",
      "run 2613, Training loss: 0.0011261624114757235, run on the CPU with time 0.06136459999834187\n",
      "run 2614, Training loss: 0.0011262204786093703, run on the CPU with time 0.06392219997360371\n",
      "run 2615, Training loss: 0.0011242811763870783, run on the CPU with time 0.0747935000108555\n",
      "run 2616, Training loss: 0.0011249964335795747, run on the CPU with time 0.07146520001697354\n",
      "run 2617, Training loss: 0.0011217708713295658, run on the CPU with time 0.062059299991233274\n",
      "run 2618, Training loss: 0.0011237224888397824, run on the CPU with time 0.06256930000381544\n",
      "run 2619, Training loss: 0.001128822329734579, run on the CPU with time 0.06655280000995845\n",
      "run 2620, Training loss: 0.0011247255917458626, run on the CPU with time 0.06149209997965954\n",
      "run 2621, Training loss: 0.0011186386104864968, run on the CPU with time 0.061167200008640066\n",
      "run 2622, Training loss: 0.0011245280239646407, run on the CPU with time 0.06234879998373799\n",
      "run 2623, Training loss: 0.0011318392826731062, run on the CPU with time 0.0623663000005763\n",
      "run 2624, Training loss: 0.0011206060055304657, run on the CPU with time 0.06218249999801628\n",
      "run 2625, Training loss: 0.0011225714270205406, run on the CPU with time 0.06390339997597039\n",
      "run 2626, Training loss: 0.0011167655962212434, run on the CPU with time 0.07165410000015981\n",
      "run 2627, Training loss: 0.00111538787423342, run on the CPU with time 0.0658300000068266\n",
      "run 2628, Training loss: 0.0011203743684745858, run on the CPU with time 0.06222129997331649\n",
      "run 2629, Training loss: 0.0011129224035275084, run on the CPU with time 0.061533400003099814\n",
      "run 2630, Training loss: 0.0011200237800287802, run on the CPU with time 0.06348799998522736\n",
      "run 2631, Training loss: 0.0011150798726372489, run on the CPU with time 0.06302810000488535\n",
      "run 2632, Training loss: 0.001114121710865716, run on the CPU with time 0.06080969999311492\n",
      "run 2633, Training loss: 0.0011173361206932564, run on the CPU with time 0.06018010000116192\n",
      "run 2634, Training loss: 0.0011112906313924626, run on the CPU with time 0.0609262999787461\n",
      "run 2635, Training loss: 0.0011142934263757938, run on the CPU with time 0.06334839999908581\n",
      "run 2636, Training loss: 0.0011130674602596131, run on the CPU with time 0.08384300000034273\n",
      "run 2637, Training loss: 0.0011085322554704937, run on the CPU with time 0.0722894000064116\n",
      "run 2638, Training loss: 0.001111573875955814, run on the CPU with time 0.062373499997192994\n",
      "run 2639, Training loss: 0.001111855317073853, run on the CPU with time 0.06230630000936799\n",
      "run 2640, Training loss: 0.0011102681566791779, run on the CPU with time 0.06182770000305027\n",
      "run 2641, Training loss: 0.001110495695585004, run on the CPU with time 0.061878300009993836\n",
      "run 2642, Training loss: 0.001104863353496264, run on the CPU with time 0.06295870000030845\n",
      "run 2643, Training loss: 0.0011071291530573614, run on the CPU with time 0.0623699999996461\n",
      "run 2644, Training loss: 0.0011049074650310848, run on the CPU with time 0.06217520000063814\n",
      "run 2645, Training loss: 0.0011110220642876812, run on the CPU with time 0.06146090000402182\n",
      "run 2646, Training loss: 0.0011018864594503116, run on the CPU with time 0.06268329999875277\n",
      "run 2647, Training loss: 0.0011036381812697404, run on the CPU with time 0.06417689999216236\n",
      "run 2648, Training loss: 0.0011028709760549562, run on the CPU with time 0.06145889998879284\n",
      "run 2649, Training loss: 0.0011098804551279912, run on the CPU with time 0.061337899998761714\n",
      "run 2650, Training loss: 0.001101626601898302, run on the CPU with time 0.0697578999970574\n",
      "run 2651, Training loss: 0.00110042648550668, run on the CPU with time 0.07678260002285242\n",
      "run 2652, Training loss: 0.0011038292108093049, run on the CPU with time 0.06533159999526106\n",
      "run 2653, Training loss: 0.0011083350730835015, run on the CPU with time 0.06242180001572706\n",
      "run 2654, Training loss: 0.0011004031190995276, run on the CPU with time 0.06178170000202954\n",
      "run 2655, Training loss: 0.001097649891760904, run on the CPU with time 0.06194099999265745\n",
      "run 2656, Training loss: 0.001095653596514074, run on the CPU with time 0.062202800007071346\n",
      "run 2657, Training loss: 0.0011029272114021957, run on the CPU with time 0.06333669999730773\n",
      "run 2658, Training loss: 0.0010971282830393068, run on the CPU with time 0.0634161000198219\n",
      "run 2659, Training loss: 0.0010940846780034587, run on the CPU with time 0.06177369999932125\n",
      "run 2660, Training loss: 0.001099645804821524, run on the CPU with time 0.061456099996576086\n",
      "run 2661, Training loss: 0.0010978781472658738, run on the CPU with time 0.07403990000602789\n",
      "run 2662, Training loss: 0.0010935835779078346, run on the CPU with time 0.06341699999757111\n",
      "run 2663, Training loss: 0.0010942261339302853, run on the CPU with time 0.06174540001666173\n",
      "run 2664, Training loss: 0.0011017864855768329, run on the CPU with time 0.06411019997904077\n",
      "run 2665, Training loss: 0.0010924331412587147, run on the CPU with time 0.06363160000182688\n",
      "run 2666, Training loss: 0.0010959010288140482, run on the CPU with time 0.06345670000882819\n",
      "run 2667, Training loss: 0.0010996074337030718, run on the CPU with time 0.06262390001211315\n",
      "run 2668, Training loss: 0.0010967565195063467, run on the CPU with time 0.06116810001549311\n",
      "run 2669, Training loss: 0.001090257293650542, run on the CPU with time 0.06354130001273006\n",
      "run 2670, Training loss: 0.001090589185233429, run on the CPU with time 0.06459600001107901\n",
      "run 2671, Training loss: 0.001096785133036478, run on the CPU with time 0.06392700001015328\n",
      "run 2672, Training loss: 0.0010925764190191827, run on the CPU with time 0.06263589998707175\n",
      "run 2673, Training loss: 0.0010897718256191267, run on the CPU with time 0.06462640000972897\n",
      "run 2674, Training loss: 0.0010948767013112294, run on the CPU with time 0.06412479997379705\n",
      "run 2675, Training loss: 0.0010891786488090556, run on the CPU with time 0.062366900005145\n",
      "run 2676, Training loss: 0.0010850941317833284, run on the CPU with time 0.06509309998364188\n",
      "run 2677, Training loss: 0.0010933003355272706, run on the CPU with time 0.06366149999666959\n",
      "run 2678, Training loss: 0.0010873165211788462, run on the CPU with time 0.06156510001164861\n",
      "run 2679, Training loss: 0.001091442587891255, run on the CPU with time 0.06363499999861233\n",
      "run 2680, Training loss: 0.0010830765374562775, run on the CPU with time 0.06190010000136681\n",
      "run 2681, Training loss: 0.0010922015091223744, run on the CPU with time 0.06247050000820309\n",
      "run 2682, Training loss: 0.0011296338338235563, run on the CPU with time 0.06293690000893548\n",
      "run 2683, Training loss: 0.0010895615099632944, run on the CPU with time 0.06175900000380352\n",
      "run 2684, Training loss: 0.0010992288400964092, run on the CPU with time 0.06232070000260137\n",
      "run 2685, Training loss: 0.001082894349449568, run on the CPU with time 0.06656790000852197\n",
      "run 2686, Training loss: 0.0010874235777009744, run on the CPU with time 0.07178829997428693\n",
      "run 2687, Training loss: 0.0010809286043778146, run on the CPU with time 0.06931479999911971\n",
      "run 2688, Training loss: 0.0010835326188085178, run on the CPU with time 0.06415609997930005\n",
      "run 2689, Training loss: 0.0010806757971377836, run on the CPU with time 0.06317909999052063\n",
      "run 2690, Training loss: 0.0010775478666553019, run on the CPU with time 0.06234699999913573\n",
      "run 2691, Training loss: 0.0010756405874086672, run on the CPU with time 0.06099639998865314\n",
      "run 2692, Training loss: 0.0010802304192938292, run on the CPU with time 0.06182269999408163\n",
      "run 2693, Training loss: 0.0010785434748289515, run on the CPU with time 0.06133820000104606\n",
      "run 2694, Training loss: 0.001081371207163267, run on the CPU with time 0.06180950000998564\n",
      "run 2695, Training loss: 0.0010749737738726915, run on the CPU with time 0.061921400018036366\n",
      "run 2696, Training loss: 0.0010744842506193726, run on the CPU with time 0.07374459999846295\n",
      "run 2697, Training loss: 0.00107190251133447, run on the CPU with time 0.07990780001273379\n",
      "run 2698, Training loss: 0.0010800942453815052, run on the CPU with time 0.06867249999777414\n",
      "run 2699, Training loss: 0.0010735549319096291, run on the CPU with time 0.06177560001378879\n",
      "run 2700, Training loss: 0.0010733208803602875, run on the CPU with time 0.0610166999977082\n",
      "run 2701, Training loss: 0.0010695773767525002, run on the CPU with time 0.06242909998400137\n",
      "run 2702, Training loss: 0.0010917022742267528, run on the CPU with time 0.06365910000749864\n",
      "run 2703, Training loss: 0.0010745496169626925, run on the CPU with time 0.0629627000016626\n",
      "run 2704, Training loss: 0.001072304023810747, run on the CPU with time 0.061668000009376556\n",
      "run 2705, Training loss: 0.0010666384040550012, run on the CPU with time 0.06471990002319217\n",
      "run 2706, Training loss: 0.0011896425482436, run on the CPU with time 0.06613640001160093\n",
      "run 2707, Training loss: 0.0010830917109904642, run on the CPU with time 0.06323940001311712\n",
      "run 2708, Training loss: 0.0010734112904835704, run on the CPU with time 0.06166559999110177\n",
      "run 2709, Training loss: 0.00106771466016653, run on the CPU with time 0.06237890000920743\n",
      "run 2710, Training loss: 0.0010623879422961512, run on the CPU with time 0.07524209999246523\n",
      "run 2711, Training loss: 0.0010677451654125681, run on the CPU with time 0.06249699997715652\n",
      "run 2712, Training loss: 0.0010660398168097758, run on the CPU with time 0.06285629997728392\n",
      "run 2713, Training loss: 0.0010655782362763684, run on the CPU with time 0.06511400002636947\n",
      "run 2714, Training loss: 0.0012268060829559214, run on the CPU with time 0.06113940000068396\n",
      "run 2715, Training loss: 0.0010666477386786772, run on the CPU with time 0.06263739999849349\n",
      "run 2716, Training loss: 0.0010632535016735676, run on the CPU with time 0.06259289997979067\n",
      "run 2717, Training loss: 0.0010595845804676751, run on the CPU with time 0.06255219999002293\n",
      "run 2718, Training loss: 0.0010636387167985296, run on the CPU with time 0.06168559999787249\n",
      "run 2719, Training loss: 0.0010615945385573221, run on the CPU with time 0.06488070002524182\n",
      "run 2720, Training loss: 0.0010663094640635377, run on the CPU with time 0.0644297000253573\n",
      "run 2721, Training loss: 0.001063126099391163, run on the CPU with time 0.06956689999788068\n",
      "run 2722, Training loss: 0.0010606127396220637, run on the CPU with time 0.06448369999998249\n",
      "run 2723, Training loss: 0.0010612559017317835, run on the CPU with time 0.06291359997703694\n",
      "run 2724, Training loss: 0.001063227791151307, run on the CPU with time 0.06456579998484813\n",
      "run 2725, Training loss: 0.0010584959693501746, run on the CPU with time 0.06171800001175143\n",
      "run 2726, Training loss: 0.001055358985732627, run on the CPU with time 0.06596159999025986\n",
      "run 2727, Training loss: 0.001053022009869344, run on the CPU with time 0.062169000011635944\n",
      "run 2728, Training loss: 0.001052460745970827, run on the CPU with time 0.06252229999518022\n",
      "run 2729, Training loss: 0.0010552024729391137, run on the CPU with time 0.06200179998995736\n",
      "run 2730, Training loss: 0.0010611617591083896, run on the CPU with time 0.06208820000756532\n",
      "run 2731, Training loss: 0.0010560106158002535, run on the CPU with time 0.06323080000584014\n",
      "run 2732, Training loss: 0.0010614065014206888, run on the CPU with time 0.06212509999750182\n",
      "run 2733, Training loss: 0.0010548008410048418, run on the CPU with time 0.0631571999983862\n",
      "run 2734, Training loss: 0.0010648168414428998, run on the CPU with time 0.06744569999864325\n",
      "run 2735, Training loss: 0.0010598823853466787, run on the CPU with time 0.06383610001648776\n",
      "run 2736, Training loss: 0.001049800844074608, run on the CPU with time 0.0632196000078693\n",
      "run 2737, Training loss: 0.0010643017130198794, run on the CPU with time 0.06454580000718124\n",
      "run 2738, Training loss: 0.0010504012004027821, run on the CPU with time 0.06291909998981282\n",
      "run 2739, Training loss: 0.0010551816131655041, run on the CPU with time 0.06347409999580123\n",
      "run 2740, Training loss: 0.001045289398528191, run on the CPU with time 0.06406249999417923\n",
      "run 2741, Training loss: 0.001054803674659607, run on the CPU with time 0.06274590000975877\n",
      "run 2742, Training loss: 0.001051584791606398, run on the CPU with time 0.0635824000055436\n",
      "run 2743, Training loss: 0.0010453312864476835, run on the CPU with time 0.06226340000284836\n",
      "run 2744, Training loss: 0.00104865163752534, run on the CPU with time 0.06160200000158511\n",
      "run 2745, Training loss: 0.0010474655656831932, run on the CPU with time 0.0639890999882482\n",
      "run 2746, Training loss: 0.0010554117474715565, run on the CPU with time 0.06102839999948628\n",
      "run 2747, Training loss: 0.001050200948057781, run on the CPU with time 0.0666862000070978\n",
      "run 2748, Training loss: 0.001051956010798511, run on the CPU with time 0.08929849998094141\n",
      "run 2749, Training loss: 0.001048342889738375, run on the CPU with time 0.06595349998679012\n",
      "run 2750, Training loss: 0.0010415289716382342, run on the CPU with time 0.061704899999313056\n",
      "run 2751, Training loss: 0.0010444800332069545, run on the CPU with time 0.06223439998575486\n",
      "run 2752, Training loss: 0.0010407978317710392, run on the CPU with time 0.06472429999848828\n",
      "run 2753, Training loss: 0.001043163623928029, run on the CPU with time 0.06261969998013228\n",
      "run 2754, Training loss: 0.0010403883519757074, run on the CPU with time 0.06383840000489727\n",
      "run 2755, Training loss: 0.0010517116681016475, run on the CPU with time 0.060925700003281236\n",
      "run 2756, Training loss: 0.0010503898162410638, run on the CPU with time 0.062014900002395734\n",
      "run 2757, Training loss: 0.0010398475641109556, run on the CPU with time 0.060202799999387935\n",
      "run 2758, Training loss: 0.0010388566288880114, run on the CPU with time 0.0607622999814339\n",
      "run 2759, Training loss: 0.0010370076219921678, run on the CPU with time 0.07314730001962744\n",
      "run 2760, Training loss: 0.0010381152284522118, run on the CPU with time 0.08574089998728596\n",
      "run 2761, Training loss: 0.0010407139856728133, run on the CPU with time 0.06402610000804998\n",
      "run 2762, Training loss: 0.0010364239706508339, run on the CPU with time 0.06315239999094047\n",
      "run 2763, Training loss: 0.0010445131801216949, run on the CPU with time 0.06521390000125393\n",
      "run 2764, Training loss: 0.0010342498206062538, run on the CPU with time 0.06198259998927824\n",
      "run 2765, Training loss: 0.0010366687909150972, run on the CPU with time 0.061208900006022304\n",
      "run 2766, Training loss: 0.0010333851410409393, run on the CPU with time 0.06069549999665469\n",
      "run 2767, Training loss: 0.0010313521714124362, run on the CPU with time 0.06097240000963211\n",
      "run 2768, Training loss: 0.0010405559156920804, run on the CPU with time 0.06321890000253916\n",
      "run 2769, Training loss: 0.0010344456481768496, run on the CPU with time 0.06227600001147948\n",
      "run 2770, Training loss: 0.0010287638032011805, run on the CPU with time 0.07665459997951984\n",
      "run 2771, Training loss: 0.0010362174548473294, run on the CPU with time 0.06846790001145564\n",
      "run 2772, Training loss: 0.001031346181959882, run on the CPU with time 0.06809159999829717\n",
      "run 2773, Training loss: 0.0010328798362355552, run on the CPU with time 0.0627262000052724\n",
      "run 2774, Training loss: 0.0010280685701714405, run on the CPU with time 0.06134270000620745\n",
      "run 2775, Training loss: 0.00103087604782724, run on the CPU with time 0.061487500002840534\n",
      "run 2776, Training loss: 0.0010336493726671588, run on the CPU with time 0.06254809998790734\n",
      "run 2777, Training loss: 0.0010726286263723689, run on the CPU with time 0.08621030001086183\n",
      "run 2778, Training loss: 0.0010325341416649858, run on the CPU with time 0.07651449998957105\n",
      "run 2779, Training loss: 0.0010299185540299566, run on the CPU with time 0.07414720000815578\n",
      "run 2780, Training loss: 0.0010254653354588134, run on the CPU with time 0.0633570000063628\n",
      "run 2781, Training loss: 0.001027129261638038, run on the CPU with time 0.06273040000814945\n",
      "run 2782, Training loss: 0.0010318188397584228, run on the CPU with time 0.06242120001115836\n",
      "run 2783, Training loss: 0.001023333871175551, run on the CPU with time 0.06375599998864345\n",
      "run 2784, Training loss: 0.0010255760994120713, run on the CPU with time 0.061252000014064834\n",
      "run 2785, Training loss: 0.00102224422655788, run on the CPU with time 0.07252909999806434\n",
      "run 2786, Training loss: 0.0010206184532628206, run on the CPU with time 0.06850910000503063\n",
      "run 2787, Training loss: 0.0010192429205000569, run on the CPU with time 0.06421129999216646\n",
      "run 2788, Training loss: 0.0010204239144844575, run on the CPU with time 0.06206409999867901\n",
      "run 2789, Training loss: 0.0010223548048419286, run on the CPU with time 0.06304030001047067\n",
      "run 2790, Training loss: 0.0010162195144878504, run on the CPU with time 0.06618759999400936\n",
      "run 2791, Training loss: 0.001020694819321348, run on the CPU with time 0.0624041999981273\n",
      "run 2792, Training loss: 0.00101677712703928, run on the CPU with time 0.061574899998959154\n",
      "run 2793, Training loss: 0.0010184937877718105, run on the CPU with time 0.06146850000368431\n",
      "run 2794, Training loss: 0.0010329214588802476, run on the CPU with time 0.06604860001243651\n",
      "run 2795, Training loss: 0.0010156745972794438, run on the CPU with time 0.06234640002367087\n",
      "run 2796, Training loss: 0.0011083304994496178, run on the CPU with time 0.07366130000445992\n",
      "run 2797, Training loss: 0.0010187137659654994, run on the CPU with time 0.06376800002180971\n",
      "run 2798, Training loss: 0.0010174156204315792, run on the CPU with time 0.06328580001718365\n",
      "run 2799, Training loss: 0.0011257754190988965, run on the CPU with time 0.06313540000701323\n",
      "run 2800, Training loss: 0.0010337173869960348, run on the CPU with time 0.06804160002502613\n",
      "run 2801, Training loss: 0.0010296455813328398, run on the CPU with time 0.06411259999731556\n",
      "run 2802, Training loss: 0.0010215612233645806, run on the CPU with time 0.06847900000866503\n",
      "run 2803, Training loss: 0.0010197196895949839, run on the CPU with time 0.06291189999319613\n",
      "run 2804, Training loss: 0.0010104726116961418, run on the CPU with time 0.06205209999461658\n",
      "run 2805, Training loss: 0.001014586162769278, run on the CPU with time 0.06346730000223033\n",
      "run 2806, Training loss: 0.0010109210761914834, run on the CPU with time 0.06330680000246502\n",
      "run 2807, Training loss: 0.0010096698251717581, run on the CPU with time 0.06176479998975992\n",
      "run 2808, Training loss: 0.0010068992247397545, run on the CPU with time 0.06098910002037883\n",
      "run 2809, Training loss: 0.0010071068598666566, run on the CPU with time 0.061764399986714125\n",
      "run 2810, Training loss: 0.0010169143922791012, run on the CPU with time 0.062422100018011406\n",
      "run 2811, Training loss: 0.0010085664321608915, run on the CPU with time 0.061161200021160766\n",
      "run 2812, Training loss: 0.001009927926332404, run on the CPU with time 0.09390779997920617\n",
      "run 2813, Training loss: 0.0010073429869821252, run on the CPU with time 0.06494710000697523\n",
      "run 2814, Training loss: 0.00101654269462663, run on the CPU with time 0.06677000000490807\n",
      "run 2815, Training loss: 0.0010984519280927171, run on the CPU with time 0.062204599991673604\n",
      "run 2816, Training loss: 0.001010460114775924, run on the CPU with time 0.061691600014455616\n",
      "run 2817, Training loss: 0.0010093958117067814, run on the CPU with time 0.06143599998904392\n",
      "run 2818, Training loss: 0.0010033528318506963, run on the CPU with time 0.06015900001511909\n",
      "run 2819, Training loss: 0.0010045789912427692, run on the CPU with time 0.06032059999415651\n",
      "run 2820, Training loss: 0.0010342750628214246, run on the CPU with time 0.0606596999859903\n",
      "run 2821, Training loss: 0.0010059681530203142, run on the CPU with time 0.06334930000593886\n",
      "run 2822, Training loss: 0.0010038919826249847, run on the CPU with time 0.061081399995600805\n",
      "run 2823, Training loss: 0.0009990359646590977, run on the CPU with time 0.0896612000069581\n",
      "run 2824, Training loss: 0.0010044319578148796, run on the CPU with time 0.06738940000650473\n",
      "run 2825, Training loss: 0.0010069867986947595, run on the CPU with time 0.06186700001126155\n",
      "run 2826, Training loss: 0.0009996103505479617, run on the CPU with time 0.060959800001000986\n",
      "run 2827, Training loss: 0.0010019672647600104, run on the CPU with time 0.06755830001202412\n",
      "run 2828, Training loss: 0.0010048434934353944, run on the CPU with time 0.06269089999841526\n",
      "run 2829, Training loss: 0.0010017285737955138, run on the CPU with time 0.06135590001940727\n",
      "run 2830, Training loss: 0.0012087172247199553, run on the CPU with time 0.06294270002399571\n",
      "run 2831, Training loss: 0.00100778550837607, run on the CPU with time 0.06326189998071641\n",
      "run 2832, Training loss: 0.000998874402284855, run on the CPU with time 0.07422100001713261\n",
      "run 2833, Training loss: 0.0009967131579866294, run on the CPU with time 0.06581179998465814\n",
      "run 2834, Training loss: 0.0009971042392532003, run on the CPU with time 0.06303940000361763\n",
      "run 2835, Training loss: 0.001001004347860792, run on the CPU with time 0.06368029999430291\n",
      "run 2836, Training loss: 0.000992773174849307, run on the CPU with time 0.06706359999952838\n",
      "run 2837, Training loss: 0.0009953475116080055, run on the CPU with time 0.06422280002152547\n",
      "run 2838, Training loss: 0.0010089001933837691, run on the CPU with time 0.06262079998850822\n",
      "run 2839, Training loss: 0.0009942481032339857, run on the CPU with time 0.06097409999347292\n",
      "run 2840, Training loss: 0.0009921840535587927, run on the CPU with time 0.06361809998634271\n",
      "run 2841, Training loss: 0.0009938002929795236, run on the CPU with time 0.06267590000061318\n",
      "run 2842, Training loss: 0.0009963408410178752, run on the CPU with time 0.06148109998321161\n",
      "run 2843, Training loss: 0.0010002646333305166, run on the CPU with time 0.06065650001983158\n",
      "run 2844, Training loss: 0.000994541781917426, run on the CPU with time 0.06513469998026267\n",
      "run 2845, Training loss: 0.0009933688441709488, run on the CPU with time 0.06359130001510493\n",
      "run 2846, Training loss: 0.0009923784323539373, run on the CPU with time 0.06519490000209771\n",
      "run 2847, Training loss: 0.0009985209082845937, run on the CPU with time 0.06275989999994636\n",
      "run 2848, Training loss: 0.0009910784222566607, run on the CPU with time 0.06524299999000505\n",
      "run 2849, Training loss: 0.0009869495501995925, run on the CPU with time 0.06362749999971129\n",
      "run 2850, Training loss: 0.0009872049294988921, run on the CPU with time 0.062499299994669855\n",
      "run 2851, Training loss: 0.0009902336094290314, run on the CPU with time 0.06185840000398457\n",
      "run 2852, Training loss: 0.00099425238837615, run on the CPU with time 0.0647080999915488\n",
      "run 2853, Training loss: 0.0009866035119723, run on the CPU with time 0.0613487999944482\n",
      "run 2854, Training loss: 0.0009851235608617902, run on the CPU with time 0.06246990000363439\n",
      "run 2855, Training loss: 0.000983953152701195, run on the CPU with time 0.06620949998614378\n",
      "run 2856, Training loss: 0.000985054837127047, run on the CPU with time 0.06552699999883771\n",
      "run 2857, Training loss: 0.0009867046079307328, run on the CPU with time 0.06447179999668151\n",
      "run 2858, Training loss: 0.0009861266812301686, run on the CPU with time 0.06289170001400635\n",
      "run 2859, Training loss: 0.0009817086374492977, run on the CPU with time 0.061857499997131526\n",
      "run 2860, Training loss: 0.0009808510746304158, run on the CPU with time 0.06420369999250397\n",
      "run 2861, Training loss: 0.000983216331753118, run on the CPU with time 0.06290930000250228\n",
      "run 2862, Training loss: 0.0009800807804574354, run on the CPU with time 0.06549949999316595\n",
      "run 2863, Training loss: 0.000981596482266799, run on the CPU with time 0.06538820001878776\n",
      "run 2864, Training loss: 0.0009882274947647768, run on the CPU with time 0.06375390000175685\n",
      "run 2865, Training loss: 0.0009899445427105423, run on the CPU with time 0.06219949998194352\n",
      "run 2866, Training loss: 0.000975730788460086, run on the CPU with time 0.062474100006511435\n",
      "run 2867, Training loss: 0.0009840785701023627, run on the CPU with time 0.062188199983211234\n",
      "run 2868, Training loss: 0.0009794565514973576, run on the CPU with time 0.06253880000440404\n",
      "run 2869, Training loss: 0.0009815612328318158, run on the CPU with time 0.06601529999170452\n",
      "run 2870, Training loss: 0.0009756200233162169, run on the CPU with time 0.062155699997674674\n",
      "run 2871, Training loss: 0.000978349888464436, run on the CPU with time 0.06259710001177154\n",
      "run 2872, Training loss: 0.000975870774114314, run on the CPU with time 0.06446900000446476\n",
      "run 2873, Training loss: 0.0009801316326378251, run on the CPU with time 0.06037490000016987\n",
      "run 2874, Training loss: 0.0009733567309343595, run on the CPU with time 0.061537500005215406\n",
      "run 2875, Training loss: 0.0009777482860623752, run on the CPU with time 0.062049000000115484\n",
      "run 2876, Training loss: 0.0009740892092602074, run on the CPU with time 0.061524499993538484\n",
      "run 2877, Training loss: 0.0009772806804738304, run on the CPU with time 0.08325440000044182\n",
      "run 2878, Training loss: 0.0009741739626571705, run on the CPU with time 0.07610409997869283\n",
      "run 2879, Training loss: 0.0009722050739789913, run on the CPU with time 0.06461239999043755\n",
      "run 2880, Training loss: 0.0010560268510413483, run on the CPU with time 0.06176340000820346\n",
      "run 2881, Training loss: 0.0009735111481859349, run on the CPU with time 0.06178250000812113\n",
      "run 2882, Training loss: 0.0009742234270313268, run on the CPU with time 0.06214280001586303\n",
      "run 2883, Training loss: 0.0009706365565679417, run on the CPU with time 0.062333099980605766\n",
      "run 2884, Training loss: 0.000973669377047124, run on the CPU with time 0.06257639999967068\n",
      "run 2885, Training loss: 0.0009769741415022342, run on the CPU with time 0.06386009999550879\n",
      "run 2886, Training loss: 0.0009675787859242229, run on the CPU with time 0.06237259999033995\n",
      "run 2887, Training loss: 0.0009726698011614975, run on the CPU with time 0.06794259999878705\n",
      "run 2888, Training loss: 0.001046638434027872, run on the CPU with time 0.06844250002177432\n",
      "run 2889, Training loss: 0.0009706592564312317, run on the CPU with time 0.06337439999333583\n",
      "run 2890, Training loss: 0.0009635037005885104, run on the CPU with time 0.061864899995271116\n",
      "run 2891, Training loss: 0.0009707500779768452, run on the CPU with time 0.062262300023576245\n",
      "run 2892, Training loss: 0.0009680482309539167, run on the CPU with time 0.06268500001169741\n",
      "run 2893, Training loss: 0.0009736991187144833, run on the CPU with time 0.06429239999852143\n",
      "run 2894, Training loss: 0.0009651801584616557, run on the CPU with time 0.06377659999998286\n",
      "run 2895, Training loss: 0.0009649982153363949, run on the CPU with time 0.06259329998283647\n",
      "run 2896, Training loss: 0.0009655180195378224, run on the CPU with time 0.06164950001402758\n",
      "run 2897, Training loss: 0.0009683231407755309, run on the CPU with time 0.06406740000238642\n",
      "run 2898, Training loss: 0.0009635705031872599, run on the CPU with time 0.06208709999918938\n",
      "run 2899, Training loss: 0.0009618126210046863, run on the CPU with time 0.06285479999496602\n",
      "run 2900, Training loss: 0.0009607735255322971, run on the CPU with time 0.062223800021456555\n",
      "run 2901, Training loss: 0.0009615375179485206, run on the CPU with time 0.07637949998024851\n",
      "run 2902, Training loss: 0.0009664260545502078, run on the CPU with time 0.06313580001005903\n",
      "run 2903, Training loss: 0.0011578207789253528, run on the CPU with time 0.07645209998008795\n",
      "run 2904, Training loss: 0.0009617815417269329, run on the CPU with time 0.06291680000140332\n",
      "run 2905, Training loss: 0.000972075646264288, run on the CPU with time 0.06264899999951012\n",
      "run 2906, Training loss: 0.0009594100740452467, run on the CPU with time 0.061678600002778694\n",
      "run 2907, Training loss: 0.000959406194339284, run on the CPU with time 0.06257899999036454\n",
      "run 2908, Training loss: 0.0009568474785985679, run on the CPU with time 0.062026700004935265\n",
      "run 2909, Training loss: 0.0009602958467439749, run on the CPU with time 0.06223469998803921\n",
      "run 2910, Training loss: 0.0009631433552799122, run on the CPU with time 0.0645780999911949\n",
      "run 2911, Training loss: 0.0009583635104883631, run on the CPU with time 0.06608889999915846\n",
      "run 2912, Training loss: 0.001015920037091498, run on the CPU with time 0.06570360000478104\n",
      "run 2913, Training loss: 0.000961388801475881, run on the CPU with time 0.06121730001177639\n",
      "run 2914, Training loss: 0.0009574470507769464, run on the CPU with time 0.06303570000454783\n",
      "run 2915, Training loss: 0.0009534622652526278, run on the CPU with time 0.0706443999952171\n",
      "run 2916, Training loss: 0.0009548609334842695, run on the CPU with time 0.06489370000781491\n",
      "run 2917, Training loss: 0.0009538766980933195, run on the CPU with time 0.06286390000605024\n",
      "run 2918, Training loss: 0.0009559898104767357, run on the CPU with time 0.06288120002136566\n",
      "run 2919, Training loss: 0.0009526594332783134, run on the CPU with time 0.06637259997660294\n",
      "run 2920, Training loss: 0.0009520684786283793, run on the CPU with time 0.06502209999598563\n",
      "run 2921, Training loss: 0.000952141543497088, run on the CPU with time 0.06470039999112487\n",
      "run 2922, Training loss: 0.0009502347731086047, run on the CPU with time 0.06183380002039485\n",
      "run 2923, Training loss: 0.0009502877154525115, run on the CPU with time 0.06250729999737814\n",
      "run 2924, Training loss: 0.0009490567633210132, run on the CPU with time 0.062290200003189966\n",
      "run 2925, Training loss: 0.0009538889177995522, run on the CPU with time 0.06163360000937246\n",
      "run 2926, Training loss: 0.0009553908017882019, run on the CPU with time 0.06128609998268075\n",
      "run 2927, Training loss: 0.0009473778058070986, run on the CPU with time 0.06246700001065619\n",
      "run 2928, Training loss: 0.0009476560260095953, run on the CPU with time 0.06250520001049154\n",
      "run 2929, Training loss: 0.0009480857507283376, run on the CPU with time 0.06670389999635518\n",
      "run 2930, Training loss: 0.0009449910713142757, run on the CPU with time 0.07820780001929961\n",
      "run 2931, Training loss: 0.0009437861222316066, run on the CPU with time 0.06756669998867437\n",
      "run 2932, Training loss: 0.0009435044234454504, run on the CPU with time 0.06146279998938553\n",
      "run 2933, Training loss: 0.0009485559612750711, run on the CPU with time 0.06322419998468831\n",
      "run 2934, Training loss: 0.0009439750000059245, run on the CPU with time 0.0624600000155624\n",
      "run 2935, Training loss: 0.0009430041428210892, run on the CPU with time 0.062359999981708825\n",
      "run 2936, Training loss: 0.0009430066302064171, run on the CPU with time 0.06018990001757629\n",
      "run 2937, Training loss: 0.0009479177363986806, run on the CPU with time 0.0602966999867931\n",
      "run 2938, Training loss: 0.0009477572626730597, run on the CPU with time 0.06104950001463294\n",
      "run 2939, Training loss: 0.0009431174450913783, run on the CPU with time 0.06193809999967925\n",
      "run 2940, Training loss: 0.0009416291691608917, run on the CPU with time 0.0672879999910947\n",
      "run 2941, Training loss: 0.000941245469320248, run on the CPU with time 0.07471179999993183\n",
      "run 2942, Training loss: 0.000945366352375872, run on the CPU with time 0.06389489999855869\n",
      "run 2943, Training loss: 0.0009396558557351785, run on the CPU with time 0.06567470001755282\n",
      "run 2944, Training loss: 0.0009446434493094619, run on the CPU with time 0.06289339999784715\n",
      "run 2945, Training loss: 0.0009431420951758892, run on the CPU with time 0.0616405000037048\n",
      "run 2946, Training loss: 0.0009449843054674354, run on the CPU with time 0.0622053999977652\n",
      "run 2947, Training loss: 0.0009643071204415967, run on the CPU with time 0.0627890000178013\n",
      "run 2948, Training loss: 0.0009414641186594963, run on the CPU with time 0.06127780000679195\n",
      "run 2949, Training loss: 0.0009361756747239269, run on the CPU with time 0.06214019999606535\n",
      "run 2950, Training loss: 0.0009341117685173892, run on the CPU with time 0.06270210002548993\n",
      "run 2951, Training loss: 0.0009394995722686872, run on the CPU with time 0.06470510002691299\n",
      "run 2952, Training loss: 0.0009417222051772776, run on the CPU with time 0.062136399996234104\n",
      "run 2953, Training loss: 0.0009342464924305783, run on the CPU with time 0.08292670000810176\n",
      "run 2954, Training loss: 0.0009359403333457356, run on the CPU with time 0.06517810001969337\n",
      "run 2955, Training loss: 0.0009334269798604179, run on the CPU with time 0.0614445999963209\n",
      "run 2956, Training loss: 0.0009441697464684363, run on the CPU with time 0.0644785999902524\n",
      "run 2957, Training loss: 0.0009362075315121646, run on the CPU with time 0.06333040000754409\n",
      "run 2958, Training loss: 0.0009318193592083513, run on the CPU with time 0.06876620001276024\n",
      "run 2959, Training loss: 0.000934710665288466, run on the CPU with time 0.0614330000244081\n",
      "run 2960, Training loss: 0.0009335576332225834, run on the CPU with time 0.06119629999739118\n",
      "run 2961, Training loss: 0.0009396113690772009, run on the CPU with time 0.06254709998029284\n",
      "run 2962, Training loss: 0.00093701501563456, run on the CPU with time 0.06708869998692535\n",
      "run 2963, Training loss: 0.000934862078760158, run on the CPU with time 0.06466249999357387\n",
      "run 2964, Training loss: 0.0009334154984323753, run on the CPU with time 0.06595760001800954\n",
      "run 2965, Training loss: 0.0009304160331256836, run on the CPU with time 0.06217109999852255\n",
      "run 2966, Training loss: 0.000956885917383426, run on the CPU with time 0.06382710000616498\n",
      "run 2967, Training loss: 0.0009283056748582975, run on the CPU with time 0.06289010000182316\n",
      "run 2968, Training loss: 0.0009307754203622277, run on the CPU with time 0.06205810001119971\n",
      "run 2969, Training loss: 0.0009281718358331339, run on the CPU with time 0.0607216999924276\n",
      "run 2970, Training loss: 0.0009253429137474582, run on the CPU with time 0.062150899990228936\n",
      "run 2971, Training loss: 0.0009369116724966179, run on the CPU with time 0.08264639999833889\n",
      "run 2972, Training loss: 0.000931149954439289, run on the CPU with time 0.06715129999793135\n",
      "run 2973, Training loss: 0.0010134077136833431, run on the CPU with time 0.06280680000782013\n",
      "run 2974, Training loss: 0.0009301504437065556, run on the CPU with time 0.06225129999802448\n",
      "run 2975, Training loss: 0.0010642518690887797, run on the CPU with time 0.06173280000803061\n",
      "run 2976, Training loss: 0.0009256945152380715, run on the CPU with time 0.06347049999749288\n",
      "run 2977, Training loss: 0.0009242601691519006, run on the CPU with time 0.06561649998184294\n",
      "run 2978, Training loss: 0.0011158138851566368, run on the CPU with time 0.06176779998349957\n",
      "run 2979, Training loss: 0.0009258099090154495, run on the CPU with time 0.06124320000526495\n",
      "run 2980, Training loss: 0.0009346891277924773, run on the CPU with time 0.06288419998600148\n",
      "run 2981, Training loss: 0.0009241435175102776, run on the CPU with time 0.07374240001081489\n",
      "run 2982, Training loss: 0.0009236719580630134, run on the CPU with time 0.06987549999030307\n",
      "run 2983, Training loss: 0.0009250102999927052, run on the CPU with time 0.06135880001238547\n",
      "run 2984, Training loss: 0.0009242132586554031, run on the CPU with time 0.06261579997953959\n",
      "run 2985, Training loss: 0.0009226585856099105, run on the CPU with time 0.06415160000324249\n",
      "run 2986, Training loss: 0.0009222520512676883, run on the CPU with time 0.06280250000418164\n",
      "run 2987, Training loss: 0.0009170472180068662, run on the CPU with time 0.061502200027462095\n",
      "run 2988, Training loss: 0.0009239374280944255, run on the CPU with time 0.06110910000279546\n",
      "run 2989, Training loss: 0.0009215134920933369, run on the CPU with time 0.07588879999821074\n",
      "run 2990, Training loss: 0.0009164736297799953, run on the CPU with time 0.06512089999159798\n",
      "run 2991, Training loss: 0.0009185825821160424, run on the CPU with time 0.062438399996608496\n",
      "run 2992, Training loss: 0.0009201669494028796, run on the CPU with time 0.06270690000383183\n",
      "run 2993, Training loss: 0.0009242015611975645, run on the CPU with time 0.06530350001412444\n",
      "run 2994, Training loss: 0.0009192143752774096, run on the CPU with time 0.062317200005054474\n",
      "run 2995, Training loss: 0.0009161347236262042, run on the CPU with time 0.0614767000079155\n",
      "run 2996, Training loss: 0.0009193650936967143, run on the CPU with time 0.06317969999508932\n",
      "run 2997, Training loss: 0.0009138016802105333, run on the CPU with time 0.07235600001877174\n",
      "run 2998, Training loss: 0.0009171888402885419, run on the CPU with time 0.06282290001399815\n",
      "run 2999, Training loss: 0.0009160222805142191, run on the CPU with time 0.06129149999469519\n",
      "run 3000, Training loss: 0.0009158325624329419, run on the CPU with time 0.06504490002407692\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import timeit as time\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 3000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    cum_loss = 0\n",
    "\n",
    "    start = time.default_timer()\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cum_loss += loss.item()\n",
    "     \n",
    "    stop = time.default_timer()\n",
    "    print(f\"run {i + 1}, Training loss: {cum_loss/len(train_loader)}, run on the CPU with time {stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run correct: 55\n",
      "run correct: 50\n",
      "run correct: 46\n",
      "run correct: 52\n",
      "run correct: 48\n",
      "run correct: 49\n",
      "run correct: 53\n",
      "run correct: 57\n",
      "run correct: 47\n",
      "run correct: 44\n",
      "run correct: 49\n",
      "run correct: 47\n",
      "run correct: 46\n",
      "run correct: 47\n",
      "run correct: 47\n",
      "run correct: 53\n",
      "run correct: 52\n",
      "run correct: 45\n",
      "run correct: 44\n",
      "run correct: 18\n",
      "total: 617\n",
      "Correct: 949\n",
      "Test Accuracy: 153.80875202593194 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# test\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "threshold = 0.5  # set your decision threshold for the probabilities\n",
    "\n",
    "# No need to track gradients for validation, saves memory and computations\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Convert the model outputs into predicted classes by thresholding\n",
    "        predicted = (outputs > threshold).float()\n",
    "        \n",
    "        # Reshape labels to match prediction shape and convert to float\n",
    "        labels = labels.view(predicted.shape).float()\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        print(f\"run correct: {(predicted == labels).sum().item()}\")\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"total: {total}\")\n",
    "print(f\"Correct: {correct}\")\n",
    "print('Test Accuracy: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
